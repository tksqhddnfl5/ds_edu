{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practices\n",
    "\n",
    "# Changjo Yu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2.\n",
    "\n",
    "### 2-1. 1. For each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) The sample size n is extremely large, and the number of predictors p is small.\n",
    "\n",
    "이 경우엔 단순한 모형으로도 충분할 것이다.\n",
    "\n",
    "### (b) The number of predictors p is extremely large, and the number of observations n is small.\n",
    "\n",
    "sparse 경우엔 더 복잡한 모형이나 특별한 가정을 필요로 할 것이다.\n",
    "\n",
    "### (c) The relationship between the predictors and response is highly non-linear.\n",
    "\n",
    "훨씬 복잡한 모형을 필요로 할 것이다.\n",
    "\n",
    "### (d) The variance of the error terms, i.e. $\\sigma^2 = Var(\\epsilon)$, is extremely high.\n",
    "\n",
    "이러한 경우 모형이 잡아낼 수 있는 불확실성이 극도로 제한되어 있으므로 그 어떤 복잡한 모형을 사용하더라도 성능이 좋지 않음을 감안하여야 할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. Explain whether each scenario is a classification or regression problem, and indicate whether we are most interested in inference or prediction. Finally, provide n and p.\n",
    "\n",
    "### (a) We collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. We are interested in understanding which factors affect CEO salary.\n",
    "\n",
    "regression problem, inference, $n=500, p=3$. (In this case, we don't count the constant term.)\n",
    "\n",
    "### (b) We are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables.\n",
    "\n",
    "classification, prediction, $n=20, p=13$.\n",
    "\n",
    "### (c) We are interested in predicting the % change in the USD/Euro exchange rate in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the USD/Euro, the % change in the US market, the % change in the British market, and the % change in the German market.\n",
    "\n",
    "regression, prediction, $n=52, p=3.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-5. What are the advantages and disadvantages of a very flexible (versus a less flexible) approach for regression or classification? Under what circumstances might a more flexible approach be preferred to a less flexible approach? When might a less flexible approach be preferred?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "매우 유연한 모델은...\n",
    "\n",
    "#### pros\n",
    "-  bias가 적다.\n",
    "- 데이터의 수가 충분하다면 데이터의 실제 구조를 더 많이 설명해줄 수 있다.\n",
    "\n",
    "#### cons\n",
    "- 모형의 variance가 크다.\n",
    "- overfitting의 문제가 발생한다.\n",
    "- 계산이 비싸다.\n",
    "\n",
    "#### 유연한 모델은\n",
    "- 데이터의 수가 변수의 수에 비해 충분히 많을 때 유리하다.\n",
    "(그러나 요즘 이러한 트렌드는 인공신경망 때문에 그 규칙이 깨지고 있다).\n",
    "- 데이터의 관계가 선형 이상으로 복잡한 관계일 때 이를 규명하는 데 유리하다.\n",
    "\n",
    "#### 단순한 모델은\n",
    "- 데이터의 구조가 매우 단순할 때 유리하다.\n",
    "- 변수의 개수(차원의 수)에 비해 데이터가 다소 적을 때 유리하다.\n",
    "- 계산비용을 감당할 수 없을 때 효율적으로 활용될 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3. Suppose we have a data set with five predictors, X1 = GPA, X2 = IQ, X3 = Level (1 for College and 0 for High School), X4 = Interaction between GPA and IQ, and X5 = Interaction between GPA and Level. The response is starting salary after graduation (in thousands of dollars). Suppose we use least squares to fit the model, and get $\\beta_0$ = 50, $\\beta_1$ = 20, $\\beta_2$ = 0.07, $\\beta_3$ = 35, $\\beta_4$ = 0.01, $\\beta_5$ = −10.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is structured as below;\n",
    "\n",
    "$$ \\widehat{{Salary}} = 50 + 20 {GPA} + 0.07 {IQ} + 35 {Level} + 0.01 {GPA*IQ} -10 {GPA*Level}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### (a) Which answer is correct, and why?\n",
    "\n",
    "i. For a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates.\n",
    "\n",
    "ii. For a fixed value of IQ and GPA, college graduates earn more, on average, than high school graduates.\n",
    "\n",
    "iii. For a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates provided that the GPA is high enough.\n",
    "\n",
    "iv. For a fixed value of IQ and GPA, college graduates earn more, on average, than high school graduates provided that the GPA is high enough.\n",
    "\n",
    "\n",
    "* sol) IQ랑 GPA가 고정되었다고 가정했을 때, Level과 관련된 항만 비교하여 보자. 고졸의 경우 term은 $0$이고, 대졸의 경우 term은 $35 -10GPA$가 된다. 만약에 GPA가 4.5라고 하면, 오히려 다른 변수가 고정되었을 때 대졸이 오히려 고졸보다 평균적으로 salary가 낮아지게 된다. 따라서 정답은 iii이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Predict the salary of a college graduate with IQ of 110 and a GPA of 4.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137.1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "level = 1\n",
    "iq = 110\n",
    "gpa = 4.0\n",
    "\n",
    "50 + 20*gpa + 0.07*iq + 35*level + 0.01*gpa*iq - 10*gpa*level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) True or false: Since the coefficient for the GPA/IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True, since we can statistically test that whether the coefficient is nearly zero, so that if the coefficient has very small value nearly to zero(and it has 'small' standard error), than we can conclude that the interaction has no effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-4. I collect a set of data (n = 100 observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression, i e. $Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\epsilon$.\n",
    "\n",
    "### (a) Suppose that the true relationship between X and Y is linear, i.e. $Y = \\beta_0 + \\beta_1 X$. Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.\n",
    "\n",
    "당연히 3차 다항회귀 모형이 training SSE가 더 적다. 왜냐하면 training SSE는 모형의 복잡도에 따라서 단조증가하기 때문이다. 그래서 당연히 주어진 데이터에 대해 말할 수 있는 것도 3차 모형이 더 많다. 그러나 true underlying structure에 대해서 더 잘 말할 수 있느냐는, 모른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Answer (a) using test rather than training RSS.\n",
    "\n",
    "이건 데이터의 true underlying structure를 보기 전까진, 혹은 모형을 돌려보기 전까진 모른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Suppose that the true relationship between X and Y is not linear, but we don’t know how far it is from linear. Consider the training RSS for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.\n",
    "\n",
    "training SSE는 실제 구조와 상관없이 모형의 복잡도와 반비례하므로 3차항이 확실히 더 적다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Answer (c) using test rather than training RSS.\n",
    "\n",
    "이런 경우 이 X와 Y의 비선형성이 얼마나 심한가에 따라 달라지겠지만, 그래도 평균적으로 3차항이 더 test SSE가 더 적을 것이라고 기대할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-5. Consider the fitted values that result from performing linear regression without an intercept. In this setting, the ith fitted value takes the form\n",
    "$$\n",
    "\\hat{y}_i=x_i \\hat{\\beta},\n",
    "$$\n",
    "### where\n",
    "$$\n",
    "\\hat{\\beta}=\\left(\\sum_{i=1}^n x_i y_i\\right) /\\left(\\sum_{i^{\\prime}=1}^n x_{i^{\\prime}}^2\\right) .\n",
    "$$\n",
    "### Show that we can write\n",
    "$$\n",
    "\\hat{y}_i=\\sum_{i^{\\prime}=1}^n a_{i^{\\prime}} y_{i^{\\prime}}\n",
    "$$\n",
    "\n",
    "### What is $a_i'$?\n",
    "\n",
    "### Note: We interpret this result by saying that the fitted values from linear regression are linear combinations of the response values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이건 너무 자명하게\n",
    "\n",
    "$$\n",
    "\\hat{y}_i=x_i \\left(\\sum_{i=1}^n x_i y_i\\right) /\\left(\\sum_{i^{\\prime}=1}^n x_{i^{\\prime}}^2\\right)\n",
    "$$\n",
    "\n",
    "이므로 당연히 y에 대한 선형결합으로 표현된다.\n",
    "\n",
    "또한, 절편이 있는 회귀에 대해서도 선형대수로 표현시 똑같이 보임을 알 수 있는데,\n",
    "\n",
    "$$\n",
    "\\widehat{y} = X^T (X^T X)^{-1} X y\n",
    "$$\n",
    "\n",
    "이므로 이 문제는 매우 자명하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4. When the number of features p is large, there tends to be a deterioration in the performance of KNN and other local approaches that perform prediction using only observations that are near the test observation for which a prediction must be made. This phenomenon is known as the curse of dimensionality, and it ties into the fact that parametric approaches often perform poorly when p is large. We will now investigate this curse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Suppose that we have a set of observations, each with measurements on p = 1 feature, X. We assume that X is uniformly (evenly) distributed on [0, 1]. Associated with each observation is a response value. Suppose that we wish to predict a test observation’s response using only observations that are within 10 % of the range of X closest to that test observation. For instance, in order to predict the response for a test observation with X = 0.6, we will use observations in the range [0.55, 0.65]. On average, what fraction of the available observations will we use to make the prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 percent (obvious)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Now suppose that we have a set of observations, each with measurements on p = 2 features, X1 and X2. We assume that (X1,X2) are uniformly distributed on [0, 1] × [0, 1]. We wish to predict a test  observation’s response using only observations that are within 10 % of the range of X1 and within 10 % of the range of X2 closest to that test observation. For instance, in order to predict the  esponse for a test observation with X1 = 0.6 and X2 = 0.35, we will use observations in the range [0.55, 0.65] for X1 and in the range [0.3, 0.4] for X2. On average, what fraction of the available  observations will we use to make the prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Now suppose that we have a set of observations on p = 100 features. Again the observations are uniformly distributed on each feature, and again each feature ranges in value from 0 to 1. We wish to predict a test observation’s response using observations within the 10 % of each feature’s range that is closest to that test observation. What fraction of the available observations will we use to make the prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$0.1^{100}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Using your answers to parts (a)–(c), argue that a drawback of KNN when p is large is that there are very few training observations “near” any given test observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "차원이 매우 크면 KNN에서 사실상 학습에 사용되는 데이터가 거의 없다시피해진다 (sparse problem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) Now suppose that we wish to make a prediction for a test observation by creating a p-dimensional hypercube centered around the test observation that contains, on average, 10 % of the training observations. For p = 1, 2, and 100, what is the length of each side of the hypercube? Comment on your answer. \n",
    "\n",
    "### Note: A hypercube is a generalization of a cube to an arbitrary number of dimensions. When p = 1, a hypercube is simply a line segment, when p = 2 it is a square, and when p = 100 it is a 100-dimensional cube."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 똑같음. $p=1$이면 0.1, $p=2$이면 $0.1^2$, $p=3$이면 $0.1^3$... 일반적인 $p$에 대해서는 $0.1^{100}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-5. We now examine the differences between LDA and QDA.\n",
    "\n",
    "### (a) If the Bayes decision boundary is linear, do we expect LDA or QDA to perform better on the training set? On the test set?\n",
    "\n",
    "training set에 대해서는 QDA가 무조건 더 낫고, test에 대해선 평균적으로는 LDA가 더 나을 것이라고 기대할 수 있다.\n",
    "\n",
    "### (b) If the Bayes decision boundary is non-linear, do we expect LDA or QDA to perform better on the training set? On the test set?\n",
    "\n",
    "training set에 대해서는 QDA가 무조건 더 낫고, test에 대해서도 평균적으로 QDA가 더 나을 것이라고 기대할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-6. Suppose we collect data for a group of students in a statistics class with variables X1 = hours studied, X2 = undergrad GPA, and Y = receive an A. We fit a logistic regression and produce estimated coefficient, $\\beta_0$ = −6, $\\beta_1$ = 0.05, $\\beta_2$ = 1. \n",
    "\n",
    "### (a) Estimate the probability that a student who studies for 40 h and has an undergrad GPA of 3.5 gets an A in the class. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37754066879814546"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "hours = 40\n",
    "gpa = 3.5\n",
    "eta = -6 + 0.05*hours + 1*gpa\n",
    "np.exp(eta) / (1 + np.exp(eta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서 약 38%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) How many hours would the student in part (a) need to study to have a 50 % chance of getting an A in the class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5011249981015413 50.08999999999799\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "hours = 40\n",
    "gpa = 3.5\n",
    "eta = -6 + 0.05*hours + 1*gpa\n",
    "phat = np.exp(eta) / (1 + np.exp(eta))\n",
    "eps = 0.001\n",
    "\n",
    "while phat - 0.5 < eps:\n",
    "    hours += 0.01\n",
    "    eta = -6 + 0.05*hours + 1*gpa\n",
    "    phat = np.exp(eta) / (1 + np.exp(eta))\n",
    "\n",
    "print(phat, hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "약 50시간정도 로동을 해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-2. We will now derive the probability that a given observation is part of a bootstrap sample. Suppose that we obtain a bootstrap sample from a set of n observations. \n",
    "\n",
    "### (a) What is the probability that the first bootstrap observation is not the jth observation from the original sample? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{n-1}{n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) What is the probability that the second bootstrap observation is not the jth observation from the original sample?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{n-1}{n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Argue that the probability that the jth observation is not in the bootstrap sample is $(1 − 1/n)^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "중복을 허용한 추출이므로 각 시행은 independent => n회 뽑으니까 당연히 $(\\frac{n-1}{n})^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) When n = 5, what is the probability that the jth observation is in the bootstrap sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6723199999999999"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=5\n",
    "1-((n-1)/n)**n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) When n = 100, what is the probability that the jth observation is in the bootstrap sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6339676587267709"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=100\n",
    "1-((n-1)/n)**n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f) When n = 10, 000, what is the probability that the jth observation is in the bootstrap sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6321389535670295"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=10000\n",
    "1-((n-1)/n)**n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (g) Create a plot that displays, for each integer value of n from 1 to 100, 000, the probability that the jth observation is in the bootstrap sample. Comment on what you observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "p_boots = []\n",
    "\n",
    "for n in range(1, 100000+1):\n",
    "    p_boots.append(1-((n-1)/n)**n)\n",
    "\n",
    "plt.plot(p_boots)\n",
    "plt.xlabel('n') ; plt.ylabel('prob_sample_in_boot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note. 사실은 $n \\rightarrow \\infty$이면, $j^{\\text{th}}$번째 샘플이 붓스트랩 샘플 내에 포함될 확률은 다음의 값으로 수렴한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6321205588285577"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "1 - np.exp(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (h) We will now investigate numerically the probability that a bootstrap sample of size $n = 100$ contains the jth observation. Here $j = 4$. We first create an array store with values that will subsequently be overwritten using the function np.empty(). We then repeatedly create bootstrap samples, and each time we record whether or not the fifth observation is contained in the bootstrap sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on the results obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6293"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 100 # sample size\n",
    "B = 10000 # iteration number\n",
    "\n",
    "cnts = np.zeros(B)\n",
    "for b in range(B):\n",
    "    bootsam = np.random.choice(range(1,101), size = 100, replace=True)\n",
    "    cnts[b] = ((bootsam == 4).sum() > 0)\n",
    "cnts.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대충 잘 수렴하는 걸로 치자..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-3. We now review k-fold cross-validation.\n",
    "\n",
    "### (a) Explain how k-fold cross-validation is implemented.\n",
    "\n",
    "데이터셋을 k뭉치로 나누어서 하나를 test로 쓰고 k-1개는 train으로 쓰고... 를 k번 반복하여 meaning한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### (b) What are the advantages and disadvantages of k-fold crossvalidation relative to:\n",
    "\n",
    "### i. The validation set approach?\n",
    "\n",
    "random sample에 대한 bias도 감소 + variance (uncertainty)도 크게 감소.\n",
    "\n",
    "### ii. LOOCV?\n",
    "\n",
    "random sample에 대한 bias는 살짝 증가하지만, variance (uncertainty)는 감소한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-1. We perform best subset, forward stepwise, and backward stepwise selection on a single data set. For each approach, we obtain p + 1 models, containing 0, 1, 2, . . . ,p predictors. Explain your answers:\n",
    "\n",
    "### (a) Which of the three models with k predictors has the smallest training RSS?\n",
    "\n",
    "동일할 수도 있고, 아니면 BSS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Which of the three models with k predictors has the smallest test RSS?\n",
    "\n",
    "BSS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) True or False:\n",
    "\n",
    "### i. The predictors in the k-variable model identified by forward stepwise are a subset of the predictors in the (k+1)-variable model identified by forward stepwise selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. The predictors in the k-variable model identified by backward stepwise are a subset of the predictors in the (k + 1)- variable model identified by backward stepwise selection.\n",
    "\n",
    "False, 반대임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii. The predictors in the k-variable model identified by backward stepwise are a subset of the predictors in the (k + 1)- variable model identified by forward stepwise selection.\n",
    "\n",
    "False. 꼭 그렇다고 할 순 없음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iv. The predictors in the k-variable model identified by forward stepwise are a subset of the predictors in the (k+1)-variable model identified by backward stepwise selection.\n",
    "\n",
    "False. 꼭 그렇다고 할 순 없음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v. The predictors in the k-variable model identified by best subset are a subset of the predictors in the (k + 1)-variable model identified by best subset selection.\n",
    "\n",
    "True. BSS는 fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-2. For parts (a) through (c), indicate which of i. through iv. is correct. Justify your answer.\n",
    "\n",
    "### (a) The lasso, relative to least squares, is:\n",
    "\n",
    "### i. More flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.\n",
    "\n",
    "### ii. More flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.\n",
    "\n",
    "### iii. Less flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.\n",
    "\n",
    "### iv. Less flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Repeat (a) for ridge regression relative to least squares.\n",
    "\n",
    "iii."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Repeat (a) for non-linear methods relative to least squares.\n",
    "\n",
    "ii."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-3. Suppose we estimate the regression coefficients in a linear regression model by minimizing\n",
    "$$\n",
    "\\sum_{i=1}^n\\left(y_i-\\beta_0-\\sum_{j=1}^p \\beta_j x_{i j}\\right)^2 \\quad \\text { subject to } \\quad \\sum_{j=1}^p\\left|\\beta_j\\right| \\leq s\n",
    "$$\n",
    "### for a particular value of s. For parts (a) through (e), indicate which of i. through v. is correct. Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) As we increase s from 0, the training RSS will:\n",
    "\n",
    "i. Increase initially, and then eventually start decreasing in an\n",
    "inverted U shape.\n",
    "\n",
    "ii. Decrease initially, and then eventually start increasing in a U shape.\n",
    "\n",
    "iii. Steadily increase.\n",
    "\n",
    "iv. Steadily decrease.\n",
    "\n",
    "v. Remain constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기에서는 s가 $\\lambda$의 역수역할을 하는 것이 아닌가?\n",
    "\n",
    "따라서 s가 증가한다는 것은 $\\lambda$가 감소한다는 것과 같으므로... OLS와 비슷해진다.\n",
    "\n",
    "iv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Repeat (a) for test RSS.\n",
    "\n",
    "lasso는 bias가 생기지만 variance가 그만큼 더 감소했다. 따라서 ii. \n",
    "\n",
    "최적의 지점에서 최소 test SSE를 갖다가, 그 다음부턴 서서히 증가한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Repeat (a) for variance.\n",
    "\n",
    "iii."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Repeat (a) for (squared) bias.\n",
    "\n",
    "iv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) Repeat (a) for the irreducible error.\n",
    "v."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-4. Suppose we estimate the regression coefficients in a linear regression model by minimizing\n",
    "$$\n",
    "\\sum_{i=1}^n\\left(y_i-\\beta_0-\\sum_{j=1}^p \\beta_j x_{i j}\\right)^2+\\lambda \\sum_{j=1}^p \\beta_j^2\n",
    "$$\n",
    "### for a particular value of s. For parts (a) through (e), indicate which of i. through v. is correct. Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) As we increase $\\lambda$ from 0, the training RSS will:\n",
    "\n",
    "i. Increase initially, and then eventually start decreasing in an\n",
    "inverted U shape.\n",
    "\n",
    "ii. Decrease initially, and then eventually start increasing in a U shape.\n",
    "\n",
    "iii. Steadily increase.\n",
    "\n",
    "iv. Steadily decrease.\n",
    "\n",
    "v. Remain constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번엔 s와 다르다. lambda가 늘어난다는 것은, OLS와 격차가 벌어진다는 것임.\n",
    "따라서 iii."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Repeat (a) for test RSS.\n",
    "\n",
    "ii. 최적의 지점에서 test SSE는 최저가 되다가, 다시 서서히 증가할거임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Repeat (a) for variance.\n",
    "\n",
    "iv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Repeat (a) for (squared) bias.\n",
    "iii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) Repeat (a) for the irreducible error.\n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7-5. Consider two curves, $\\widehat{g_1}$ and $\\widehat{g_2}$, defined by\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\hat{g}_1=\\arg \\min _g\\left(\\sum_{i=1}^n\\left(y_i-g\\left(x_i\\right)\\right)^2+\\lambda \\int\\left[g^{(3)}(x)\\right]^2 d x\\right), \\\\\n",
    "& \\hat{g}_2=\\arg \\min _g\\left(\\sum_{i=1}^n\\left(y_i-g\\left(x_i\\right)\\right)^2+\\lambda \\int\\left[g^{(4)}(x)\\right]^2 d x\\right),\n",
    "\\end{aligned}\n",
    "$$\n",
    "### where $g^{(m)}$ represents the mth derivative of g.\n",
    "\n",
    "### (a) As $\\lambda \\rightarrow \\infty$, will $\\widehat{g_1}$ or $\\widehat{g_2}$ have the smaller training RSS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) As $\\lambda \\rightarrow \\infty$, will $\\widehat{g_1}$ or $\\widehat{g_2}$ have the smaller test RSS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) As $\\lambda = 0$, will $\\widehat{g_1}$ or $\\widehat{g_2}$ have the smaller training and test RSS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8-5. Suppose we produce ten bootstrapped samples from a data set containing red and green classes. We then apply a classification tree to each bootstrapped sample and, for a specific value of X, produce 10 estimates of $P(\\text{Class is Red}|X)$:\n",
    "\n",
    "$$\\text{0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, and 0.75.}$$\n",
    "\n",
    "### There are two common ways to combine these results together into a single class prediction. One is the majority vote approach discussed in this chapter. The second approach is to classify based on he average probability. In this example, what is the final classification under each of these two approaches?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8-6. Provide a detailed explanation of the algorithm that is used to fit a regression tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9-3. Here we explore the maximal margin classifier on a toy data set. \n",
    "\n",
    "### (a) We are given $n = 7$ observations in $p = 2$ dimensions. For each observation, there is an associated class label.\n",
    "$$\n",
    "\\begin{array}{cccc}\n",
    "\\hline \\text { Obs. } & X_1 & X_2 & Y \\\\\n",
    "\\hline 1 & 3 & 4 & \\text { Red } \\\\\n",
    "2 & 2 & 2 & \\text { Red } \\\\\n",
    "3 & 4 & 4 & \\text { Red } \\\\\n",
    "4 & 1 & 4 & \\text { Red } \\\\\n",
    "5 & 2 & 1 & \\text { Blue } \\\\\n",
    "6 & 4 & 3 & \\text { Blue } \\\\\n",
    "7 & 4 & 1 & \\text { Blue } \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "### Sketch the observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Sketch the optimal separating hyperplane, and provide the equation for this hyperplane (of the form (9.1))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Describe the classification rule for the maximal margin classifier. It should be something along the lines of “Classify to Red if $\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 > 0$, and classify to Blue otherwise.” Provide the values for $\\beta_0, \\beta_1$, and $\\beta_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  (d) On your sketch, indicate the margin for the maximal margin hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) Indicate the support vectors for the maximal margin classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f) Argue that a slight movement of the seventh observation would not affect the maximal margin hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (g) Sketch a hyperplane that is not the optimal separating hyperplane, and provide the equation for this hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (h) Draw an additional observation on the plot so that the two classes are no longer separable by a hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "중요한 연습문제 번호\n",
    "\n",
    "챕터 2 : 1, 2, 5\n",
    "\n",
    "챕터 3 : 3, 4, 5\n",
    "\n",
    "챕터 4 : 4, 5, 6\n",
    "\n",
    "챕터 5 : 2, 3\n",
    "\n",
    "챕터 6 : 1, 2, 3, 4\n",
    "\n",
    "챕터 7 : 5\n",
    "\n",
    "챕터 8 : 5, 6\n",
    "\n",
    "챕터 9 : 3\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
