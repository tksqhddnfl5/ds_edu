{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/TirendazAcademy/PyTorch-Lightning-Tutorials/blob/main/Lightning_with_Tensorboard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yeOc9GJRN5kO"
   },
   "source": [
    "### What is Pytorch Lightning?\n",
    "![figure.png](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F8qhjh%2Fbtr5eobWvx3%2FXslpIFC0apO8lmSCUe8VVK%2Fimg.png)\n",
    "PyTorch Lightning is an open-source Python library that provides a high-level interface for PyTorch. While PyTorch alone is sufficient for easily creating various AI models, the code can become complex when experimenting under more advanced conditions such as using GPUs, TPUs, 16-bit precision, or distributed learning. To address this, PyTorch Lightning was developed as a project that abstracts the code, aiming to establish a unified coding style beyond just a framework.\n",
    "\n",
    "```python\n",
    "dataset = LightningDataset()\n",
    "model = LightningModel()\n",
    "trainer = pl.Trainer(max_epochs=10)\n",
    "trainer.fit(model=model, datamodule=dataset)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucGH1wfvN5kO"
   },
   "source": [
    "This tutorial is heavily inspired by great pytorch-lightning tutorials before, including:\n",
    "\n",
    "* [Pytorch lightning tutorials](https://lightning.ai/docs/pytorch/stable/tutorials.html?utm_source=chatgpt.com)\n",
    "* [Lightning examples](https://github.com/Lightning-AI/tutorials/tree/main/lightning_examples)\n",
    "* [Why You Should Use PyTorch Lightning and How to Get Started](https://www.sabrepc.com/blog/Deep-Learning-and-AI/why-use-pytorch-lightning)\n",
    "* [Beginner guide to pytorch-lightning](https://www.kaggle.com/code/shivanandmn/beginners-guide-to-pytorch-lightning/notebook)\n",
    "\n",
    "And the documentations:\n",
    "* [Pytorch lightning - Read the Docs](https://lightning.ai/docs/pytorch/LTS/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58iWrVhNzrjp"
   },
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qRcajzRczmW0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "from torchmetrics import Metric\n",
    "from pytorch_lightning.callbacks import EarlyStopping, Callback\n",
    "from torchvision.transforms import RandomHorizontalFlip, RandomVerticalFlip\n",
    "import torchvision\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import lightning as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "acg3mfQM0RpH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.4.1+cu121\n",
      "pytorch ligthening version: 2.5.0.post0\n"
     ]
    }
   ],
   "source": [
    "print(\"torch version:\",torch.__version__)\n",
    "print(\"pytorch ligthening version:\",pl.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QoxQnKId7yey"
   },
   "source": [
    "### Define a LightningModule\n",
    "A LightningModule enables your PyTorch nn.Module to play together in complex ways inside the training_step (there is also an optional validation_step and test_step).\n",
    "\n",
    "There are many reserved methods in the lighningmodules called hooks:\n",
    "\n",
    "- ```configure_optimizers``` - this should return optimizer(Adam/SGD)\n",
    "- ```training_step``` - training loop, takes batch and batch_idx as parameters\n",
    "- ```validation_step```-validation loop, takes batch and batch_idx as parameters\n",
    "- ```testing_step```- testing loop, takes batch and batch_idx as parameters\n",
    "\n",
    "\n",
    "```python\n",
    "class LightningModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        pass\n",
    "  \n",
    "    def configure_optimizers(self):\n",
    "        pass\n",
    "  \n",
    "    def loss_fn(self, output, target):\n",
    "        pass \n",
    "  \n",
    "    def training_step(self):\n",
    "        pass\n",
    "  \n",
    "    def validation_step(self):\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cNINOx4eN5kQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class MLP(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(1 * 28 * 28, 200)  # MNIST image size\n",
    "        self.layer2 = nn.Linear(200, 200)\n",
    "        self.layer3 = nn.Linear(200, 10)  # MNIST has 10 classes\n",
    "        self.relu = nn.ReLU()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()  # MNIST는 다중 분류 문제\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten MNIST image\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)  # Forward pass\n",
    "        loss = self.loss_fn(logits, y)  # Compute loss\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)  # 로그 저장\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=1e-3)  # Adam Optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ug9NUD7KN5kQ"
   },
   "source": [
    "Define the validation_step and test_step methods for your MLP model in PyTorch Lightning:\n",
    "- log \"val_loss\", \"val_acc\", \"test_loss\" and \"test_acc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QI48v9LIN5kQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class MLP(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(1 * 28 * 28, 200)  # MNIST image size\n",
    "        self.layer2 = nn.Linear(200, 200)\n",
    "        self.layer3 = nn.Linear(200, 10)  # MNIST has 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)  # Flatten, (B, 1*28*28)\n",
    "        x = F.relu(self.layer1(x))  # (B, 200)\n",
    "        x = F.relu(self.layer2(x))  # (B, 200)\n",
    "        x = self.layer3(x)  # (B, 10)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)  # Forward pass\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        acc = (logits.argmax(dim=1) == y).float().mean()  # Accuracy 계산\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        acc = (logits.argmax(dim=1) == y).float().mean()  # Accuracy 계산\n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        self.log(\"test_acc\", acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4Yz_7Yiz6i4"
   },
   "source": [
    "### Define a dataset\n",
    "Lightning supports ANY iterable (DataLoader, numpy, etc…) for the train/val/test/predict splits.\n",
    "\n",
    "Hooks:\n",
    "- ```train_dataloader()```\n",
    "- ```val_dataloader()```\n",
    "- ```test_dataloader()```\n",
    "Above methods in lightning datamodule are dataloaders\n",
    "\n",
    "- prepare_data(): Download and tokenize or do preprocessing on complete dataset, because this is called on single gpu if your using mulitple gpu, data here is not shared accross gpus.\n",
    "- setup(): splitting or transformations etc. setup takes stage argument None by default or fit or test for training and testing respectively.\n",
    "\n",
    "```python\n",
    "class LightningDataset(pl.LightningDataModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "  \n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "  \n",
    "    def setup(self, stage=None):\n",
    "        pass\n",
    "  \n",
    "    def train_dataloader(self):\n",
    "        pass\n",
    "  \n",
    "    def val_dataloader(self):\n",
    "        pass\n",
    "  \n",
    "    def test_dataloader(self):\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Wp0FJMvFz6Mp"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "class MnistDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir=\"D:/data\", batch_size=32, num_workers=4):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # MNIST 데이터 다운로드\n",
    "        datasets.MNIST(self.data_dir, train=True, download=True)\n",
    "        datasets.MNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # 데이터 변환 (augmentation 포함)\n",
    "        transform = transforms.Compose([\n",
    "            transforms.RandomVerticalFlip(p=0.1),\n",
    "            transforms.RandomHorizontalFlip(p=0.1),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "        # 전체 MNIST 데이터셋 로드\n",
    "        entire_dataset = datasets.MNIST(self.data_dir, train=True, transform=transform)\n",
    "        \n",
    "        # train/val 데이터셋 분할 (9:1 비율)\n",
    "        train_size = int(0.9 * len(entire_dataset))\n",
    "        val_size = len(entire_dataset) - train_size\n",
    "        self.train_ds, self.val_ds = random_split(entire_dataset, [train_size, val_size])\n",
    "\n",
    "        # 테스트 데이터셋\n",
    "        self.test_ds = datasets.MNIST(self.data_dir, train=False, transform=transforms.ToTensor())\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_ds, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_ds, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_ds, batch_size=self.batch_size, num_workers=self.num_workers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylzA5IAj74_L"
   },
   "source": [
    "### Callbacks\n",
    "PyTorch Lightning의 Callback 함수는 모델 학습 과정에서 특정 이벤트가 발생할 때 실행되는 사용자 정의 기능을 추가할 수 있도록 도와주는 강력한 도구입니다. 이를 통해 모델 학습, 검증, 예측 등의 과정에서 다양한 작업을 자동화할 수 있습니다.\n",
    "- Early Stopping (학습 조기 종료)\n",
    "- Model Checkpointing (최적의 모델 저장)\n",
    "- Logging & Visualization (TensorBoard, WandB 등의 로깅)\n",
    "- Learning Rate Scheduling (학습률 조정)\n",
    "- Custom Actions (모델 성능 평가, 추가적인 데이터 로깅 등)\n",
    "\n",
    "```\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    callbacks=[\n",
    "        early_stopping,\n",
    "        checkpoint_callback,\n",
    "        PrintLearningRateCallback()\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "1. [Built-in-callbacks](https://lightning.ai/docs/pytorch/stable/extensions/callbacks.html#built-in-callbacks)\n",
    "2. [Callback API](https://lightning.ai/docs/pytorch/stable/extensions/callbacks.html#callback-api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYxlHhCNN5kQ"
   },
   "source": [
    "#### 1️⃣ 간단한 Callback 예제\n",
    "\n",
    "다음은 학습 시작과 종료 시 로그를 출력하는 간단한 Callback 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "VT0Q2QHR01bW"
   },
   "outputs": [],
   "source": [
    "class MyPrintingCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def on_train_start(self, trainer, pl_module):\n",
    "        print(\"Starting to train!\")\n",
    "\n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        print(\"Training is done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ItCwnYW2N5kR"
   },
   "source": [
    "#### 2️⃣ Early Stopping Callback\n",
    "\n",
    "PyTorch Lightning에서는 EarlyStopping 콜백을 제공하여 모델의 성능이 개선되지 않으면 학습을 자동으로 종료할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "CRuV_fSlN5kR"
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",  # 모니터링할 값\n",
    "    patience=3,          # 개선이 없을 경우 종료할 epoch 수\n",
    "    verbose=True,\n",
    "    mode=\"min\"           # 최소값이 가장 좋은 경우로 설정 (loss는 낮을수록 좋음)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dd61vNZSN5kR"
   },
   "source": [
    "#### 3️⃣ Model Checkpointing Callback\n",
    "\n",
    "최고 성능을 보이는 모델을 저장하는 Callback도 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fMAvgpMxN5kR"
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",       # 기준이 되는 metric\n",
    "    dirpath=\"checkpoints/\",   # 저장 경로\n",
    "    filename=\"best-checkpoint\",  # 파일 이름\n",
    "    save_top_k=1,             # 가장 좋은 k개의 모델만 저장\n",
    "    mode=\"min\",               # loss가 낮을수록 좋은 모델로 판단\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lA15XpiVN5kR"
   },
   "source": [
    "#### 4️⃣ Practice: Custom Callback (사용자 정의)\n",
    "\n",
    "커스텀 Callback을 직접 만들 수도 있습니다.\n",
    "\n",
    "먼저, 모든 10번째 배치마다 현재 학습률을 출력하는 Callback 을 만들어보겠습니다. (Hint: on_train_batch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "139ert28N5kR"
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "class PrintLearningRateCallback(pl.Callback):\n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        # Optimizer에서 현재 학습률 가져오기\n",
    "        lr = trainer.optimizers[0].param_groups[0]['lr']\n",
    "        \n",
    "        # 10번째 배치마다 출력\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(f\"Batch {batch_idx + 1}: Learning Rate = {lr:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHC1hn5-N5kR"
   },
   "source": [
    "#### 5️⃣ Practice: Gradient Clipping (자동 그래디언트 클리핑)\n",
    "\n",
    "PyTorch Lightning에서는 trainer에 gradient_clip_val을 설정하면 그래디언트 클리핑을 할 수 있지만,\n",
    "커스텀 Callback을 사용하면 특정 조건에서만 적용할 수도 있습니다.\n",
    "\n",
    "1. [torch.clamp](https://pytorch.org/docs/stable/generated/torch.clamp.html#torch.clamp)\n",
    "2. [on_after_backward](https://lightning.ai/docs/pytorch/stable/extensions/callbacks.html#on-after-backward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xUp-5WTwN5kR"
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch.nn.utils as nn_utils\n",
    "\n",
    "class GradientClippingCallback(pl.Callback):\n",
    "    def __init__(self, clip_value=0.5):\n",
    "        super().__init__()\n",
    "        self.clip_value = clip_value\n",
    "\n",
    "    def on_after_backward(self, trainer, pl_module):\n",
    "        # 모든 파라미터의 그래디언트를 clip_value 이하로 클리핑\n",
    "        nn_utils.clip_grad_norm_(pl_module.parameters(), self.clip_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6TipRFFNN5kR"
   },
   "source": [
    "#### 6️⃣ Practice: Epoch 별 학습 시간 측정 Callback\n",
    "각 epoch이 끝날 때마다 소요 시간을 측정하여 출력하는 Callback입니다.\n",
    "\n",
    "1. [on_train_epoch_start](https://lightning.ai/docs/pytorch/stable/extensions/callbacks.html#on-train-epoch-start)\n",
    "2. [on_train_epoch_end](https://lightning.ai/docs/pytorch/stable/extensions/callbacks.html#on-train-epoch-end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "UWZ3o9hfN5kR"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class TimerCallback(pl.Callback):\n",
    "    def on_train_epoch_start(self, trainer, pl_module):\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        elapsed_time = time.time() - self.epoch_start_time\n",
    "        print(f\"Epoch {trainer.current_epoch} 소요 시간: {elapsed_time:.2f}초\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2VpT32078wF"
   },
   "source": [
    "# Setting the hyperparameters\n",
    "PyTorch Lightning에서는 여러 가지 하이퍼파라미터를 제공합니다.\n",
    "- [Trainer flags](https://lightning.ai/docs/pytorch/stable/common/trainer.html#trainer-flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "BEZnC9qvN5kR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "model = MLP()\n",
    "dm = MnistDataModule(\n",
    "    data_dir=\"D:/data\",\n",
    "    batch_size=100,\n",
    "    num_workers=4,\n",
    ")\n",
    "trainer = pl.Trainer(\n",
    "    logger=TensorBoardLogger(\"tb_logs\", name=\"mnist_model_v0\"),\n",
    "    accelerator=\"gpu\",\n",
    "    devices=[0],\n",
    "    min_epochs=1,\n",
    "    max_epochs=2,\n",
    "    callbacks=[PrintLearningRateCallback(), GradientClippingCallback(), TimerCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZEGk1568AFi"
   },
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "EHAqulDF0tom"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type   | Params | Mode \n",
      "------------------------------------------\n",
      "0 | layer1 | Linear | 157 K  | train\n",
      "1 | layer2 | Linear | 40.2 K | train\n",
      "2 | layer3 | Linear | 2.0 K  | train\n",
      "------------------------------------------\n",
      "199 K     Trainable params\n",
      "0         Non-trainable params\n",
      "199 K     Total params\n",
      "0.797     Total estimated model params size (MB)\n",
      "3         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   2%|▏         | 9/540 [00:00<00:10, 51.10it/s, v_num=1, train_loss=2.080]Batch 10: Learning Rate = 0.001000\n",
      "Epoch 0:   4%|▎         | 19/540 [00:00<00:06, 83.65it/s, v_num=1, train_loss=1.410]Batch 20: Learning Rate = 0.001000\n",
      "Epoch 0:   5%|▌         | 29/540 [00:00<00:04, 104.27it/s, v_num=1, train_loss=1.000]Batch 30: Learning Rate = 0.001000\n",
      "Epoch 0:   7%|▋         | 39/540 [00:00<00:04, 117.62it/s, v_num=1, train_loss=0.732]Batch 40: Learning Rate = 0.001000\n",
      "Epoch 0:   9%|▉         | 49/540 [00:00<00:03, 128.75it/s, v_num=1, train_loss=0.668]Batch 50: Learning Rate = 0.001000\n",
      "Epoch 0:  11%|█         | 59/540 [00:00<00:03, 135.14it/s, v_num=1, train_loss=0.736]Batch 60: Learning Rate = 0.001000\n",
      "Epoch 0:  13%|█▎        | 69/540 [00:00<00:03, 139.23it/s, v_num=1, train_loss=0.811]Batch 70: Learning Rate = 0.001000\n",
      "Epoch 0:  15%|█▍        | 79/540 [00:00<00:03, 143.75it/s, v_num=1, train_loss=0.753]Batch 80: Learning Rate = 0.001000\n",
      "Epoch 0:  16%|█▋        | 89/540 [00:00<00:03, 148.68it/s, v_num=1, train_loss=0.715]Batch 90: Learning Rate = 0.001000\n",
      "Epoch 0:  18%|█▊        | 99/540 [00:00<00:02, 153.35it/s, v_num=1, train_loss=0.588]Batch 100: Learning Rate = 0.001000\n",
      "Epoch 0:  20%|██        | 109/540 [00:00<00:02, 156.25it/s, v_num=1, train_loss=0.463]Batch 110: Learning Rate = 0.001000\n",
      "Epoch 0:  22%|██▏       | 119/540 [00:00<00:02, 159.39it/s, v_num=1, train_loss=0.661]Batch 120: Learning Rate = 0.001000\n",
      "Epoch 0:  24%|██▍       | 129/540 [00:00<00:02, 162.35it/s, v_num=1, train_loss=0.578]Batch 130: Learning Rate = 0.001000\n",
      "Epoch 0:  26%|██▌       | 139/540 [00:00<00:02, 165.36it/s, v_num=1, train_loss=0.515]Batch 140: Learning Rate = 0.001000\n",
      "Epoch 0:  28%|██▊       | 149/540 [00:00<00:02, 168.82it/s, v_num=1, train_loss=0.760]Batch 150: Learning Rate = 0.001000\n",
      "Epoch 0:  29%|██▉       | 159/540 [00:00<00:02, 172.15it/s, v_num=1, train_loss=0.579]Batch 160: Learning Rate = 0.001000\n",
      "Epoch 0:  31%|███▏      | 169/540 [00:00<00:02, 174.59it/s, v_num=1, train_loss=0.574]Batch 170: Learning Rate = 0.001000\n",
      "Epoch 0:  33%|███▎      | 179/540 [00:01<00:02, 177.94it/s, v_num=1, train_loss=0.718]Batch 180: Learning Rate = 0.001000\n",
      "Epoch 0:  35%|███▌      | 189/540 [00:01<00:01, 179.49it/s, v_num=1, train_loss=0.635]Batch 190: Learning Rate = 0.001000\n",
      "Epoch 0:  37%|███▋      | 199/540 [00:01<00:01, 181.16it/s, v_num=1, train_loss=0.581]Batch 200: Learning Rate = 0.001000\n",
      "Epoch 0:  39%|███▊      | 209/540 [00:01<00:01, 180.25it/s, v_num=1, train_loss=0.514]Batch 210: Learning Rate = 0.001000\n",
      "Epoch 0:  41%|████      | 219/540 [00:01<00:01, 180.62it/s, v_num=1, train_loss=0.461]Batch 220: Learning Rate = 0.001000\n",
      "Epoch 0:  42%|████▏     | 229/540 [00:01<00:01, 182.98it/s, v_num=1, train_loss=0.471]Batch 230: Learning Rate = 0.001000\n",
      "Epoch 0:  44%|████▍     | 239/540 [00:01<00:01, 185.29it/s, v_num=1, train_loss=0.397]Batch 240: Learning Rate = 0.001000\n",
      "Epoch 0:  46%|████▌     | 249/540 [00:01<00:01, 187.10it/s, v_num=1, train_loss=0.562]Batch 250: Learning Rate = 0.001000\n",
      "Epoch 0:  48%|████▊     | 259/540 [00:01<00:01, 188.25it/s, v_num=1, train_loss=0.547]Batch 260: Learning Rate = 0.001000\n",
      "Epoch 0:  50%|████▉     | 269/540 [00:01<00:01, 189.59it/s, v_num=1, train_loss=0.452]Batch 270: Learning Rate = 0.001000\n",
      "Epoch 0:  52%|█████▏    | 279/540 [00:01<00:01, 191.11it/s, v_num=1, train_loss=0.355]Batch 280: Learning Rate = 0.001000\n",
      "Epoch 0:  54%|█████▎    | 289/540 [00:01<00:01, 192.49it/s, v_num=1, train_loss=0.373]Batch 290: Learning Rate = 0.001000\n",
      "Epoch 0:  55%|█████▌    | 299/540 [00:01<00:01, 193.98it/s, v_num=1, train_loss=0.518]Batch 300: Learning Rate = 0.001000\n",
      "Epoch 0:  57%|█████▋    | 309/540 [00:01<00:01, 194.42it/s, v_num=1, train_loss=0.418]Batch 310: Learning Rate = 0.001000\n",
      "Epoch 0:  59%|█████▉    | 319/540 [00:01<00:01, 195.66it/s, v_num=1, train_loss=0.289]Batch 320: Learning Rate = 0.001000\n",
      "Epoch 0:  61%|██████    | 329/540 [00:01<00:01, 196.73it/s, v_num=1, train_loss=0.363]Batch 330: Learning Rate = 0.001000\n",
      "Epoch 0:  63%|██████▎   | 339/540 [00:01<00:01, 197.86it/s, v_num=1, train_loss=0.327]Batch 340: Learning Rate = 0.001000\n",
      "Epoch 0:  65%|██████▍   | 349/540 [00:01<00:00, 198.37it/s, v_num=1, train_loss=0.465]Batch 350: Learning Rate = 0.001000\n",
      "Epoch 0:  66%|██████▋   | 359/540 [00:01<00:00, 198.85it/s, v_num=1, train_loss=0.412]Batch 360: Learning Rate = 0.001000\n",
      "Epoch 0:  68%|██████▊   | 369/540 [00:01<00:00, 199.92it/s, v_num=1, train_loss=0.305]Batch 370: Learning Rate = 0.001000\n",
      "Epoch 0:  70%|███████   | 379/540 [00:01<00:00, 201.41it/s, v_num=1, train_loss=0.321]Batch 380: Learning Rate = 0.001000\n",
      "Epoch 0:  72%|███████▏  | 389/540 [00:01<00:00, 200.24it/s, v_num=1, train_loss=0.369]Batch 390: Learning Rate = 0.001000\n",
      "Epoch 0:  74%|███████▍  | 399/540 [00:01<00:00, 200.43it/s, v_num=1, train_loss=0.450]Batch 400: Learning Rate = 0.001000\n",
      "Epoch 0:  76%|███████▌  | 409/540 [00:02<00:00, 200.91it/s, v_num=1, train_loss=0.426]Batch 410: Learning Rate = 0.001000\n",
      "Epoch 0:  78%|███████▊  | 419/540 [00:02<00:00, 201.66it/s, v_num=1, train_loss=0.299]Batch 420: Learning Rate = 0.001000\n",
      "Epoch 0:  79%|███████▉  | 429/540 [00:02<00:00, 202.58it/s, v_num=1, train_loss=0.377]Batch 430: Learning Rate = 0.001000\n",
      "Epoch 0:  81%|████████▏ | 439/540 [00:02<00:00, 203.36it/s, v_num=1, train_loss=0.280]Batch 440: Learning Rate = 0.001000\n",
      "Epoch 0:  83%|████████▎ | 449/540 [00:02<00:00, 204.30it/s, v_num=1, train_loss=0.284]Batch 450: Learning Rate = 0.001000\n",
      "Epoch 0:  85%|████████▌ | 459/540 [00:02<00:00, 204.85it/s, v_num=1, train_loss=0.378]Batch 460: Learning Rate = 0.001000\n",
      "Epoch 0:  87%|████████▋ | 469/540 [00:02<00:00, 205.64it/s, v_num=1, train_loss=0.359]Batch 470: Learning Rate = 0.001000\n",
      "Epoch 0:  89%|████████▊ | 479/540 [00:02<00:00, 206.49it/s, v_num=1, train_loss=0.363]Batch 480: Learning Rate = 0.001000\n",
      "Epoch 0:  91%|█████████ | 489/540 [00:02<00:00, 206.94it/s, v_num=1, train_loss=0.309]Batch 490: Learning Rate = 0.001000\n",
      "Epoch 0:  92%|█████████▏| 499/540 [00:02<00:00, 207.48it/s, v_num=1, train_loss=0.373]Batch 500: Learning Rate = 0.001000\n",
      "Epoch 0:  94%|█████████▍| 509/540 [00:02<00:00, 208.09it/s, v_num=1, train_loss=0.372]Batch 510: Learning Rate = 0.001000\n",
      "Epoch 0:  96%|█████████▌| 519/540 [00:02<00:00, 208.43it/s, v_num=1, train_loss=0.189]Batch 520: Learning Rate = 0.001000\n",
      "Epoch 0:  98%|█████████▊| 529/540 [00:02<00:00, 207.69it/s, v_num=1, train_loss=0.203]Batch 530: Learning Rate = 0.001000\n",
      "Epoch 0: 100%|█████████▉| 539/540 [00:02<00:00, 208.02it/s, v_num=1, train_loss=0.317]Batch 540: Learning Rate = 0.001000\n",
      "Epoch 0: 100%|██████████| 540/540 [00:04<00:00, 110.09it/s, v_num=1, train_loss=0.196, val_loss=0.311, val_acc=0.909]Epoch 0 소요 시간: 4.91초\n",
      "Epoch 1:   2%|▏         | 9/540 [00:07<06:58,  1.27it/s, v_num=1, train_loss=0.238, val_loss=0.311, val_acc=0.909]   Batch 10: Learning Rate = 0.001000\n",
      "Epoch 1:   4%|▎         | 19/540 [00:07<03:15,  2.66it/s, v_num=1, train_loss=0.260, val_loss=0.311, val_acc=0.909]Batch 20: Learning Rate = 0.001000\n",
      "Epoch 1:   5%|▌         | 29/540 [00:07<02:06,  4.03it/s, v_num=1, train_loss=0.190, val_loss=0.311, val_acc=0.909]Batch 30: Learning Rate = 0.001000\n",
      "Epoch 1:   7%|▋         | 39/540 [00:07<01:33,  5.38it/s, v_num=1, train_loss=0.245, val_loss=0.311, val_acc=0.909]Batch 40: Learning Rate = 0.001000\n",
      "Epoch 1:   9%|▉         | 49/540 [00:07<01:13,  6.70it/s, v_num=1, train_loss=0.335, val_loss=0.311, val_acc=0.909]Batch 50: Learning Rate = 0.001000\n",
      "Epoch 1:  11%|█         | 59/540 [00:07<01:00,  8.01it/s, v_num=1, train_loss=0.216, val_loss=0.311, val_acc=0.909]Batch 60: Learning Rate = 0.001000\n",
      "Epoch 1:  13%|█▎        | 69/540 [00:07<00:50,  9.28it/s, v_num=1, train_loss=0.223, val_loss=0.311, val_acc=0.909]Batch 70: Learning Rate = 0.001000\n",
      "Epoch 1:  15%|█▍        | 79/540 [00:07<00:43, 10.55it/s, v_num=1, train_loss=0.248, val_loss=0.311, val_acc=0.909]Batch 80: Learning Rate = 0.001000\n",
      "Epoch 1:  16%|█▋        | 89/540 [00:07<00:38, 11.80it/s, v_num=1, train_loss=0.325, val_loss=0.311, val_acc=0.909]Batch 90: Learning Rate = 0.001000\n",
      "Epoch 1:  18%|█▊        | 99/540 [00:07<00:33, 13.04it/s, v_num=1, train_loss=0.350, val_loss=0.311, val_acc=0.909]Batch 100: Learning Rate = 0.001000\n",
      "Epoch 1:  20%|██        | 109/540 [00:07<00:30, 14.26it/s, v_num=1, train_loss=0.175, val_loss=0.311, val_acc=0.909]Batch 110: Learning Rate = 0.001000\n",
      "Epoch 1:  22%|██▏       | 119/540 [00:07<00:27, 15.47it/s, v_num=1, train_loss=0.372, val_loss=0.311, val_acc=0.909]Batch 120: Learning Rate = 0.001000\n",
      "Epoch 1:  24%|██▍       | 129/540 [00:07<00:24, 16.66it/s, v_num=1, train_loss=0.371, val_loss=0.311, val_acc=0.909]Batch 130: Learning Rate = 0.001000\n",
      "Epoch 1:  26%|██▌       | 139/540 [00:07<00:22, 17.84it/s, v_num=1, train_loss=0.118, val_loss=0.311, val_acc=0.909]Batch 140: Learning Rate = 0.001000\n",
      "Epoch 1:  28%|██▊       | 149/540 [00:07<00:20, 18.99it/s, v_num=1, train_loss=0.177, val_loss=0.311, val_acc=0.909]Batch 150: Learning Rate = 0.001000\n",
      "Epoch 1:  29%|██▉       | 159/540 [00:07<00:18, 20.16it/s, v_num=1, train_loss=0.279, val_loss=0.311, val_acc=0.909]Batch 160: Learning Rate = 0.001000\n",
      "Epoch 1:  31%|███▏      | 169/540 [00:07<00:17, 21.28it/s, v_num=1, train_loss=0.228, val_loss=0.311, val_acc=0.909]Batch 170: Learning Rate = 0.001000\n",
      "Epoch 1:  33%|███▎      | 179/540 [00:08<00:16, 22.37it/s, v_num=1, train_loss=0.205, val_loss=0.311, val_acc=0.909]Batch 180: Learning Rate = 0.001000\n",
      "Epoch 1:  35%|███▌      | 189/540 [00:08<00:14, 23.49it/s, v_num=1, train_loss=0.299, val_loss=0.311, val_acc=0.909]Batch 190: Learning Rate = 0.001000\n",
      "Epoch 1:  37%|███▋      | 199/540 [00:08<00:13, 24.59it/s, v_num=1, train_loss=0.265, val_loss=0.311, val_acc=0.909]Batch 200: Learning Rate = 0.001000\n",
      "Epoch 1:  39%|███▊      | 209/540 [00:08<00:12, 25.70it/s, v_num=1, train_loss=0.293, val_loss=0.311, val_acc=0.909]Batch 210: Learning Rate = 0.001000\n",
      "Epoch 1:  41%|████      | 219/540 [00:08<00:11, 26.78it/s, v_num=1, train_loss=0.223, val_loss=0.311, val_acc=0.909]Batch 220: Learning Rate = 0.001000\n",
      "Epoch 1:  42%|████▏     | 229/540 [00:08<00:11, 27.86it/s, v_num=1, train_loss=0.187, val_loss=0.311, val_acc=0.909]Batch 230: Learning Rate = 0.001000\n",
      "Epoch 1:  44%|████▍     | 239/540 [00:08<00:10, 28.94it/s, v_num=1, train_loss=0.132, val_loss=0.311, val_acc=0.909]Batch 240: Learning Rate = 0.001000\n",
      "Epoch 1:  46%|████▌     | 249/540 [00:08<00:09, 29.98it/s, v_num=1, train_loss=0.394, val_loss=0.311, val_acc=0.909]Batch 250: Learning Rate = 0.001000\n",
      "Epoch 1:  48%|████▊     | 259/540 [00:08<00:09, 30.96it/s, v_num=1, train_loss=0.300, val_loss=0.311, val_acc=0.909]Batch 260: Learning Rate = 0.001000\n",
      "Epoch 1:  50%|████▉     | 269/540 [00:08<00:08, 32.00it/s, v_num=1, train_loss=0.307, val_loss=0.311, val_acc=0.909]Batch 270: Learning Rate = 0.001000\n",
      "Epoch 1:  52%|█████▏    | 279/540 [00:08<00:07, 33.02it/s, v_num=1, train_loss=0.294, val_loss=0.311, val_acc=0.909]Batch 280: Learning Rate = 0.001000\n",
      "Epoch 1:  54%|█████▎    | 289/540 [00:08<00:07, 34.01it/s, v_num=1, train_loss=0.340, val_loss=0.311, val_acc=0.909]Batch 290: Learning Rate = 0.001000\n",
      "Epoch 1:  55%|█████▌    | 299/540 [00:08<00:06, 35.02it/s, v_num=1, train_loss=0.237, val_loss=0.311, val_acc=0.909] Batch 300: Learning Rate = 0.001000\n",
      "Epoch 1:  57%|█████▋    | 309/540 [00:08<00:06, 36.00it/s, v_num=1, train_loss=0.245, val_loss=0.311, val_acc=0.909] Batch 310: Learning Rate = 0.001000\n",
      "Epoch 1:  59%|█████▉    | 319/540 [00:08<00:05, 36.96it/s, v_num=1, train_loss=0.277, val_loss=0.311, val_acc=0.909]Batch 320: Learning Rate = 0.001000\n",
      "Epoch 1:  61%|██████    | 329/540 [00:08<00:05, 37.92it/s, v_num=1, train_loss=0.205, val_loss=0.311, val_acc=0.909]Batch 330: Learning Rate = 0.001000\n",
      "Epoch 1:  63%|██████▎   | 339/540 [00:08<00:05, 38.82it/s, v_num=1, train_loss=0.303, val_loss=0.311, val_acc=0.909]Batch 340: Learning Rate = 0.001000\n",
      "Epoch 1:  65%|██████▍   | 349/540 [00:08<00:04, 39.71it/s, v_num=1, train_loss=0.192, val_loss=0.311, val_acc=0.909]Batch 350: Learning Rate = 0.001000\n",
      "Epoch 1:  66%|██████▋   | 359/540 [00:08<00:04, 40.64it/s, v_num=1, train_loss=0.273, val_loss=0.311, val_acc=0.909]Batch 360: Learning Rate = 0.001000\n",
      "Epoch 1:  68%|██████▊   | 369/540 [00:08<00:04, 41.55it/s, v_num=1, train_loss=0.116, val_loss=0.311, val_acc=0.909] Batch 370: Learning Rate = 0.001000\n",
      "Epoch 1:  70%|███████   | 379/540 [00:08<00:03, 42.45it/s, v_num=1, train_loss=0.277, val_loss=0.311, val_acc=0.909]Batch 380: Learning Rate = 0.001000\n",
      "Epoch 1:  72%|███████▏  | 389/540 [00:08<00:03, 43.36it/s, v_num=1, train_loss=0.180, val_loss=0.311, val_acc=0.909]Batch 390: Learning Rate = 0.001000\n",
      "Epoch 1:  74%|███████▍  | 399/540 [00:09<00:03, 44.25it/s, v_num=1, train_loss=0.243, val_loss=0.311, val_acc=0.909]Batch 400: Learning Rate = 0.001000\n",
      "Epoch 1:  76%|███████▌  | 409/540 [00:09<00:02, 45.13it/s, v_num=1, train_loss=0.125, val_loss=0.311, val_acc=0.909]Batch 410: Learning Rate = 0.001000\n",
      "Epoch 1:  78%|███████▊  | 419/540 [00:09<00:02, 45.92it/s, v_num=1, train_loss=0.209, val_loss=0.311, val_acc=0.909]Batch 420: Learning Rate = 0.001000\n",
      "Epoch 1:  79%|███████▉  | 429/540 [00:09<00:02, 46.79it/s, v_num=1, train_loss=0.386, val_loss=0.311, val_acc=0.909]Batch 430: Learning Rate = 0.001000\n",
      "Epoch 1:  81%|████████▏ | 439/540 [00:09<00:02, 47.65it/s, v_num=1, train_loss=0.260, val_loss=0.311, val_acc=0.909] Batch 440: Learning Rate = 0.001000\n",
      "Epoch 1:  83%|████████▎ | 449/540 [00:09<00:01, 48.53it/s, v_num=1, train_loss=0.251, val_loss=0.311, val_acc=0.909] Batch 450: Learning Rate = 0.001000\n",
      "Epoch 1:  85%|████████▌ | 459/540 [00:09<00:01, 49.39it/s, v_num=1, train_loss=0.258, val_loss=0.311, val_acc=0.909]Batch 460: Learning Rate = 0.001000\n",
      "Epoch 1:  87%|████████▋ | 469/540 [00:09<00:01, 50.19it/s, v_num=1, train_loss=0.358, val_loss=0.311, val_acc=0.909]Batch 470: Learning Rate = 0.001000\n",
      "Epoch 1:  89%|████████▊ | 479/540 [00:09<00:01, 51.03it/s, v_num=1, train_loss=0.332, val_loss=0.311, val_acc=0.909]Batch 480: Learning Rate = 0.001000\n",
      "Epoch 1:  91%|█████████ | 489/540 [00:09<00:00, 51.78it/s, v_num=1, train_loss=0.215, val_loss=0.311, val_acc=0.909] Batch 490: Learning Rate = 0.001000\n",
      "Epoch 1:  92%|█████████▏| 499/540 [00:09<00:00, 52.52it/s, v_num=1, train_loss=0.171, val_loss=0.311, val_acc=0.909]Batch 500: Learning Rate = 0.001000\n",
      "Epoch 1:  94%|█████████▍| 509/540 [00:09<00:00, 53.34it/s, v_num=1, train_loss=0.144, val_loss=0.311, val_acc=0.909]Batch 510: Learning Rate = 0.001000\n",
      "Epoch 1:  96%|█████████▌| 519/540 [00:09<00:00, 54.10it/s, v_num=1, train_loss=0.306, val_loss=0.311, val_acc=0.909] Batch 520: Learning Rate = 0.001000\n",
      "Epoch 1:  98%|█████████▊| 529/540 [00:09<00:00, 54.85it/s, v_num=1, train_loss=0.305, val_loss=0.311, val_acc=0.909]Batch 530: Learning Rate = 0.001000\n",
      "Epoch 1: 100%|█████████▉| 539/540 [00:09<00:00, 55.64it/s, v_num=1, train_loss=0.203, val_loss=0.311, val_acc=0.909]Batch 540: Learning Rate = 0.001000\n",
      "Epoch 1: 100%|██████████| 540/540 [00:12<00:00, 44.66it/s, v_num=1, train_loss=0.179, val_loss=0.225, val_acc=0.931]Epoch 1 소요 시간: 12.09초\n",
      "Epoch 1: 100%|██████████| 540/540 [00:12<00:00, 44.66it/s, v_num=1, train_loss=0.179, val_loss=0.225, val_acc=0.931]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 540/540 [00:12<00:00, 44.61it/s, v_num=1, train_loss=0.179, val_loss=0.225, val_acc=0.931]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 60/60 [00:00<00:00, 296.01it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "         val_acc            0.9396666884422302\n",
      "        val_loss            0.19124844670295715\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'test_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 100/100 [00:00<00:00, 493.82it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.9559999704360962\n",
      "        test_loss           0.1369308978319168\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.1369308978319168, 'test_acc': 0.9559999704360962}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model, dm)\n",
    "trainer.validate(model, dm)\n",
    "trainer.test(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "CCDg4k_c1wYu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 19084), started 0:00:07 ago. (Use '!kill 19084' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-62be3be17242a108\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-62be3be17242a108\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir tb_logs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
