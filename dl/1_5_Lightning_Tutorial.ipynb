{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/TirendazAcademy/PyTorch-Lightning-Tutorials/blob/main/Lightning_with_Tensorboard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yeOc9GJRN5kO"
   },
   "source": [
    "### What is Pytorch Lightning?\n",
    "![figure.png](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F8qhjh%2Fbtr5eobWvx3%2FXslpIFC0apO8lmSCUe8VVK%2Fimg.png)\n",
    "PyTorch Lightning is an open-source Python library that provides a high-level interface for PyTorch. While PyTorch alone is sufficient for easily creating various AI models, the code can become complex when experimenting under more advanced conditions such as using GPUs, TPUs, 16-bit precision, or distributed learning. To address this, PyTorch Lightning was developed as a project that abstracts the code, aiming to establish a unified coding style beyond just a framework.\n",
    "\n",
    "```python\n",
    "dataset = LightningDataset()\n",
    "model = LightningModel()\n",
    "trainer = pl.Trainer(max_epochs=10)\n",
    "trainer.fit(model=model, datamodule=dataset)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucGH1wfvN5kO"
   },
   "source": [
    "This tutorial is heavily inspired by great pytorch-lightning tutorials before, including:\n",
    "\n",
    "* [Pytorch lightning tutorials](https://lightning.ai/docs/pytorch/stable/tutorials.html?utm_source=chatgpt.com)\n",
    "* [Lightning examples](https://github.com/Lightning-AI/tutorials/tree/main/lightning_examples)\n",
    "* [Why You Should Use PyTorch Lightning and How to Get Started](https://www.sabrepc.com/blog/Deep-Learning-and-AI/why-use-pytorch-lightning)\n",
    "* [Beginner guide to pytorch-lightning](https://www.kaggle.com/code/shivanandmn/beginners-guide-to-pytorch-lightning/notebook)\n",
    "\n",
    "And the documentations:\n",
    "* [Pytorch lightning - Read the Docs](https://lightning.ai/docs/pytorch/LTS/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58iWrVhNzrjp"
   },
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4631,
     "status": "ok",
     "timestamp": 1738676996008,
     "user": {
      "displayName": "Seokwon Song",
      "userId": "09646560012447147254"
     },
     "user_tz": -540
    },
    "id": "N3aAQYw4zZSe",
    "outputId": "36de8711-a3f2-4cd4-d519-8b0f25fcd4b4"
   },
   "outputs": [],
   "source": [
    "!pip install lightning -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qRcajzRczmW0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "from torchmetrics import Metric\n",
    "from pytorch_lightning.callbacks import EarlyStopping, Callback\n",
    "from torchvision.transforms import RandomHorizontalFlip, RandomVerticalFlip\n",
    "import torchvision\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import lightning as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "acg3mfQM0RpH"
   },
   "outputs": [],
   "source": [
    "print(\"torch version:\",torch.__version__)\n",
    "print(\"pytorch ligthening version:\",pl.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QoxQnKId7yey"
   },
   "source": [
    "### Define a LightningModule\n",
    "A LightningModule enables your PyTorch nn.Module to play together in complex ways inside the training_step (there is also an optional validation_step and test_step).\n",
    "\n",
    "There are many reserved methods in the lighningmodules called hooks:\n",
    "\n",
    "- ```configure_optimizers``` - this should return optimizer(Adam/SGD)\n",
    "- ```training_step``` - training loop, takes batch and batch_idx as parameters\n",
    "- ```validation_step```-validation loop, takes batch and batch_idx as parameters\n",
    "- ```testing_step```- testing loop, takes batch and batch_idx as parameters\n",
    "\n",
    "\n",
    "```python\n",
    "class LightningModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        pass\n",
    "  \n",
    "    def configure_optimizers(self):\n",
    "        pass\n",
    "  \n",
    "    def loss_fn(self, output, target):\n",
    "        pass \n",
    "  \n",
    "    def training_step(self):\n",
    "        pass\n",
    "  \n",
    "    def validation_step(self):\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNINOx4eN5kQ"
   },
   "outputs": [],
   "source": [
    "class MLP(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(1 * 28 * 28, 200)  # MNIST image size\n",
    "        self.layer2 = nn.Linear(200, 200)\n",
    "        self.layer3 = nn.Linear(200, 10)  # MNIST has 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Implement forward pass\n",
    "        ...\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # 2. Implement training step\n",
    "        ...\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # 3. Define optimizer\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ug9NUD7KN5kQ"
   },
   "source": [
    "Define the validation_step and test_step methods for your MLP model in PyTorch Lightning:\n",
    "- log \"val_loss\", \"val_acc\", \"test_loss\" and \"test_acc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QI48v9LIN5kQ"
   },
   "outputs": [],
   "source": [
    "class MLP(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(1 * 28 * 28, 200)  # MNIST image size\n",
    "        self.layer2 = nn.Linear(200, 200)\n",
    "        self.layer3 = nn.Linear(200, 10)  # MNIST has 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)  # flatten, (B, 1*28*28)\n",
    "        x = F.relu(self.layer1(x))  # (B, 200)\n",
    "        x = F.relu(self.layer2(x))  # (B, 200)\n",
    "        x = self.layer3(x)  # (B, 10)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)  # Forward pass\n",
    "        loss = nn.functional.cross_entropy(logits, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    # 1. Implement the validation_step\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        ...\n",
    "\n",
    "    # 2. Implement the test_step\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4Yz_7Yiz6i4"
   },
   "source": [
    "### Define a dataset\n",
    "Lightning supports ANY iterable (DataLoader, numpy, etc…) for the train/val/test/predict splits.\n",
    "\n",
    "Hooks:\n",
    "- ```train_dataloader()```\n",
    "- ```val_dataloader()```\n",
    "- ```test_dataloader()```\n",
    "Above methods in lightning datamodule are dataloaders\n",
    "\n",
    "- prepare_data(): Download and tokenize or do preprocessing on complete dataset, because this is called on single gpu if your using mulitple gpu, data here is not shared accross gpus.\n",
    "- setup(): splitting or transformations etc. setup takes stage argument None by default or fit or test for training and testing respectively.\n",
    "\n",
    "```python\n",
    "class LightningDataset(pl.LightningDataModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "  \n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "  \n",
    "    def setup(self, stage=None):\n",
    "        pass\n",
    "  \n",
    "    def train_dataloader(self):\n",
    "        pass\n",
    "  \n",
    "    def val_dataloader(self):\n",
    "        pass\n",
    "  \n",
    "    def test_dataloader(self):\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wp0FJMvFz6Mp"
   },
   "outputs": [],
   "source": [
    "class MnistDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir, batch_size, num_workers):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # 1. prepare data (download MNIST)\n",
    "        ...\n",
    "\n",
    "    def setup(self, stage):\n",
    "        # 2. Preprocess data. Define train, val & test datasets.\n",
    "        # Image augmentation: RandomVerticalFlip, RandomHorizontalFlip, ToTensor\n",
    "        entire_dataset = ...\n",
    "        # There is no validation dataset in MNIST, so we split the training dataset\n",
    "        self.train_ds, self.val_ds = ...\n",
    "        self.test_ds = ...\n",
    "\n",
    "    # 3-1. Implement train_dataloader\n",
    "    def train_dataloader(self):\n",
    "        ...\n",
    "\n",
    "    # 3-2. Implement val_dataloader\n",
    "    def val_dataloader(self):\n",
    "        ...\n",
    "\n",
    "    # 3-3. Implement test_dataloader\n",
    "    def test_dataloader(self):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylzA5IAj74_L"
   },
   "source": [
    "### Callbacks\n",
    "PyTorch Lightning의 Callback 함수는 모델 학습 과정에서 특정 이벤트가 발생할 때 실행되는 사용자 정의 기능을 추가할 수 있도록 도와주는 강력한 도구입니다. 이를 통해 모델 학습, 검증, 예측 등의 과정에서 다양한 작업을 자동화할 수 있습니다.\n",
    "- Early Stopping (학습 조기 종료)\n",
    "- Model Checkpointing (최적의 모델 저장)\n",
    "- Logging & Visualization (TensorBoard, WandB 등의 로깅)\n",
    "- Learning Rate Scheduling (학습률 조정)\n",
    "- Custom Actions (모델 성능 평가, 추가적인 데이터 로깅 등)\n",
    "\n",
    "```\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    callbacks=[\n",
    "        early_stopping,\n",
    "        checkpoint_callback,\n",
    "        PrintLearningRateCallback()\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "1. [Built-in-callbacks](https://lightning.ai/docs/pytorch/stable/extensions/callbacks.html#built-in-callbacks)\n",
    "2. [Callback API](https://lightning.ai/docs/pytorch/stable/extensions/callbacks.html#callback-api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYxlHhCNN5kQ"
   },
   "source": [
    "#### 1️⃣ 간단한 Callback 예제\n",
    "\n",
    "다음은 학습 시작과 종료 시 로그를 출력하는 간단한 Callback 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VT0Q2QHR01bW"
   },
   "outputs": [],
   "source": [
    "class MyPrintingCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def on_train_start(self, trainer, pl_module):\n",
    "        print(\"Starting to train!\")\n",
    "\n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        print(\"Training is done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ItCwnYW2N5kR"
   },
   "source": [
    "#### 2️⃣ Early Stopping Callback\n",
    "\n",
    "PyTorch Lightning에서는 EarlyStopping 콜백을 제공하여 모델의 성능이 개선되지 않으면 학습을 자동으로 종료할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CRuV_fSlN5kR"
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",  # 모니터링할 값\n",
    "    patience=3,          # 개선이 없을 경우 종료할 epoch 수\n",
    "    verbose=True,\n",
    "    mode=\"min\"           # 최소값이 가장 좋은 경우로 설정 (loss는 낮을수록 좋음)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dd61vNZSN5kR"
   },
   "source": [
    "#### 3️⃣ Model Checkpointing Callback\n",
    "\n",
    "최고 성능을 보이는 모델을 저장하는 Callback도 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fMAvgpMxN5kR"
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",       # 기준이 되는 metric\n",
    "    dirpath=\"checkpoints/\",   # 저장 경로\n",
    "    filename=\"best-checkpoint\",  # 파일 이름\n",
    "    save_top_k=1,             # 가장 좋은 k개의 모델만 저장\n",
    "    mode=\"min\",               # loss가 낮을수록 좋은 모델로 판단\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lA15XpiVN5kR"
   },
   "source": [
    "#### 4️⃣ Practice: Custom Callback (사용자 정의)\n",
    "\n",
    "커스텀 Callback을 직접 만들 수도 있습니다.\n",
    "\n",
    "먼저, 모든 10번째 배치마다 현재 학습률을 출력하는 Callback 을 만들어보겠습니다. (Hint: on_train_batch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "139ert28N5kR"
   },
   "outputs": [],
   "source": [
    "class PrintLearningRateCallback(pl.Callback):\n",
    "    # 1. 모든 10번째 배치마다 학습률 출력\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHC1hn5-N5kR"
   },
   "source": [
    "#### 5️⃣ Practice: Gradient Clipping (자동 그래디언트 클리핑)\n",
    "\n",
    "PyTorch Lightning에서는 trainer에 gradient_clip_val을 설정하면 그래디언트 클리핑을 할 수 있지만,\n",
    "커스텀 Callback을 사용하면 특정 조건에서만 적용할 수도 있습니다.\n",
    "\n",
    "1. [torch.clamp](https://pytorch.org/docs/stable/generated/torch.clamp.html#torch.clamp)\n",
    "2. [on_after_backward](https://lightning.ai/docs/pytorch/stable/extensions/callbacks.html#on-after-backward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xUp-5WTwN5kR"
   },
   "outputs": [],
   "source": [
    "class GradientClippingCallback(pl.Callback):\n",
    "    def __init__(self, clip_value=0.5):\n",
    "        self.clip_value = clip_value\n",
    "\n",
    "    # 1. 역전파 후 그래디언트 클리핑\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6TipRFFNN5kR"
   },
   "source": [
    "#### 6️⃣ Practice: Epoch 별 학습 시간 측정 Callback\n",
    "각 epoch이 끝날 때마다 소요 시간을 측정하여 출력하는 Callback입니다.\n",
    "\n",
    "1. [on_train_epoch_start](https://lightning.ai/docs/pytorch/stable/extensions/callbacks.html#on-train-epoch-start)\n",
    "2. [on_train_epoch_end](https://lightning.ai/docs/pytorch/stable/extensions/callbacks.html#on-train-epoch-end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UWZ3o9hfN5kR"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class TimerCallback(pl.Callback):\n",
    "    def on_train_epoch_start(self, trainer, pl_module):\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        elapsed_time = time.time() - self.epoch_start_time\n",
    "        print(f\"Epoch {trainer.current_epoch} 소요 시간: {elapsed_time:.2f}초\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2VpT32078wF"
   },
   "source": [
    "# Setting the hyperparameters\n",
    "PyTorch Lightning에서는 여러 가지 하이퍼파라미터를 제공합니다.\n",
    "- [Trainer flags](https://lightning.ai/docs/pytorch/stable/common/trainer.html#trainer-flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BEZnC9qvN5kR"
   },
   "outputs": [],
   "source": [
    "model = MLP()\n",
    "dm = MnistDataModule(\n",
    "    data_dir=\"dataset/\",\n",
    "    batch_size=100,\n",
    "    num_workers=4,\n",
    ")\n",
    "trainer = pl.Trainer(\n",
    "    logger=TensorBoardLogger(\"tb_logs\", name=\"mnist_model_v0\"),\n",
    "    accelerator=\"gpu\",\n",
    "    devices=[0],\n",
    "    min_epochs=1,\n",
    "    max_epochs=2,\n",
    "    callbacks=[PrintLearningRateCallback(), GradientClippingCallback(), TimerCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZEGk1568AFi"
   },
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EHAqulDF0tom"
   },
   "outputs": [],
   "source": [
    "trainer.fit(model, dm)\n",
    "trainer.validate(model, dm)\n",
    "trainer.test(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CCDg4k_c1wYu"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir tb_logs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
