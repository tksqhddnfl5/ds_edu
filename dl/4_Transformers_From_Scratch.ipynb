{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "OO9yPXOXeo-V"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Transformers from Scratch\n",
        "\n"
      ],
      "metadata": {
        "id": "36k-EDb8F9Eh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install loguru\n",
        "import sys\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
        "from math import sqrt\n",
        "from torch import nn\n",
        "from loguru import logger\n",
        "from typing import Optional\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zr3QR_G0UTsY",
        "outputId": "36fd0936-ac40-42a0-9423-fcbef0fdc46b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting loguru\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: loguru\n",
            "Successfully installed loguru-0.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Overview of the Transformer Architecture\n",
        "\n",
        "<p align=\"center\"><img src=\"https://github.com/happy-jihye/Natural-Language-Processing/blob/main/images/transformer1.png?raw=1\" width = \"400\" ></p>\n",
        "\n"
      ],
      "metadata": {
        "id": "9_RCQJ0eHHaq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What are transformers used for?\n",
        "**Transformers generate text!** You feed in language, and the model generates a probability distribution over tokens. And you can repeatedly sample from this to generate text!\n",
        "\n",
        "Goal of Training a Transformer: You give it a bunch of text, and train it to predict the next token.\n",
        "\n",
        "Importantly, if you give a model 100 tokens in a sequence, it predicts the next token for *each* prefix, i.e. it produces 100 logit vectors (= probability distributions) over the set of all words in our vocabulary, with the `i`-th logit vector representing the probability distribution over the token *following* the `i`-th token in the sequence."
      ],
      "metadata": {
        "id": "5vY1AAr_l7bX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main Components"
      ],
      "metadata": {
        "id": "OO9yPXOXeo-V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Encoder\n",
        "The encoder in a Transformer model is responsible for processing the input sequence, such as a sentence or a document. It consists of a stack (Nx) of encoder layers or \"blocks\". Each encoder layer receives a sequence of token embeddings, which are representations of the input tokens obtained through tokenization and embedding techniques.\n",
        "#### Components:\n",
        "\n",
        "1. Multi-head self-attention layer\n",
        "-  This layer allows each token to attend to other tokens in the input sequence. It computes attention weights that determine the importance of each token with respect to other tokens. The self-attention mechanism helps the model capture the input sequence's dependencies and relationships between different tokens.\n",
        "- Because the self-attention mechanism works across the whole input sequence, the encoder is bidirectional by design.\n",
        "- 입력 벡터들이 서로의 정보를 참고하도록 함\n",
        "2. Fully Connected feed-forward layer\n",
        "- After the self-attention layer, the output embeddings from the previous step are passed through a feed-forward neural network layer. This layer applies a non-linear transformation to each input embedding independently. The feed-forward layer introduces additional modeling capacity and helps capture more complex relationships within the input sequence.\n"
      ],
      "metadata": {
        "id": "yTRFqoHDexSG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output embeddings of each encoder layer have the same size as the inputs. The role of the encoder stack is to \"update\" the input embeddings at each layer, gradually incorporating contextual information and capturing higher-level representations of the sequence."
      ],
      "metadata": {
        "id": "yNOULXUWg_uo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Decoder\n",
        "The decoder in the Transformer model is structurally similar to the encoder, but it has some key differences. The input at each step of the decoder is its own predicted output word from the previous step, similar to an autoregressive model. The input word is embedded and combined with positional encodings, just like in the encoder.\n",
        "\n",
        "#### Components:\n",
        "1. Masked multihead self-attention layer:\n",
        "- This self-attention mechanism in the decoder allows each position in the sequence to attend to preceding positions in the partially generated target sequence. Unlike the encoder's self-attention, which attends to the entire input sequence, **the decoder's self-attention only attends to the preceding sequence elements. **\n",
        "- This is achieved by applying a mask to the softmax input, setting the corresponding values to -∞, which prevents illegal connections between future positions and the current position being attended to. This masking ensures that the decoder is unidirectional, attending only to the preceding positions.\n",
        "2. Encoder-decoder attention layer\n",
        "- This layer allows every position in the decoder to attend over all positions in the input sequence (encoder output).\n",
        "3. Feed-forward network\n",
        "- Similar to the encoder, the decoder includes a feed-forward network. This network applies a non-linear transformation to each position's representation independently"
      ],
      "metadata": {
        "id": "3chnpEinhIyB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Transformer"
      ],
      "metadata": {
        "id": "nD3h-pQMlnzD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminaries\n",
        "### 0. Key, Query, Value\n",
        "- In the context of attention mechanisms, each element in the input sequence is associated with a query, key, and value vector.\n",
        "  - Imagine you’re attending a conference where multiple speakers give presentations. Each presentation corresponds to a token in the input sequence. Now, let’s break down the key, query, and value in this context:\n",
        "\n",
        "  1. **Key**:  The key represents the content or context of each presentation. It captures the main ideas, themes, or relevant information associated with each talk.\n",
        "  2. **Query**: The query represents the specific topic or question you’re interested in or want to focus on during the conference. It could be a specific area of interest or a particular subject you’re curious about.\n",
        "  3. **Value**:  The value contains the detailed information, insights, or knowledge provided by each speaker during their presentation.\n",
        "\n",
        "### 1. Self-Attention\n",
        "- The self-attention mechanism calculates attention weights that indicate the relevance of each element with respect to the other elements within the same sequence.\n",
        "- The term “self” in self-attention emphasizes that attention is computed within the same sequence, without considering any external context or other sequences. It highlights the capability of the self-attention mechanism to capture dependencies and relationships between elements within the input sequence itself.\n",
        "- In the scenario previously described, the attention mechanism allows you to attend to relevant presentations and extract valuable information based on your query. The key vectors help determine which presentations are most relevant to your query, while the query vector represents your specific area of interest or focus. The value vectors contain the detailed content of each presentation.\n",
        "\n",
        "- The model identifies the most important presentations that align with your interests by calculating attention weights between the query and the keys. It then combines the values of these selected presentations using the attention weights, effectively capturing the relevant information from each presentation based on your query.\n",
        "\n",
        "\n",
        "#### 1.1. Scaled dot-product attention\n",
        "- It computes the attention weights between a query vector and a set of key-value pairs by calculating the dot product similarity between them.\n",
        "$$Attention(Q,K,V) = softmax(\\frac{QK^{T}}{\\sqrt{d_k}})$$\n",
        "- Steps:\n",
        "  1. Project each token embedding, with dimension dₘ, into three vectors.\n",
        "  2. Compute the attention scores using the dot product similarity ($QK^{T}$). The dot product between the query vector Q and the key vectors K^T for a sequence with dₖ input tokens will yield a similarity matrix of dimensions dₖ × dₖ.\n",
        "  3. Scale the similarity matrix by dividing it by the square root of the dimensionality of the query/key vectors (${\\sqrt{d_k}}$). This scaling ensures that the dot product values are not too large and helps prevent gradient explosion during training.\n",
        "  4. Compute attention weights w. Apply the softmax function to the scaled similarity matrix. The resulting attention weights represent the importance of each key with respect to each query.\n",
        "  5. Update the token embeddings. Multiply the attention weights w by the value vectors V to obtain a weighted sum of the values. The output is a weighted representation of the values based on the attention weights.\n",
        "\n"
      ],
      "metadata": {
        "id": "4Yn2xzaVmTPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(\n",
        "    query: torch.Tensor,\n",
        "    key: torch.Tensor,\n",
        "    value: torch.Tensor,\n",
        "    mask: Optional[torch.Tensor] = None,\n",
        "    dropout: Optional[nn.Dropout] = None\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Compute scaled dot product attention weights.\n",
        "\n",
        "    Args:\n",
        "        query: Tensor with shape [batch_size, seq_length_q, depth_q].\n",
        "        key: Tensor with shape [batch_size, seq_length_k, depth_k].\n",
        "        value: Tensor with shape [batch_size, seq_length_v, depth_v].\n",
        "        mask: Optional tensor with shape [batch_size, seq_length_q, seq_length_k],\n",
        "            containing values to be masked. Default is None.\n",
        "\n",
        "    Returns:\n",
        "        Tensor with shape [batch_size, seq_length_q, depth_v].\n",
        "    \"\"\"\n",
        "\n",
        "    dim_k = query.size(-1)\n",
        "    logger.debug(f\"query_size: {query.size()}\")\n",
        "    logger.debug(f\"key: {key.transpose(-2, -1).size()}\")\n",
        "    # TODO\n",
        "    # Compute the attention scores.\n",
        "    # Scale the dot product simialrity between the query and the key tensors.\n",
        "    scores =\n",
        "    if mask is not None:\n",
        "        mask = mask.unsqueeze(1)\n",
        "        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
        "    # TODO:\n",
        "    # Compute attention weights w.\n",
        "    # Apply the softmax function to the scaled similarity matrix.\n",
        "    weights =\n",
        "\n",
        "    if dropout is not None:\n",
        "        weights = dropout(weights)\n",
        "    # TODO:\n",
        "    # Update the token embeddings.\n",
        "    # Multiply the attention weights w by the value vectors V\n",
        "    # to obtain a weighted sum of the values.\n",
        "    result_embeddings =\n",
        "    return result_embeddings"
      ],
      "metadata": {
        "id": "BruiP950Q9xC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2. Multi-head Attention\n",
        "- The multi-head attention is an extension of the self-attention mechanism. It enhances the modeling capability by performing multiple attention computations in parallel, with different learned linear projections.\n",
        "$$ \\begin{matrix}\n",
        "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1,...,\\text{head}_h)W^O\\\\\n",
        "\\text{where}~\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K,VW_i^V)\n",
        "\\end{matrix} $$\n",
        "- Steps:\n",
        "  1. It applies linear transformations to the query, key, and value tensors using the learned linear layers self.q, self.k, and self.v, respectively. This projects the tensors to the appropriate dimensions for attention computation.\n",
        "  2. The attention scores are computed by performing matrix multiplication between the query and key tensors.\n",
        "  3. The attention scores are scaled by dividing them by the square root of the head dimension (`self.head_dim`).\n",
        "  4. If a mask is provided, the attention scores are masked by setting the scores corresponding to masked positions to negative infinity.\n",
        "  5. The attention scores are passed through a softmax activation function along the last dimension (`dim=-1`).\n",
        "  6. The attention probabilities are used to weight the value tensor.\n",
        "  7. The resulting attention output is transposed and reshaped to match the original shape.\n",
        "  8. Finally, the attention output is passed through the `self.output_linear` linear layer, which applies another linear transformation to the output representation."
      ],
      "metadata": {
        "id": "emCFMXzTud9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-head attention module.\n",
        "\n",
        "    Args:\n",
        "        config: Configuration for the multi-head attention.\n",
        "    \"\"\"\n",
        "    def __init__(self, config) -> None:\n",
        "        super().__init__()\n",
        "        self.embed_dim = config.hidden_size\n",
        "        self.num_heads = config.num_attention_heads\n",
        "        logger.debug(f\"hidden_dim: {self.embed_dim}\")\n",
        "        logger.debug(f\"num_heads: {self.num_heads}\")\n",
        "\n",
        "        assert self.embed_dim % self.num_heads == 0\n",
        "        self.head_dim = self.embed_dim // self.num_heads\n",
        "        logger.debug(f\"head_dim: {self.head_dim}\")\n",
        "\n",
        "        self.q = nn.Linear(self.embed_dim, self.head_dim * self.num_heads)\n",
        "        self.k = nn.Linear(self.embed_dim, self.head_dim * self.num_heads)\n",
        "        self.v = nn.Linear(self.embed_dim, self.head_dim * self.num_heads)\n",
        "        self.output_linear = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            query: torch.Tensor,\n",
        "            key: torch.Tensor,\n",
        "            value: torch.Tensor,\n",
        "            mask: Optional[torch.Tensor] = None\n",
        "        ) -> torch.Tensor:\n",
        "            \"\"\"\n",
        "            Perform a forward pass of the multi-head attention.\n",
        "\n",
        "            Args:\n",
        "                query: Query tensor of shape [batch_size, seq_len, embed_dim].\n",
        "                key: Key tensor of shape [batch_size, seq_len, embed_dim].\n",
        "                value: Value tensor of shape [batch_size, seq_len, embed_dim].\n",
        "                mask: Optional mask tensor. Default is None.\n",
        "\n",
        "            Returns:\n",
        "                Tensor of shape [batch_size, seq_len, embed_dim],\n",
        "                representing the output of the multi-head attention.\n",
        "            \"\"\"\n",
        "            # TODO:\n",
        "            # Apply linear transformations to the query, key, and value tensors\n",
        "            q =\n",
        "            k =\n",
        "            v =\n",
        "            logger.debug(f\"q_size: {q.size()}\")\n",
        "            logger.debug(f\"k_size: {k.size()}\")\n",
        "            logger.debug(f\"v_size: {v.size()}\")\n",
        "\n",
        "            # Reshape and transpose tensors for matrix multiplication\n",
        "            q = q.view(q.size(0), -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "            k = k.view(k.size(0), -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "            v = v.view(v.size(0), -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "            logger.debug(f\"qT_size: {q.size()}\")\n",
        "            logger.debug(f\"kT_size: {k.size()}\")\n",
        "            logger.debug(f\"vT_size: {v.size()}\")\n",
        "            # TODO:\n",
        "            # Calculate the attention scores using the\n",
        "            # scaled_dot_product_attention function defined earlier\n",
        "\n",
        "            attn_scores =\n",
        "            attn_scores = attn_scores.transpose(1, 2).contiguous()\n",
        "            attn_scores = attn_scores.view(attn_scores.size(0), -1, self.embed_dim)\n",
        "            logger.debug(f\"attn_scores: {attn_scores.size()}\")\n",
        "\n",
        "            output = self.output_linear(attn_scores)\n",
        "            logger.debug(f\"output_size: {output.size()}\")\n",
        "            return output"
      ],
      "metadata": {
        "id": "maV8g_uZsnQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ckpt = \"bert-base-uncased\"\n",
        "config = AutoConfig.from_pretrained(model_ckpt)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "token_emb = nn.Embedding(config.vocab_size, config.hidden_size)\n",
        "\n",
        "text = \"The quick brown fox jumps over the lazy dog\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
        "inputs_embeds = token_emb(inputs.input_ids)\n",
        "\n",
        "query = key = value = inputs_embeds\n",
        "\n",
        "multihead_attn = MultiHeadAttention(config)\n",
        "attn_output = multihead_attn(query, key, value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwuGuVlq2Ah1",
        "outputId": "c0588b6e-7109-41ad-9cdb-db7d2fb4beb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-02-06 10:40:36.719\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m12\u001b[0m - \u001b[34m\u001b[1mhidden_dim: 768\u001b[0m\n",
            "\u001b[32m2025-02-06 10:40:36.725\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m13\u001b[0m - \u001b[34m\u001b[1mnum_heads: 12\u001b[0m\n",
            "\u001b[32m2025-02-06 10:40:36.732\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m17\u001b[0m - \u001b[34m\u001b[1mhead_dim: 64\u001b[0m\n",
            "\u001b[32m2025-02-06 10:40:36.824\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m50\u001b[0m - \u001b[34m\u001b[1mq_size: torch.Size([1, 9, 768])\u001b[0m\n",
            "\u001b[32m2025-02-06 10:40:36.832\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m51\u001b[0m - \u001b[34m\u001b[1mk_size: torch.Size([1, 9, 768])\u001b[0m\n",
            "\u001b[32m2025-02-06 10:40:36.839\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m52\u001b[0m - \u001b[34m\u001b[1mv_size: torch.Size([1, 9, 768])\u001b[0m\n",
            "\u001b[32m2025-02-06 10:40:36.844\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m59\u001b[0m - \u001b[34m\u001b[1mqT_size: torch.Size([1, 12, 9, 64])\u001b[0m\n",
            "\u001b[32m2025-02-06 10:40:36.846\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m60\u001b[0m - \u001b[34m\u001b[1mkT_size: torch.Size([1, 12, 9, 64])\u001b[0m\n",
            "\u001b[32m2025-02-06 10:40:36.858\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m61\u001b[0m - \u001b[34m\u001b[1mvT_size: torch.Size([1, 12, 9, 64])\u001b[0m\n",
            "\u001b[32m2025-02-06 10:40:36.866\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscaled_dot_product_attention\u001b[0m:\u001b[36m22\u001b[0m - \u001b[34m\u001b[1mquery_size: torch.Size([1, 12, 9, 64])\u001b[0m\n",
            "\u001b[32m2025-02-06 10:40:36.872\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscaled_dot_product_attention\u001b[0m:\u001b[36m23\u001b[0m - \u001b[34m\u001b[1mkey: torch.Size([1, 12, 64, 9])\u001b[0m\n",
            "\u001b[32m2025-02-06 10:40:36.878\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m69\u001b[0m - \u001b[34m\u001b[1mattn_scores: torch.Size([1, 9, 768])\u001b[0m\n",
            "\u001b[32m2025-02-06 10:40:36.882\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m72\u001b[0m - \u001b[34m\u001b[1moutput_size: torch.Size([1, 9, 768])\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. The Feed-Forward Layer\n",
        "- The feed-forward layer is a type of neural network layer that processes the input data independently at each position in the input sequence, without considering the dependencies between different positions. This means that the computations for different positions can be parallelized, making the Transformer architecture highly efficient for sequence processing tasks.\n",
        "\n",
        "- The feed-forward layer in Transformers typically consists of two linear transformations with a non-linear activation function in between. The input to the feed-forward layer is a tensor representing the hidden states of the previous layer or the input embeddings.\n",
        "\n",
        "- The input to the feed-forward layer is a tensor representing the hidden states of the previous layer or the input embeddings. The feed-forward layer is a critical component of Transformers as it helps capture local patterns and dependencies in the input data.\n",
        "\n"
      ],
      "metadata": {
        "id": "HMR5MVuKyf5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    Feed-forward layer module.\n",
        "\n",
        "    Args:\n",
        "        config: Configuration for the feed-forward layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, config) -> None:\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Perform a forward pass of the feed-forward layer.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape [batch_size, seq_len, hidden_dim].\n",
        "\n",
        "        Returns:\n",
        "            Tensor of shape [batch_size, seq_len, hidden_dim],\n",
        "            representing the output of the feed-forward layer.\n",
        "        \"\"\"\n",
        "        x = self.linear_1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.linear_2(x)\n",
        "        x = self.dropout(x)\n",
        "        logger.debug(f\"ff_output_size: {x.size()}\")\n",
        "        return x"
      ],
      "metadata": {
        "id": "Mb5eXEE5ybmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feed_forward = FeedForward(config)\n",
        "ff_outputs = feed_forward(attn_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRjTrmh_19PO",
        "outputId": "3800c769-4f4a-45f0-f2df-a1e46fbf3320"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-02-06 10:41:00.346\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m30\u001b[0m - \u001b[34m\u001b[1mff_output_size: torch.Size([1, 9, 768])\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Positional Embeddings\n",
        "- The purpose of positional embeddings is to provide the model with a representation that encodes the relative positions of tokens within the sequence. This allows the model to differentiate between tokens based on their position, even though all tokens initially have the same embeddings.\n",
        "\n",
        "- In the original Transformer model, the positional embeddings used to encode the sequential order of tokens are learned as part of the model training process. The positional embeddings are initialized with fixed sinusoidal functions of different frequencies and then fine-tuned during training.\n",
        "\n",
        "- Steps:\n",
        "  1. The constructor of the Embedding class defines two embedding layers `self.token_embeddings` and `self.position_embeddings`. These layers are initialized with different vocabulary sizes and hidden sizes.\n",
        "  2. In the forward method, position IDs are created using `torch.arange(seq_length).unsqueeze(0)`.  This creates a tensor of sequential integers from 0 to seq_lenght-1 and unsqueezes it to have a shape of [1, seq_lenght]. These position IDs represent the positions of the tokens in the input sequence.\n",
        "  3. The token embeddings for the input sequence are obtained by passing `input_ids` to `self.token_embeddings`. This maps each token ID to its corresponding embedding vector. On the other hand, the position embeddings for the input sequence are obtained by passing `position_ids` to `self.position_embeddings`. This maps each position ID to its corresponding embedding vector.\n",
        "  4. The token embeddings and position embeddings are added element-wise (`token_embeddings` + `position_embeddings`) to create the combined embeddings. This operation incorporates both the token information and the positional information of each token in the input sequence.\n",
        "\n"
      ],
      "metadata": {
        "id": "S20D0CzBzP4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Embeddings layer module.\n",
        "    Combines a token embedding layer that projects the `input_ids` to a dense hidden state\n",
        "    with the positional embedding that does the same for `position_ids`.\n",
        "    The resulting embedding is simply the sum of both embeddings.\n",
        "\n",
        "    Args:\n",
        "        config: Configuration for the embeddings layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.token_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
        "        self.dropout = nn.Dropout()\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Perform a forward pass of the embeddings layer.\n",
        "\n",
        "        Args:\n",
        "            input_ids: Input tensor of shape [batch_size, seq_len].\n",
        "\n",
        "        Returns:\n",
        "            Tensor of shape [batch_size, seq_len, hidden_dim],\n",
        "            representing the embeddings of the input.\n",
        "\n",
        "        Notes:\n",
        "            1. Create position IDs for input sequence.\n",
        "            2. Create token and position embeddings.\n",
        "            3. Combine token and position embeddings.\n",
        "        \"\"\"\n",
        "        logger.debug(f\"input_size: {input_ids.size()}\")\n",
        "        seq_length = input_ids.size(1)\n",
        "        # TODO: 1. Create position IDs\n",
        "        position_ids =\n",
        "        # TODO: 2. Create token and position embeddings\n",
        "        token_embeddings =\n",
        "        logger.debug(f\"token_embd_size: {token_embeddings.size()}\")\n",
        "        position_embeddings =\n",
        "        logger.debug(f\"position_embd_size: {token_embeddings.size()}\")\n",
        "        # TODO 3: Combine token and position embeddings\n",
        "        embeddings =\n",
        "\n",
        "        embeddings = self.layer_norm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        logger.debug(f\"embd_size: {token_embeddings.size()}\")\n",
        "\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "U0xHcN1OzgIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Encoder\n",
        "### 1. TransformerEncoderBlock\n",
        "- With the previously defined components, we can now define the TransformerEncoderBlock class. It is responsible for performing one layer of the encoder in a Transformer model.\n",
        "- Steps:\n",
        "  1. Layer Normalization: The input tensor `x` is first passed through a layer normalization operation using `self.layer_norm_1`. This operation normalizes the activations across the hidden dimension of x to have zero mean and unit variance. The result is stored in hidden_state.\n",
        "  2. Attention with Skip Connection: The attention mechanism is applied to `hidden_state` using `self.attention`. This attention operation takes hidden_state as the input and produces an attention-based output. The output is then element-wise added (`+`) to the original input tensor `x`. This skip connection allows the model to directly incorporate the original input along with the attention-based output.\n",
        "  3. Feed-Forward Layer with Skip Connection: The output of the previous step is passed through another layer normalization operation `self.layer_norm_2` to normalize the activations. Then, the result is passed through the feed-forward layer self.feed_forward. The output of the feed-forward layer is again element-wise added (`+`) to the input tensor from the previous step (`x`). This skip connection allows the model to combine the information from the original input with the transformed output from the feed-forward layer.\n",
        "\n",
        "- In summary, the skip connections enable the model to incorporate the original input tensor x into the output of each layer. By adding the transformed outputs to the original input, the model can retain important information from the input and facilitate the flow of gradients during training. The skip connections help in addressing the vanishing gradient problem and make it easier to train deep Transformer architectures by ensuring the model has access to the original input information at each layer."
      ],
      "metadata": {
        "id": "oWD4WqXry7m_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer Encoder block module.\n",
        "\n",
        "    Args:\n",
        "        config: Configuration for the encoder block.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config) -> None:\n",
        "        super().__init__()\n",
        "        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n",
        "        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n",
        "        self.attention = MultiHeadAttention(config)\n",
        "        self.feed_forward = FeedForward(config)\n",
        "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor]=None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Perform a forward pass of the transformer encoder block.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape [batch_size, seq_len, hidden_dim].\n",
        "            mask: Optional mask tensor. Default is None.\n",
        "\n",
        "        Returns:\n",
        "            Tensor of shape [batch_size, seq_len, hidden_dim],\n",
        "            representing the output of the encoder block.\n",
        "        \"\"\"\n",
        "        logger.debug(f\"encoder_block_input_size: {x.size()}\")\n",
        "        # TODO:\n",
        "        # Normalize input tensor x\n",
        "        hidden_state =\n",
        "        # TODO:\n",
        "        # Apply the attention mechanism to the hidden_state using self.attention\n",
        "        # Add the output to the original input tensor (skip connection)\n",
        "\n",
        "        attention_output =\n",
        "        x = x + self.dropout(x)\n",
        "        # TODO:\n",
        "        # Normalize the activations using self.layer_norm_2\n",
        "        # Pass it to the feed-forward layer\n",
        "        # Add the output of the feed_forward layer to the input tensor from\n",
        "        # the previous step (skip connection)\n",
        "        x =\n",
        "        x =\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        logger.debug(f\"encoder_block_output_size: {x.size()}\")\n",
        "        return x"
      ],
      "metadata": {
        "id": "1wEQzHPLyTPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_layer = TransformerEncoderBlock(config)\n",
        "_ = encoder_layer(inputs_embeds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tm3nJF_g1lOA",
        "outputId": "31ab81e6-4b85-40be-8817-9d32ecf44f91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-02-06 10:42:07.947\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m12\u001b[0m - \u001b[34m\u001b[1mhidden_dim: 768\u001b[0m\n",
            "\u001b[32m2025-02-06 10:42:07.949\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m13\u001b[0m - \u001b[34m\u001b[1mnum_heads: 12\u001b[0m\n",
            "\u001b[32m2025-02-06 10:42:07.951\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m17\u001b[0m - \u001b[34m\u001b[1mhead_dim: 64\u001b[0m\n",
            "\u001b[32m2025-02-06 10:42:08.086\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m29\u001b[0m - \u001b[34m\u001b[1mencoder_block_input_size: torch.Size([1, 9, 768])\u001b[0m\n",
            "\u001b[32m2025-02-06 10:42:08.093\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m50\u001b[0m - \u001b[34m\u001b[1mq_size: torch.Size([1, 9, 768])\u001b[0m\n",
            "\u001b[32m2025-02-06 10:42:08.095\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m51\u001b[0m - \u001b[34m\u001b[1mk_size: torch.Size([1, 9, 768])\u001b[0m\n",
            "\u001b[32m2025-02-06 10:42:08.098\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m52\u001b[0m - \u001b[34m\u001b[1mv_size: torch.Size([1, 9, 768])\u001b[0m\n",
            "\u001b[32m2025-02-06 10:42:08.101\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m59\u001b[0m - \u001b[34m\u001b[1mqT_size: torch.Size([1, 12, 9, 64])\u001b[0m\n",
            "\u001b[32m2025-02-06 10:42:08.104\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m60\u001b[0m - \u001b[34m\u001b[1mkT_size: torch.Size([1, 12, 9, 64])\u001b[0m\n",
            "\u001b[32m2025-02-06 10:42:08.105\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m61\u001b[0m - \u001b[34m\u001b[1mvT_size: torch.Size([1, 12, 9, 64])\u001b[0m\n",
            "\u001b[32m2025-02-06 10:42:08.107\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscaled_dot_product_attention\u001b[0m:\u001b[36m22\u001b[0m - \u001b[34m\u001b[1mquery_size: torch.Size([1, 12, 9, 64])\u001b[0m\n",
            "\u001b[32m2025-02-06 10:42:08.109\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscaled_dot_product_attention\u001b[0m:\u001b[36m23\u001b[0m - \u001b[34m\u001b[1mkey: torch.Size([1, 12, 64, 9])\u001b[0m\n",
            "\u001b[32m2025-02-06 10:42:08.112\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m69\u001b[0m - \u001b[34m\u001b[1mattn_scores: torch.Size([1, 9, 768])\u001b[0m\n",
            "\u001b[32m2025-02-06 10:42:08.114\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m72\u001b[0m - \u001b[34m\u001b[1moutput_size: torch.Size([1, 9, 768])\u001b[0m\n",
            "\u001b[32m2025-02-06 10:42:08.124\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m30\u001b[0m - \u001b[34m\u001b[1mff_output_size: torch.Size([1, 9, 768])\u001b[0m\n",
            "\u001b[32m2025-02-06 10:42:08.131\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m48\u001b[0m - \u001b[34m\u001b[1mencoder_block_output_size: torch.Size([1, 9, 768])\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. TransformerEncoder\n",
        "- Finally, putting everything together, we can now define the TransoformerEncoder class. It is responsible for processing the input sequence using multiple stacked Transformer Encoder Blocks."
      ],
      "metadata": {
        "id": "3Kfege8l2yzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer Encoder module.\n",
        "\n",
        "    Args:\n",
        "        config: Configuration for the encoder.\n",
        "    \"\"\"\n",
        "    def __init__(self, config) -> None:\n",
        "        super().__init__()\n",
        "        self.embeddings = Embeddings(config)\n",
        "        self.layers = nn.ModuleList([TransformerEncoderBlock(config) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Perform a forward pass of the transformer encoder.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape [batch_size, seq_len].\n",
        "            mask: Optional mask tensor. Default is None.\n",
        "\n",
        "        Returns:\n",
        "            Tensor of shape [batch_size, seq_len, hidden_dim],\n",
        "            representing the output of the encoder.\n",
        "        \"\"\"\n",
        "        x = self.embeddings(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "RqThqFrx2wDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger.remove()\n",
        "logger.add(sys.stderr, level=\"INFO\")\n",
        "encoder = TransformerEncoder(config)\n",
        "encoder_output = encoder(inputs.input_ids)\n",
        "encoder_output.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOoxpD5G15uV",
        "outputId": "0e52b007-ba62-450d-a417-f7c04429ffb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 9, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZZOtkEL2yXv",
        "outputId": "4e5eae72-87cd-4b2e-9124-ef46be53bd23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.3491, -0.3320, -0.0517,  ..., -0.5293, -0.0000,  0.8004],\n",
              "         [ 0.0000, -0.7515,  1.4030,  ...,  0.3103,  0.1716, -0.0362],\n",
              "         [ 0.3268, -0.5307, -1.9108,  ..., -0.4951,  0.1939, -3.6646],\n",
              "         ...,\n",
              "         [ 0.0061, -0.0843,  0.4482,  ...,  0.1206,  0.3052, -5.8102],\n",
              "         [ 0.8655,  0.0621,  0.0000,  ..., -0.5356,  0.2064, -3.1653],\n",
              "         [-1.2111,  0.1792,  0.1255,  ...,  0.4157, -0.6652, -0.3626]]],\n",
              "       grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Decoder\n",
        "-  The main difference between the decoder and encoder is that the decoder has two attention sublayers.\n",
        "\n",
        "- The first attention sublayer, known as the self-attention sublayer, allows the decoder to attend to its own previously generated tokens, capturing dependencies and relationships within the output sequence.\n",
        "\n",
        "- The second attention sublayer is the encoder-decoder attention, which allows the decoder to attend to the encoded representations produced by the encoder, incorporating contextual information from the input sequence.\n",
        "\n",
        "- **Mask** is applied in the self-attention mechanism to enforce the causality constraint during the decoding process. Since the decoder generates the target sequence autoregressively, each position in the target sequence should only attend to previous positions and not future positions. If you recall the scaled_dot_product_attention function, we set the upper values to infinity. This guarantees that the attention weights are all zero once we take the softmax over the scores (as e^-∞=0).\n",
        "\n",
        "### 1. The Decoder Block\n",
        "- Similarly to the TransformerEncoderBlock, the TransformerDecoderBlock is responsible for performing one layer of the decoder in a Transformer model:"
      ],
      "metadata": {
        "id": "_-MsG8s53FTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer Decoder layer module.\n",
        "\n",
        "    Args:\n",
        "        config: Configuration for the decoder layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, ) -> None:\n",
        "        super().__init__()\n",
        "        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n",
        "        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n",
        "        self.layer_norm_3 = nn.LayerNorm(config.hidden_size)\n",
        "        self.self_attention = MultiHeadAttention(config)\n",
        "        self.encoder_decoder_attention = MultiHeadAttention(config)\n",
        "        self.feed_forward = FeedForward(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        encoder_output: torch.Tensor,\n",
        "        source_mask: Optional[torch.Tensor] = None,\n",
        "        target_mask: Optional[torch.Tensor] = None,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Perform a forward pass of the transformer decoder block.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape [batch_size, seq_len, hidden_dim].\n",
        "            encoder_output: Output tensor from the encoder of shape [batch_size, seq_len, hidden_dim].\n",
        "            source_mask: Optional source mask tensor. Default is None.\n",
        "            target_mask: mask: Optional target mask tensor. Default is None.\n",
        "        Returns:\n",
        "            Tensor of shape [batch_size, seq_len, hidden_dim],\n",
        "            representing the output of the decoder block.\n",
        "        \"\"\"\n",
        "        logger.debug(f\"decoder_block_input_size: {x.size()}\")\n",
        "\n",
        "        # TODO:\n",
        "        # First attention sublayer, attending to its own previously generated\n",
        "        # tokens. mask 필요: 디코더가 미래 단어를 참조하지 못하도록 차단\n",
        "        hidden_state = self.layer_norm_1(x)\n",
        "        attn_1_out =\n",
        "        x =\n",
        "        # TODO:\n",
        "        # Second attention sublayer, attending to the encoded representations\n",
        "        # from the encoder.\n",
        "        # mask 필요 없음: 인코더 출력을 전체적으로 봐도 됨. 인코더-디코더 간의 Attention에서는 미래 정보 제한이 필요 없음\n",
        "        x = self.layer_norm_2(x)\n",
        "        attn_2_out =\n",
        "        x =\n",
        "        x = self.layer_norm_3(x)\n",
        "\n",
        "        feed_forward_output = self.feed_forward(x)\n",
        "        x = x + self.dropout(feed_forward_output)\n",
        "        logger.debug(f\"decoder_block_output_size: {x.size()} \")\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "tPPEMp5K29Ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. TransformerDecoder\n"
      ],
      "metadata": {
        "id": "YUImseSe5XiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, config) -> None:\n",
        "        \"\"\"\n",
        "        Transformer Decoder module.\n",
        "\n",
        "        Args:\n",
        "            config: Configuration object for the decoder.\n",
        "            mask: Masking object for attention layers.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.embeddings = Embeddings(config)\n",
        "        self.layers = nn.ModuleList([TransformerDecoderBlock(config) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        encoder_output: torch.Tensor,\n",
        "        source_mask: torch.Tensor = None,\n",
        "        target_mask: torch.Tensor = None\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Perform a forward pass of the transformer decoder.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape [batch_size, tgt_len].\n",
        "\n",
        "        Returns:\n",
        "            Tensor of shape [batch_size, tgt_len, vocab_size],\n",
        "            representing the predicted probabilities over the vocabulary.\n",
        "        \"\"\"\n",
        "        x = self.embeddings(input_ids)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, source_mask=source_mask, target_mask=target_mask)\n",
        "        return x"
      ],
      "metadata": {
        "id": "-tAjtMyE30bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger.remove()\n",
        "logger.add(sys.stderr, level=\"INFO\")\n",
        "seq_len = inputs.input_ids.size(-1)\n",
        "mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0)\n",
        "encoder = TransformerEncoder(config)\n",
        "encoder_output = encoder(inputs.input_ids)\n",
        "decoder = TransformerDecoder(config)\n",
        "output = decoder(inputs.input_ids, encoder_output, target_mask=mask)\n",
        "output.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBrgosWp5fyt",
        "outputId": "90e5efc9-7417-4607-bce4-75c270d51614"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 9, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Transformer\n",
        "- With all the required components now in place, we can proceed to define and implement this model"
      ],
      "metadata": {
        "id": "9_M3EgAk5t6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder-Decoder model that combines the TransformerEncoder and TransformerDecoder.\n",
        "\n",
        "    Args:\n",
        "        encoder_config: Configuration for the encoder.\n",
        "        decoder_config: Configuration for the decoder.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        config,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.encoder = TransformerEncoder(config)\n",
        "        self.decoder = TransformerDecoder(config)\n",
        "        self.fc = nn.Linear(config.hidden_size, config.vocab_size)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        target_ids: torch.Tensor,\n",
        "        source_mask: Optional[torch.Tensor] = None,\n",
        "        target_mask: Optional[torch.Tensor] = None\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Perform a forward pass of the encoder-decoder model.\n",
        "\n",
        "        Args:\n",
        "            input_ids: Input tensor of shape [batch_size, src_len].\n",
        "            target_ids: Target tensor of shape [batch_size, tgt_len].\n",
        "\n",
        "        Returns:\n",
        "            Tensor of shape [batch_size, tgt_len, vocab_size],\n",
        "            representing the predicted probabilities over the vocabulary.\n",
        "        \"\"\"\n",
        "        encoder_output = self.encoder(input_ids)\n",
        "        decoder_output = self.decoder(\n",
        "            target_ids,\n",
        "            encoder_output,\n",
        "            source_mask=source_mask,\n",
        "            target_mask=target_mask\n",
        "        )\n",
        "\n",
        "        x = self.fc(decoder_output)  # Apply linear layer to transform to vocab_size\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "pUMsP5BT5gOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Masking\n",
        "The mask used in the Transformer model should have a specific shape and values to ensure proper masking during the attention mechanism. Here's how you can define the mask:\n",
        "\n",
        "- Padding Mask: The padding mask is used to mask out padding tokens in the input sequences. It should have a shape of (batch_size, seq_length) and contain 1 where the padding tokens are present and 0 for the non-padding tokens. This mask ensures that the padding tokens do not contribute to the attention scores.\n",
        "\n",
        "- Future Mask: The future mask is used to prevent attending to future positions in the self-attention mechanism. It should have a shape of (seq_length, seq_length) and have 1 for positions that can be attended and 0 for positions that should be masked or ignored.\n",
        "\n",
        "- Combined Mask: To create the final mask, you need to combine the padding mask and the future mask. This can be done by applying logical operations, such as element-wise multiplication or logical OR, to the two masks."
      ],
      "metadata": {
        "id": "31CEsEwv52Bv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mask(batch_size: int, seq_length: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Create a lower triangular mask with ones below the diagonal.\n",
        "\n",
        "    Args:\n",
        "        batch_size: The batch size.\n",
        "        seq_length: The length of the sequence.\n",
        "\n",
        "    Returns:\n",
        "        The mask tensor with shape (batch_size, seq_length, seq_length).\n",
        "    \"\"\"\n",
        "\n",
        "    mask = torch.tril(torch.ones(seq_length, seq_length))\n",
        "    mask = mask.unsqueeze(0).expand(batch_size, seq_length, seq_length)  # Expand the mask along the batch dimension\n",
        "\n",
        "    return mask"
      ],
      "metadata": {
        "id": "JD82AcP551Ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the Transformer!"
      ],
      "metadata": {
        "id": "-Yc5nw316ZhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerConfig:\n",
        "    \"\"\"\n",
        "    Configuration class for the Transformer model.\n",
        "\n",
        "    Args:\n",
        "        hidden_size: Size of the hidden state.\n",
        "        intermediate_size: Size of the intermediate layer in the feed-forward network.\n",
        "        num_hidden_layers: Number of hidden layers in the Transformer.\n",
        "        vocab_size: Size of the vocabulary.\n",
        "        max_position_embeddings: Maximum number of positional embeddings.\n",
        "        hidden_dropout_prob: Dropout probability for the hidden layers.\n",
        "        num_attention_heads: Number of attention heads in the multi-head attention.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size: int,\n",
        "        intermediate_size: int,\n",
        "        num_hidden_layers: int,\n",
        "        vocab_size: int,\n",
        "        max_position_embeddings: int,\n",
        "        hidden_dropout_prob: float,\n",
        "        num_attention_heads: int\n",
        "    ):\n",
        "        self.hidden_size = hidden_size\n",
        "        self.intermediate_size = intermediate_size\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "        self.hidden_dropout_prob = hidden_dropout_prob\n",
        "        self.num_attention_heads = num_attention_heads"
      ],
      "metadata": {
        "id": "LnY4YNdo6uX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up hyperparameters and configuration\n",
        "config = TransformerConfig(\n",
        "    hidden_size=512,\n",
        "    intermediate_size=2048,\n",
        "    num_hidden_layers=6,\n",
        "    vocab_size=100,\n",
        "    max_position_embeddings=512,\n",
        "    hidden_dropout_prob=0.1,\n",
        "    num_attention_heads=8\n",
        ")"
      ],
      "metadata": {
        "id": "5TpLAIm57AKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define some fake data\n",
        "batch_size = 16\n",
        "source_length = 10\n",
        "target_length = 12\n",
        "\n",
        "source_ids = torch.randint(0, config.vocab_size, (batch_size, source_length))\n",
        "target_ids = torch.randint(0, config.vocab_size, (batch_size, target_length))\n",
        "\n",
        "source_ids.size(), target_ids.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjcVfnNm64Et",
        "outputId": "c8ff1ee3-b725-4a80-f379-832a7410521b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([16, 10]), torch.Size([16, 12]))"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source_mask = create_mask(batch_size, source_length)\n",
        "target_mask = create_mask(batch_size, target_length)\n",
        "source_mask.size(), target_mask.size()\n",
        "\n",
        "logger.remove()\n",
        "logger.add(sys.stderr, level=\"INFO\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnUlJ6N08edU",
        "outputId": "7d7bf902-cdf7-4e11-ee6c-c407a5508fb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = TransformerEncoder(config)\n",
        "encoder_output = encoder(source_ids)\n",
        "decoder = TransformerDecoder(config)\n",
        "output = decoder(source_ids, encoder_output, source_mask=source_mask)\n",
        "output.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAhasmZd8i07",
        "outputId": "27a8218e-1401-4cd8-e125-0adbcd16ad9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 10, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the EncoderDecoder model\n",
        "encoder_decoder = EncoderDecoder(config)\n",
        "output = encoder_decoder(source_ids, target_ids, target_mask=target_mask)\n",
        "print(\"Output Shape:\", output.shape)  # Should be (batch_size, target_length, vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aWfXu5B8jMn",
        "outputId": "7d3912bb-7990-433e-ff04-112dc9910df3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output Shape: torch.Size([16, 12, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_ids.size()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCeLQFe_8kuS",
        "outputId": "bcddd058-cdfa-4ee7-c88c-f711c4fcfc27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 12])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n"
      ],
      "metadata": {
        "id": "uNVyswOS8r7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    Provides random data copy dataset for training.\n",
        "\n",
        "    Args:\n",
        "        vocabulary_size: The vocabulary size.\n",
        "        batch_size: The batch size.\n",
        "        num_samples: The number of samples.\n",
        "        sample_length: The length of each sample.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocabulary_size: int, batch_size: int, num_samples: int, sample_length: int):\n",
        "        self.samples = list()\n",
        "\n",
        "        for i in range(batch_size * num_samples):\n",
        "            data = torch.from_numpy(np.random.randint(1, vocabulary_size, size=(sample_length,)))\n",
        "            data[0] = 1\n",
        "            source = torch.autograd.Variable(data, requires_grad=False)\n",
        "            target = torch.autograd.Variable(data, requires_grad=False)\n",
        "\n",
        "            # Prepare the sample dictionary\n",
        "            sample = {\n",
        "                'source': source,\n",
        "                'target': target[:-1],\n",
        "                'target_y': target[1:],\n",
        "                'source_mask': (source != 0).unsqueeze(-2),\n",
        "                'target_mask': self.make_std_mask(target, 0),\n",
        "                'tokens_count': (target[1:] != 0).data.sum()  # Assuming target_y is the actual target shifted by 1\n",
        "            }\n",
        "\n",
        "            self.samples.append(sample)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Get the number of samples in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            The number of samples.\n",
        "        \"\"\"\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> dict:\n",
        "        \"\"\"\n",
        "        Get a sample from the dataset.\n",
        "\n",
        "        Args:\n",
        "            idx: The index of the sample to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            A dictionary containing the source, target, target_y, source_mask, target_mask, and tokens_count.\n",
        "        \"\"\"\n",
        "        return self.samples[idx]\n",
        "\n",
        "    @staticmethod\n",
        "    def make_std_mask(target: torch.Tensor, pad: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Create a mask to hide padding and future words.\n",
        "\n",
        "        Args:\n",
        "            target (torch.Tensor): The target tensor.\n",
        "            pad (int): The padding value.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The mask tensor.\n",
        "        \"\"\"\n",
        "        target_mask = (target != pad)\n",
        "        target_mask = target_mask & torch.autograd.Variable(\n",
        "            RandomDataset.subsequent_mask(target.size(-1)).type_as(target_mask.data))\n",
        "\n",
        "        return target_mask\n",
        "\n",
        "    @staticmethod\n",
        "    def subsequent_mask(size: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Mask out subsequent positions.\n",
        "\n",
        "        Args:\n",
        "            size: The size of the mask.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The subsequent mask tensor.\n",
        "        \"\"\"\n",
        "        attn_shape = (size, size)\n",
        "        subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "        return torch.from_numpy(subsequent_mask) == 0"
      ],
      "metadata": {
        "id": "VQcHWjA08lnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "num_samples = 1000\n",
        "samples_len = 10\n",
        "train_set = RandomDataset(config.vocab_size, batch_size, num_samples, samples_len)\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "jMijjGYv8yRK",
        "outputId": "8fff48e9-f954-425c-c5fc-59b3a4664d98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-af035ec3f0fd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msamples_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-9265f15a18d0>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocabulary_size, batch_size, num_samples, sample_length)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = EncoderDecoder(config)\n",
        "\n",
        "# Initialize parameters.\n",
        "for p in encoder_decoder.parameters():\n",
        "    if p.dim() > 1:\n",
        "        torch.nn.init.xavier_uniform_(p)\n",
        "\n",
        "model.train()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "current_loss = 0.0\n",
        "counter = 0\n",
        "\n",
        "for i, batch in enumerate(train_loader):\n",
        "    with torch.set_grad_enabled(True):\n",
        "        out = model.forward(batch['source'], batch['target'], batch['source_mask'], batch['target_mask'])\n",
        "        loss = loss_function(out.contiguous().view(-1, out.size(-1)), batch['target_y'].contiguous().view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        current_loss += loss\n",
        "        counter += 1\n",
        "\n",
        "        if counter % 5 == 0:\n",
        "            print(\"Batch: %d; Loss: %f\" % (i + 1, current_loss / counter))\n",
        "            current_loss = 0.0\n",
        "            counter = 0"
      ],
      "metadata": {
        "id": "9AYKuyXM8ta4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dwED_KAF8ynF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}