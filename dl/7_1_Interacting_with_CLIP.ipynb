{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPHN7PJgKOzb"
      },
      "source": [
        "# Interacting with CLIP\n",
        "\n",
        "[CLIP paper](https://arxiv.org/abs/2103.00020)\n",
        "\n",
        "![CLIP objective](https://github.com/openai/CLIP/raw/main/CLIP.png)\n",
        "\n",
        "This is a self-contained notebook that shows how to download and run CLIP models, calculate the similarity between arbitrary image and text inputs, and perform zero-shot image classifications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53N4k0pj_9qL"
      },
      "source": [
        "# Preparation for Colab\n",
        "\n",
        "Make sure you're running a GPU runtime; if not, select \"GPU\" as the hardware accelerator in Runtime > Change Runtime Type in the menu. The next cells will install the `clip` package and its dependencies, and check if PyTorch 1.7.1 or later is installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BpdJkdBssk9"
      },
      "outputs": [],
      "source": [
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1hkDT38hSaP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from pkg_resources import packaging\n",
        "\n",
        "print(\"Torch version:\", torch.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFxgLV5HAEEw"
      },
      "source": [
        "# Loading the model\n",
        "\n",
        "`clip.available_models()` will list the names of available CLIP models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLFS29hnhlY4"
      },
      "outputs": [],
      "source": [
        "import clip\n",
        "\n",
        "clip.available_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBRVTY9lbGm8"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model, preprocess = clip.load(\"ViT-B/16\")\n",
        "model = model.to(device).eval()\n",
        "input_resolution = model.visual.input_resolution\n",
        "context_length = model.context_length\n",
        "vocab_size = model.vocab_size\n",
        "\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "print(\"Input resolution:\", input_resolution)\n",
        "print(\"Context length:\", context_length)\n",
        "print(\"Vocab size:\", vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21slhZGCqANb"
      },
      "source": [
        "# Image Preprocessing\n",
        "\n",
        "We resize the input images and center-crop them to conform with the image resolution that the model expects. Before doing so, we will normalize the pixel intensity using the dataset mean and standard deviation.\n",
        "\n",
        "The second return value from `clip.load()` contains a torchvision `Transform` that performs this preprocessing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6cpiIFHp9N6"
      },
      "outputs": [],
      "source": [
        "preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwSB5jZki3Cj"
      },
      "source": [
        "# Text Preprocessing\n",
        "\n",
        "We use a case-insensitive tokenizer, which can be invoked using `clip.tokenize()`. By default, the outputs are padded to become 77 tokens long, which is what the CLIP models expects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGom156-i2kL"
      },
      "outputs": [],
      "source": [
        "clip.tokenize(\"Hello World!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W8ARJVqBJXs"
      },
      "source": [
        "# Setting up input images and texts\n",
        "\n",
        "We are going to feed 8 example images and their textual descriptions to the model, and compare the similarity between the corresponding features.\n",
        "\n",
        "The tokenizer is case-insensitive, and we can freely give any suitable textual descriptions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMc1AXzBlhzm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import skimage\n",
        "from skimage import data\n",
        "import IPython.display\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "# images in skimage to use and their textual descriptions\n",
        "descriptions = {\n",
        "    \"page\": \"a page of text about segmentation\",\n",
        "    \"cat\": \"a facial photo of a tabby cat\",\n",
        "    \"astronaut\": \"a portrait of an astronaut with the American flag\",\n",
        "    \"rocket\": \"a rocket standing on a launchpad\",\n",
        "    \"camera\": \"a person looking at a camera on a tripod\",\n",
        "    \"horse\": \"a black-and-white silhouette of a horse\",\n",
        "    \"coffee\": \"a cup of coffee on a saucer\",\n",
        "    \"eagle\": \"a black-and-white photo of an eagle\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSSrLY185jSf"
      },
      "outputs": [],
      "source": [
        "original_images = []\n",
        "images = []\n",
        "texts = []\n",
        "plt.figure(figsize=(16, 5))\n",
        "\n",
        "for name in descriptions:\n",
        "    caller = getattr(data, name)\n",
        "    image = caller()\n",
        "    image = Image.fromarray(image).convert(\"RGB\")\n",
        "\n",
        "    plt.subplot(2, 4, len(images) + 1)\n",
        "    plt.imshow(image)\n",
        "    plt.title(f\"{name}\\n{descriptions[name]}\")\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    original_images.append(image)\n",
        "    images.append(preprocess(image))\n",
        "    texts.append(descriptions[name])\n",
        "\n",
        "plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_YF_s2ytHaY"
      },
      "outputs": [],
      "source": [
        "print(images[0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEVKsji6WOIX"
      },
      "source": [
        "## Building features\n",
        "\n",
        "We normalize the images, tokenize each text input, and run the forward pass of the model to get the image and text features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBgCanxi8JKw"
      },
      "outputs": [],
      "source": [
        "image_input = torch.tensor(np.stack(images)).to(device)\n",
        "text_tokens = clip.tokenize([\"This is \" + desc for desc in texts]).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZN9I0nIBZ_vW"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image_input).float()\n",
        "    text_features = model.encode_text(text_tokens).float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbmCQZyqxBfh"
      },
      "outputs": [],
      "source": [
        "print(image_features.shape)\n",
        "print(text_features.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuxm2Gt4Wvzt"
      },
      "source": [
        "## Calculating cosine similarity\n",
        "\n",
        "We normalize the features and calculate the dot product of each pair.\n",
        "\n",
        "$$\\text{Normalize}(x) = \\frac{x}{||x||}$$\n",
        "\n",
        "$$\\text{cos}(x, y) = \\frac{x \\cdot y}{||x|| ||y||}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKAxkQR7bf3A"
      },
      "outputs": [],
      "source": [
        "\n",
        "# TODO: normalize image_features and text_features\n",
        "# hint: use torch.norm\n",
        "image_features = None\n",
        "text_features = None\n",
        "\n",
        "# computing cosine similarity between all image features and all text features\n",
        "# then convert to numpy array\n",
        "# TODO\n",
        "similarity = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5zvMxh8cU6m"
      },
      "outputs": [],
      "source": [
        "count = len(descriptions)\n",
        "\n",
        "plt.figure(figsize=(20, 14))\n",
        "plt.imshow(similarity, vmin=0.1, vmax=0.3)\n",
        "# plt.colorbar()\n",
        "plt.yticks(range(count), texts, fontsize=18)\n",
        "plt.xticks([])\n",
        "for i, image in enumerate(original_images):\n",
        "    plt.imshow(image, extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin=\"lower\")\n",
        "for x in range(similarity.shape[1]):\n",
        "    for y in range(similarity.shape[0]):\n",
        "        plt.text(x, y, f\"{similarity[y, x]:.2f}\", ha=\"center\", va=\"center\", size=12)\n",
        "\n",
        "for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
        "  plt.gca().spines[side].set_visible(False)\n",
        "\n",
        "plt.xlim([-0.5, count - 0.5])\n",
        "plt.ylim([count + 0.5, -2])\n",
        "\n",
        "plt.title(\"Cosine similarity between text and image features\", size=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alePijoXy6AH"
      },
      "source": [
        "# Zero-Shot Image Classification\n",
        "\n",
        "You can classify images using the cosine similarity (times 100) as the logits to the softmax operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nqu4GlfPfr-p"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import CIFAR100\n",
        "\n",
        "cifar100 = CIFAR100('data', transform=preprocess, download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4S__zCGy2MT"
      },
      "outputs": [],
      "source": [
        "text_descriptions = [f\"This is a photo of a {label}\" for label in cifar100.classes]\n",
        "text_tokens = clip.tokenize(text_descriptions).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1XkwhsGdXmh"
      },
      "outputs": [],
      "source": [
        "print(cifar100.classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4z1fm9vCpSR"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    text_features = model.encode_text(text_tokens).float()\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "top_probs, top_labels = text_probs.cpu().topk(5, dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6Ju_6IBE2Iz"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16, 16))\n",
        "\n",
        "for i, image in enumerate(original_images):\n",
        "    plt.subplot(4, 4, 2 * i + 1)\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(4, 4, 2 * i + 2)\n",
        "    y = np.arange(top_probs.shape[-1])\n",
        "    plt.grid()\n",
        "    plt.barh(y, top_probs[i])\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.gca().set_axisbelow(True)\n",
        "    plt.yticks(y, [cifar100.classes[index] for index in top_labels[i].numpy()])\n",
        "    plt.xlabel(\"probability\")\n",
        "\n",
        "plt.subplots_adjust(wspace=0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_Mz9uXeqVFo"
      },
      "source": [
        "#### Try your own images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PBvUCG1oe6b"
      },
      "outputs": [],
      "source": [
        "!wget https://heronscrossing.vet/wp-content/uploads/Golden-Retriever-2048x1365.jpg\n",
        "!wget https://upload.wikimedia.org/wikipedia/commons/thumb/6/68/Orange_tabby_cat_sitting_on_fallen_leaves-Hisashi-01A.jpg/224px-Orange_tabby_cat_sitting_on_fallen_leaves-Hisashi-01A.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrNvMKi7qjlm"
      },
      "outputs": [],
      "source": [
        "# load image and get image feature\n",
        "orig_image = Image.open(\"Golden-Retriever-2048x1365.jpg\").convert(\"RGB\")\n",
        "\n",
        "# TODO\n",
        "# preprocess image, then convert to shape (1, 3, 224, 224) and move it to device\n",
        "image = None\n",
        "\n",
        "\n",
        "# TODO\n",
        "# extract image features and normalize\n",
        "with torch.no_grad():\n",
        "    image_features = None\n",
        "\n",
        "\n",
        "print(image_features.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2-tDW2jrS8E"
      },
      "outputs": [],
      "source": [
        "# text features\n",
        "classes = ['dog', 'cat']\n",
        "text_descriptions = [f\"This is a photo of a {label}\" for label in classes]\n",
        "\n",
        "# TODO\n",
        "# tokenize and move to device\n",
        "text_tokens = None\n",
        "\n",
        "# TODO\n",
        "# extract text features and normalize\n",
        "with torch.no_grad():\n",
        "    text_features = None\n",
        "\n",
        "print(text_features.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NieOZU0MsUsy"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "# compute probability for each class\n",
        "class_probs = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Wyf_nxksabB"
      },
      "outputs": [],
      "source": [
        "print(class_probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDKInpA2shH7"
      },
      "outputs": [],
      "source": [
        "print('Classification Result')\n",
        "for ix, label in enumerate(classes):\n",
        "    print(f\"{label}: {class_probs[0][ix] * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHXsENvvs9Ts"
      },
      "outputs": [],
      "source": [
        "# feel free to try your own image\n",
        "# and also try changing classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibzem59--6n_"
      },
      "source": [
        "## Evaluating CLIP on Zero-Shot Image Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5AxL3U7--Bz"
      },
      "outputs": [],
      "source": [
        "# We will evaluate CLIP on conventional image classification dataset (CIFAR10)\n",
        "\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "cifar10 = CIFAR10('data', transform=preprocess, download=True, train=False)\n",
        "\n",
        "test_loader = DataLoader(cifar10, batch_size=100, shuffle=False, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caOnioKtexN3"
      },
      "outputs": [],
      "source": [
        "print(cifar10.classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJRmPiDK_zJt"
      },
      "outputs": [],
      "source": [
        "# Preparing text features of 10 classes\n",
        "\n",
        "text_descriptions = [f\"This is a photo of a {label}\" for label in cifar10.classes]\n",
        "text_tokens = clip.tokenize(text_descriptions).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    text_features = model.encode_text(text_tokens).float()\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9lHrYO2_8T-"
      },
      "outputs": [],
      "source": [
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "for images, labels in tqdm(test_loader):\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    # TODO: extract image features, compute prediction, and compute accuracy\n",
        "    with torch.no_grad():\n",
        "        image_features = None\n",
        "\n",
        "    prediction = None\n",
        "    correct += None\n",
        "    total += None\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NRvaWMefG_C"
      },
      "source": [
        "### Prompt Tuning to Improve Zero-Shot Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsasUVC2fZSe"
      },
      "outputs": [],
      "source": [
        "templates = [\n",
        "        \"a photo of a {c}.\",\n",
        "        \"a blurry photo of a {c}.\",\n",
        "        \"a black and white photo of a {c}.\",\n",
        "        \"a low contrast photo of a {c}.\",\n",
        "        \"a high contrast photo of a {c}.\",\n",
        "        \"a bad photo of a {c}.\",\n",
        "        \"a good photo of a {c}.\",\n",
        "        \"a photo of a small {c}.\",\n",
        "        \"a photo of a big {c}.\",\n",
        "        \"a photo of the {c}.\",\n",
        "        \"a blurry photo of the {c}.\",\n",
        "        \"a black and white photo of the {c}.\",\n",
        "        \"a low contrast photo of the {c}.\",\n",
        "        \"a high contrast photo of the {c}.\",\n",
        "        \"a bad photo of the {c}.\",\n",
        "        \"a good photo of the {c}.\",\n",
        "        \"a photo of the small {c}.\",\n",
        "        \"a photo of the big {c}.\"\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYugWRF7fmmx"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "text_features = []\n",
        "for classname in cifar10.classes:\n",
        "    text_descriptions = [template.format(c=classname) for template in templates]\n",
        "    text_tokens = clip.tokenize(text_descriptions).to(device)\n",
        "    with torch.no_grad():\n",
        "        class_text_features = model.encode_text(text_tokens).float()\n",
        "        class_text_features /= class_text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        class_text_feature = class_text_features.mean(dim=0)\n",
        "        class_text_feature /= class_text_feature.norm(dim=-1, keepdim=True)\n",
        "    text_features.append(class_text_feature)\n",
        "\n",
        "text_features = torch.stack(text_features, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YryhV29zgr-W"
      },
      "outputs": [],
      "source": [
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "for images, labels in tqdm(test_loader):\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    # TODO: extract image features, compute prediction, and compute accuracy\n",
        "    with torch.no_grad():\n",
        "        image_features = None\n",
        "\n",
        "    prediction = None\n",
        "    correct += None\n",
        "    total += None\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8hyYp1Ygs26"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
