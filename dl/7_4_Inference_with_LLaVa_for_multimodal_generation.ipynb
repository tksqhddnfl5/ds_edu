{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Inference with LLaVa, a multimodal LLM\n",
        "\n",
        "LLaVa is an exciting new multimodal LLM which extends large language models like [LLaMa](https://huggingface.co/docs/transformers/model_doc/llama) with visual inputs.\n",
        "\n",
        "For multimodal LLMs, one typically takes a pre-trained/fine-tuned LLM and additionally conditions it on image features. In case of LLaVa, the image features come from a pre-trained [CLIP](https://huggingface.co/docs/transformers/model_doc/clip)'s vision encoder. To match the dimension of the image features with those of the text features, one applies a projection module, which could be a simple linear projection (like the original LLaVa), or more sophisticated like a two-layer MLP (used in LLaVa 1.5).\n",
        "\n",
        "One then trains the model to predict the next text token, given image features and text tokens.\n",
        "\n",
        "![image/png](https://cdn-uploads.huggingface.co/production/uploads/62441d1d9fdefb55a0b7d12c/FPshq08TKYD0e-qwPLDVO.png)\n",
        "\n",
        "* Transformers docs: https://huggingface.co/docs/transformers/main/en/model_doc/llava.\n",
        "* Original LLaVa paper: https://arxiv.org/abs/2304.08485\n",
        "* LLaVa 1.5 paper: https://arxiv.org/pdf/2310.03744.pdf\n",
        "\n",
        "## Set-up environment\n",
        "\n",
        "Let's start by installing the necessary libraries. Here we install Accelerate and Bitsandbytes in order to load the model in Google Colab. This enables [4-bit inference](https://huggingface.co/blog/4bit-transformers-bitsandbytes) with clever quantization techniques, shrinking the size of the model considerably, while maintaining performance of the original size."
      ],
      "metadata": {
        "id": "zHkkJ5CFY2xT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade -q accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "PuWVAAOinC8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also update the Transformers version of Google Colab to use the main branch, as the model is brand new at the time of writing."
      ],
      "metadata": {
        "id": "tvrTAVR3umXf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hfant_DrYrL4"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load model and processor\n",
        "\n",
        "Next, we load a model and corresponding processor from the hub. We specify device_map=\"auto\" in order to automatically place the model on the available GPUs/CPUs (see [this guide](https://huggingface.co/docs/accelerate/usage_guides/big_modeling) for details).\n",
        "\n",
        "Regarding [quantization](https://huggingface.co/blog/4bit-transformers-bitsandbytes), refer to the blog post for all details."
      ],
      "metadata": {
        "id": "R9eOO4G5kiOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "\n",
        "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "model = LlavaForConditionalGeneration.from_pretrained(model_id, quantization_config=quantization_config, device_map=\"auto\")"
      ],
      "metadata": {
        "id": "OD8C08WYY6db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "SBiiJAVNMUdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare image and text for the model"
      ],
      "metadata": {
        "id": "dhkgD9rfkuFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "\n",
        "image1 = Image.open(requests.get(\"https://llava-vl.github.io/static/images/view.jpg\", stream=True).raw)\n",
        "image2 = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n",
        "display(image1)\n",
        "display(image2)"
      ],
      "metadata": {
        "id": "MjlIoxq3u8ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the prompt, you can refer to images using the special \\<image> token. To indicate which text comes from a human vs. the model, one uses USER and ASSISTANT respectively. The format looks as follows:\n",
        "\n",
        "```bash\n",
        "USER: <image>\\n<prompt>\\nASSISTANT:\n",
        "```"
      ],
      "metadata": {
        "id": "LdzOupJqxKVZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In other words, you always need to end your prompt with `ASSISTANT:`. Here we will perform batched generation (i.e generating on several prompts)."
      ],
      "metadata": {
        "id": "BOnex7yYKGIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "            \"USER: <image>\\nWhat are the things I should be cautious about when I visit this place? What should I bring with me?\\nASSISTANT:\",\n",
        "            \"USER: <image>\\nPlease describe this image\\nASSISTANT:\",\n",
        "]\n",
        "\n",
        "inputs = processor(prompts, images=[image1, image2], padding=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "for k,v in inputs.items():\n",
        "  print(k,v.shape)"
      ],
      "metadata": {
        "id": "XbTjsy-uktg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autoregressively generate completion\n",
        "\n",
        "Finally, we simply let the model predict the next tokens given the images + prompt. Of course one can adjust all the [generation parameters](https://huggingface.co/docs/transformers/v4.35.2/en/main_classes/text_generation#transformers.GenerationMixin.generate). By default, greedy decoding is used."
      ],
      "metadata": {
        "id": "x9Lviy9zk3QW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.generate(**inputs, max_new_tokens=20)\n",
        "generated_text = processor.batch_decode(output, skip_special_tokens=True)\n",
        "for text in generated_text:\n",
        "  print(text.split(\"ASSISTANT:\")[-1])"
      ],
      "metadata": {
        "id": "XN2vJCPZk1UY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for text in generated_text:\n",
        "    print(text)"
      ],
      "metadata": {
        "id": "ehJv-TUDM49U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline API\n",
        "\n",
        "Alternatively, you can leverage the [pipeline](https://huggingface.co/docs/transformers/main_classes/pipelines) API which abstracts all of the logic above away for the user. We also provide the quantization config to make sure we leverage 4-bit inference."
      ],
      "metadata": {
        "id": "eS_YdZAKztu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"image-to-text\", model=model_id, model_kwargs={\"quantization_config\": quantization_config})"
      ],
      "metadata": {
        "id": "Xwf9cRoVqO1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_new_tokens = 200\n",
        "prompt = \"USER: <image>\\nWhat are the things I should be cautious about when I visit this place?\\nASSISTANT:\"\n",
        "\n",
        "outputs = pipe(image1, prompt=prompt, generate_kwargs={\"max_new_tokens\": 200})"
      ],
      "metadata": {
        "id": "W48r3NxDRskb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "XX80v0pgSr_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try your own image\n",
        "!wget https://heronscrossing.vet/wp-content/uploads/Golden-Retriever-2048x1365.jpg\n",
        "!wget https://upload.wikimedia.org/wikipedia/commons/thumb/6/68/Orange_tabby_cat_sitting_on_fallen_leaves-Hisashi-01A.jpg/224px-Orange_tabby_cat_sitting_on_fallen_leaves-Hisashi-01A.jpg"
      ],
      "metadata": {
        "id": "pb_YcZU80YIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_image = Image.open('Golden-Retriever-2048x1365.jpg')"
      ],
      "metadata": {
        "id": "Zb01DNMPNQPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_new_tokens = 200\n",
        "prompt = \"USER: <image>\\nWhat is the breed of the dog in the image?\\nASSISTANT:\"\n",
        "\n",
        "outputs = pipe(my_image, prompt=prompt, generate_kwargs={\"max_new_tokens\": 200})\n",
        "print(outputs[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "C6jgxb9iNVNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://media.cnn.com/api/v1/images/stellar/prod/160621115931-seoul-after.jpg?q=x_4,y_219,h_2370,w_4213,c_crop/h_833,w_1480"
      ],
      "metadata": {
        "id": "0Yssv7uXNeyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_image = Image.open('/content/160621115931-seoul-after.jpg?q=x_4,y_219,h_2370,w_4213,c_crop%2Fh_833,w_1480')\n",
        "max_new_tokens = 200\n",
        "prompt = \"USER: <image>\\nDescribe the city in the image\\nASSISTANT:\"\n",
        "\n",
        "outputs = pipe(my_image, prompt=prompt, generate_kwargs={\"max_new_tokens\": 200})\n",
        "print(outputs[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "YD2yxhgONwtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gncVZBO6N5JU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}