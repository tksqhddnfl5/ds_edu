{"cells":[{"cell_type":"markdown","metadata":{"id":"FTTCbheBgnQC"},"source":["# Linear probe using CLIP features\n","\n","In our previous tutorial, `Interacting_with_CLIP.ipynb`, we evaluated CLIP in zero-shot setting in which we use the cosine similarity between image features and label features as model prediction.\n","\n","In this tutorial, we will cover another approach for using pretrained models for classification tasks, namely, linear probe.\n","Unlike zero-shot classification, linear probe involves training using the training dataset.\n","However, to keep the training cost low, we only train a linear classifier on top of the frozen pretrained model.\n","\n","Side Note: Linear probe is not something new. Indeed, we did similar thing in CNN transfer learning tutorial, when we froze the main CNN and only trained linear classifier. The name 'linear probe' is often used in self-supervised learning literature to highlight that only the linear classifier is trained while the main network is being frozen. The name 'linear probe' is used because it evaluates the 'linear separability' of features learned during\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"3GdA9ohlgbJm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739320548416,"user_tz":-540,"elapsed":125329,"user":{"displayName":"Jihwan Moon","userId":"18370547518756862570"}},"outputId":"c5fbde3c-bc5b-480d-a174-2d3883cd4164"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ftfy\n","  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\n","Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: ftfy\n","Successfully installed ftfy-6.3.1\n","Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-rf5wdss2\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-rf5wdss2\n","  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (6.3.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (24.2)\n","Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.5.1+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.20.1+cu124)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.17.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2024.10.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->clip==1.0)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->clip==1.0)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->clip==1.0)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->clip==1.0)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->clip==1.0)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->clip==1.0)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch->clip==1.0)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->clip==1.0)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->clip==1.0)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->clip==1.0)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (1.26.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: clip\n","  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369489 sha256=b1cd83a3d8be340578e7a76cb79da64d79ab71c1d66e7bc64be719b4b2214585\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-1h6zw62s/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\n","Successfully built clip\n","Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, clip\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed clip-1.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"]}],"source":["! pip install ftfy regex tqdm\n","! pip install git+https://github.com/openai/CLIP.git"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"_C_1XGvIhMYU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739320556340,"user_tz":-540,"elapsed":7932,"user":{"displayName":"Jihwan Moon","userId":"18370547518756862570"}},"outputId":"0f500ae6-5828-4ec2-8038-caa027215346"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-2-cc4b7f78b657>:3: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n","  from pkg_resources import packaging\n"]},{"output_type":"stream","name":"stdout","text":["Torch version: 2.5.1+cu124\n"]}],"source":["\n","import numpy as np\n","import torch\n","from pkg_resources import packaging\n","\n","print(\"Torch version:\", torch.__version__)\n"]},{"cell_type":"markdown","metadata":{"id":"GQTKpQRZhNpc"},"source":["## Load model"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"T-WGj65thOYa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739320587327,"user_tz":-540,"elapsed":30995,"user":{"displayName":"Jihwan Moon","userId":"18370547518756862570"}},"outputId":"ffb59d51-d2aa-4683-a790-a632c5466b20"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['RN50',\n"," 'RN101',\n"," 'RN50x4',\n"," 'RN50x16',\n"," 'RN50x64',\n"," 'ViT-B/32',\n"," 'ViT-B/16',\n"," 'ViT-L/14',\n"," 'ViT-L/14@336px']"]},"metadata":{},"execution_count":3}],"source":["import clip\n","\n","clip.available_models()"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"JsKU-ToMhQbg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739320597100,"user_tz":-540,"elapsed":9781,"user":{"displayName":"Jihwan Moon","userId":"18370547518756862570"}},"outputId":"48bc4914-ca40-4dfd-d33c-384ed28f4c58"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|███████████████████████████████████████| 335M/335M [00:03<00:00, 99.3MiB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Model parameters: 149,620,737\n","Input resolution: 224\n","Context length: 77\n","Vocab size: 49408\n"]}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","model, preprocess = clip.load(\"ViT-B/16\")\n","model = model.to(device).eval()\n","input_resolution = model.visual.input_resolution\n","context_length = model.context_length\n","vocab_size = model.vocab_size\n","\n","print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n","print(\"Input resolution:\", input_resolution)\n","print(\"Context length:\", context_length)\n","print(\"Vocab size:\", vocab_size)"]},{"cell_type":"markdown","metadata":{"id":"StobnseThflW"},"source":["## Setting up train and test dataset"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"lcMXi3n0hhU3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739320602839,"user_tz":-540,"elapsed":5746,"user":{"displayName":"Jihwan Moon","userId":"18370547518756862570"}},"outputId":"bd785d77-6efa-4962-a5fa-b038e3e487e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170M/170M [00:02<00:00, 74.0MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/cifar-10-python.tar.gz to data\n","Files already downloaded and verified\n"]}],"source":["# We will evaluate CLIP on conventional image classification dataset (CIFAR10)\n","\n","from torchvision.datasets import CIFAR10\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","\n","cifar10_train = CIFAR10('data', transform=preprocess, download=True, train=True)\n","cifar10_test = CIFAR10('data', transform=preprocess, download=True, train=False)\n","\n","train_loader = DataLoader(cifar10_train, batch_size=100, shuffle=True, num_workers=2)\n","test_loader = DataLoader(cifar10_test, batch_size=100, shuffle=False, num_workers=2)\n"]},{"cell_type":"markdown","metadata":{"id":"bQC2aFkoiBoB"},"source":["## Linear probe option 1: using torch\n","\n","In the [CLIP paper](https://arxiv.org/pdf/2103.00020), the authors use image feature before projecting it to shared projection space for linear probe.\n","\n","To do so, we need to remove the projection layer (weight, to be specific) from the model"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"IA4UsCdOiDNO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739320754902,"user_tz":-540,"elapsed":753,"user":{"displayName":"Jihwan Moon","userId":"18370547518756862570"}},"outputId":"829a1ad6-94ca-4624-80c7-3280d6eb28f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 3, 224, 224])\n","torch.Size([1, 512])\n","torch.Size([1, 768])\n"]}],"source":["# See: https://github.com/openai/CLIP/blob/main/clip/model.py\n","\n","sample_image = cifar10_test[0][0].unsqueeze(0).to(device)  # (1, 3, 224, 224)\n","print(sample_image.shape)\n","\n","# before removing projection weight\n","with torch.no_grad():\n","    out_before = model.encode_image(sample_image).float()\n","print(out_before.shape)\n","\n","# after removing projection weight\n","visual_proj = model.visual.proj\n","model.visual.proj = None\n","\n","with torch.no_grad():\n","    out_after = model.encode_image(sample_image).float()\n","print(out_after.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NHT4TjiIlCmo"},"outputs":[],"source":["import torch.nn as nn\n","import torch.optim as optim\n","\n","linear_classifier = nn.Linear(768, 10).to(device)\n","optimizer = optim.Adam(linear_classifier.parameters(), lr=1e-3)\n","criterion = nn.CrossEntropyLoss()\n","\n","for epoch in range(3):\n","    for x, y in tqdm(train_loader):\n","        x, y = x.to(device), y.to(device)\n","        # TODO: compute loss and update parameter using optimizer.step()\n","        # 1. extract image feature and convert its dtype to float\n","        # 2. compute logits using linear_classifier and image feature\n","        # 3. compute loss using criterion\n","\n","    # run evaluation every epoch\n","    test_loss = 0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for x, y in tqdm(test_loader):\n","            x, y = x.to(device), y.to(device)\n","            image_feature = model.encode_image(x).float()\n","            logits = linear_classifier(image_feature)\n","            loss = criterion(logits, y)\n","            test_loss += loss.item() * len(y)\n","            correct += (logits.argmax(dim=1) == y).sum().item()\n","            total += len(y)\n","    test_loss = test_loss / len(cifar10_test)\n","    test_acc = correct / total\n","\n","    print()\n","    print(f\"[Epoch {epoch+1}] test_loss: {test_loss:.4f}, test_acc: {test_acc * 100:.2f}%\")\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9kQ1BRyIiDlj"},"source":["## Linear probe option 2: using external library\n","\n","Another possible way to train a linear classifier on top of the learned feature is to first extract image features for all images and then use external library (e.g., scikit-learn) to train a linear classifier.\n","\n","This allows us to easily use more complicated optimization algorithms implemented in scikit-learn, such as [L-BFGS](https://ko.wikipedia.org/wiki/L-BFGS) which is a [Quasi-Newton Method](https://en.wikipedia.org/wiki/Quasi-Newton_method).\n","\n","In fact, the [CLIP paper](https://arxiv.org/pdf/2103.00020) uses this approach for linear probe evaluation (see Appendix A.3)\n","\n","\"We train a logistic regression classifier using scikit-learn’s L-BFGS implementation, with maximum 1,000 iteration\"\n","\n","scikit-learn LogisticRegression: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n","\n","However, if you need data augmentation, the first approach is preferable."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1SiK9FUWjQEV"},"outputs":[],"source":["import numpy as np\n","\n","# TODO:\n","# extract train image features, convert to numpy\n","# store both image feature and label (y) as numpy arrays, each with name `train_features` and `train_labels`\n","\n","print()\n","print(train_features.shape)\n","print(train_labels.shape)\n","\n","# TODO:\n","# extract test image features, convert to numpy\n","# store both image feature and label (y) as numpy arrays, each with name `test_features` and `test_labels`\n","\n","print()\n","print(test_features.shape)\n","print(test_labels.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vg9cs0UjpgsF"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","\n","C = 0.1\n","logistic_regression = LogisticRegression(solver=\"lbfgs\", max_iter=1000, C=C)\n","logistic_regression.fit(train_features, train_labels)\n","\n","test_pred = logistic_regression.predict(test_features)\n","test_acc = (test_pred == test_labels).sum() / len(test_labels)\n","print(f\"Test acc: {test_acc * 100:.2f}%\")"]},{"cell_type":"markdown","metadata":{"id":"dLA7LHxau4Cn"},"source":["## Exercise: Linear probe vs zero-shot classification on CIFAR100\n","\n","1. Compute zero-shot classification accuracy of CLIP on CIFAR100 as in tutorial `7_1_Interacting_with_CLIP.ipynb`.\n","\n","2. Implement linear probe evaluation on CIFAR100 (option 2 using scikit-learn).\n","\n","3. Compare results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"otIdDzEB1qKT"},"outputs":[],"source":["from torchvision.datasets import CIFAR100\n","\n","cifar100_train = CIFAR100('data', transform=preprocess, download=True, train=True)\n","cifar100_test = CIFAR100('data', transform=preprocess, download=True, train=False)\n","\n","train_loader = DataLoader(cifar100_train, batch_size=100, shuffle=True, num_workers=2)\n","test_loader = DataLoader(cifar100_test, batch_size=100, shuffle=False, num_workers=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UWb0vF0ZtYES"},"outputs":[],"source":["# TODO 1: zero-shot classification\n","\n","# make sure to re-set model.visual.proj with visual_proj for zero-shot classification\n","model.visual.proj = visual_proj\n","\n","\n","print()\n","print(f\"Accuracy: {accuracy * 100:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"peRT0Xi3yXyu"},"outputs":[],"source":["# TODO 2: linear probe evaluation option 2\n","\n","# make sure to remove model.visual.proj for linear probe\n","model.visual.proj = None\n","\n","\n","print(f\"Test acc: {test_acc * 100:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9rlC9Jlq7XkI"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}