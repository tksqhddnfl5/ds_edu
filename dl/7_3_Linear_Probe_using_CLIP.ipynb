{"cells":[{"cell_type":"markdown","metadata":{"id":"FTTCbheBgnQC"},"source":["# Linear probe using CLIP features\n","\n","In our previous tutorial, `Interacting_with_CLIP.ipynb`, we evaluated CLIP in zero-shot setting in which we use the cosine similarity between image features and label features as model prediction.\n","\n","In this tutorial, we will cover another approach for using pretrained models for classification tasks, namely, linear probe.\n","Unlike zero-shot classification, linear probe involves training using the training dataset.\n","However, to keep the training cost low, we only train a linear classifier on top of the frozen pretrained model.\n","\n","Side Note: Linear probe is not something new. Indeed, we did similar thing in CNN transfer learning tutorial, when we froze the main CNN and only trained linear classifier. The name 'linear probe' is often used in self-supervised learning literature to highlight that only the linear classifier is trained while the main network is being frozen. The name 'linear probe' is used because it evaluates the 'linear separability' of features learned during\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":125329,"status":"ok","timestamp":1739320548416,"user":{"displayName":"Jihwan Moon","userId":"18370547518756862570"},"user_tz":-540},"id":"3GdA9ohlgbJm","outputId":"c5fbde3c-bc5b-480d-a174-2d3883cd4164"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to c:\\users\\hp\\appdata\\local\\temp\\pip-req-build-vgj2sb9e\n","  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n","  Installing build dependencies: started\n","  Installing build dependencies: finished with status 'done'\n","  Getting requirements to build wheel: started\n","  Getting requirements to build wheel: finished with status 'done'\n","  Preparing metadata (pyproject.toml): started\n","  Preparing metadata (pyproject.toml): finished with status 'done'\n","Requirement already satisfied: ftfy in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from clip==1.0) (6.3.1)\n","Requirement already satisfied: packaging in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from clip==1.0) (24.1)\n","Requirement already satisfied: regex in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from clip==1.0) (2024.11.6)\n","Requirement already satisfied: tqdm in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from clip==1.0) (4.66.5)\n","Requirement already satisfied: torch in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from clip==1.0) (2.4.1+cu121)\n","Requirement already satisfied: torchvision in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from clip==1.0) (0.19.1)\n","Requirement already satisfied: wcwidth in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from ftfy->clip==1.0) (0.2.13)\n","Requirement already satisfied: filelock in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from torch->clip==1.0) (3.16.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from torch->clip==1.0) (4.12.2)\n","Requirement already satisfied: sympy in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from torch->clip==1.0) (1.13.2)\n","Requirement already satisfied: networkx in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from torch->clip==1.0) (3.2.1)\n","Requirement already satisfied: jinja2 in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from torch->clip==1.0) (3.1.4)\n","Requirement already satisfied: fsspec in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from torch->clip==1.0) (2024.9.0)\n","Requirement already satisfied: numpy in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchvision->clip==1.0) (1.26.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from torchvision->clip==1.0) (10.4.0)\n","Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from tqdm->clip==1.0) (0.4.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from jinja2->torch->clip==1.0) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from sympy->torch->clip==1.0) (1.3.0)\n"]},{"name":"stderr","output_type":"stream","text":["  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git 'C:\\Users\\HP\\AppData\\Local\\Temp\\pip-req-build-vgj2sb9e'\n"]}],"source":["# ! pip install ftfy regex tqdm\n","! pip install git+https://github.com/openai/CLIP.git"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7932,"status":"ok","timestamp":1739320556340,"user":{"displayName":"Jihwan Moon","userId":"18370547518756862570"},"user_tz":-540},"id":"_C_1XGvIhMYU","outputId":"0f500ae6-5828-4ec2-8038-caa027215346"},"outputs":[{"name":"stdout","output_type":"stream","text":["Torch version: 2.4.1+cu121\n"]}],"source":["\n","import numpy as np\n","import torch\n","from pkg_resources import packaging\n","\n","print(\"Torch version:\", torch.__version__)\n"]},{"cell_type":"markdown","metadata":{"id":"GQTKpQRZhNpc"},"source":["## Load model"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30995,"status":"ok","timestamp":1739320587327,"user":{"displayName":"Jihwan Moon","userId":"18370547518756862570"},"user_tz":-540},"id":"T-WGj65thOYa","outputId":"ffb59d51-d2aa-4683-a790-a632c5466b20"},"outputs":[{"data":{"text/plain":["['RN50',\n"," 'RN101',\n"," 'RN50x4',\n"," 'RN50x16',\n"," 'RN50x64',\n"," 'ViT-B/32',\n"," 'ViT-B/16',\n"," 'ViT-L/14',\n"," 'ViT-L/14@336px']"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import clip\n","\n","clip.available_models()"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9781,"status":"ok","timestamp":1739320597100,"user":{"displayName":"Jihwan Moon","userId":"18370547518756862570"},"user_tz":-540},"id":"JsKU-ToMhQbg","outputId":"48bc4914-ca40-4dfd-d33c-384ed28f4c58"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model parameters: 149,620,737\n","Input resolution: 224\n","Context length: 77\n","Vocab size: 49408\n"]}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","model, preprocess = clip.load(\"ViT-B/16\")\n","model = model.to(device).eval()\n","input_resolution = model.visual.input_resolution\n","context_length = model.context_length\n","vocab_size = model.vocab_size\n","\n","print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n","print(\"Input resolution:\", input_resolution)\n","print(\"Context length:\", context_length)\n","print(\"Vocab size:\", vocab_size)"]},{"cell_type":"markdown","metadata":{"id":"StobnseThflW"},"source":["## Setting up train and test dataset"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5746,"status":"ok","timestamp":1739320602839,"user":{"displayName":"Jihwan Moon","userId":"18370547518756862570"},"user_tz":-540},"id":"lcMXi3n0hhU3","outputId":"bd785d77-6efa-4962-a5fa-b038e3e487e3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]}],"source":["# We will evaluate CLIP on conventional image classification dataset (CIFAR10)\n","\n","from torchvision.datasets import CIFAR10\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","\n","cifar10_train = CIFAR10('D:/data', transform=preprocess, download=True, train=True)\n","cifar10_test = CIFAR10('D:/data', transform=preprocess, download=True, train=False)\n","\n","train_loader = DataLoader(cifar10_train, batch_size=100, shuffle=True, num_workers=2)\n","test_loader = DataLoader(cifar10_test, batch_size=100, shuffle=False, num_workers=2)\n"]},{"cell_type":"markdown","metadata":{"id":"bQC2aFkoiBoB"},"source":["## Linear probe option 1: using torch\n","\n","In the [CLIP paper](https://arxiv.org/pdf/2103.00020), the authors use image feature before projecting it to shared projection space for linear probe.\n","\n","To do so, we need to remove the projection layer (weight, to be specific) from the model"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":753,"status":"ok","timestamp":1739320754902,"user":{"displayName":"Jihwan Moon","userId":"18370547518756862570"},"user_tz":-540},"id":"IA4UsCdOiDNO","outputId":"829a1ad6-94ca-4624-80c7-3280d6eb28f1"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([1, 3, 224, 224])\n","torch.Size([1, 512])\n","torch.Size([1, 768])\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n","  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"]}],"source":["# See: https://github.com/openai/CLIP/blob/main/clip/model.py\n","\n","sample_image = cifar10_test[0][0].unsqueeze(0).to(device)  # (1, 3, 224, 224)\n","print(sample_image.shape)\n","\n","# before removing projection weight\n","with torch.no_grad():\n","    out_before = model.encode_image(sample_image).float()\n","print(out_before.shape)\n","\n","# after removing projection weight\n","visual_proj = model.visual.proj\n","model.visual.proj = None\n","\n","with torch.no_grad():\n","    out_after = model.encode_image(sample_image).float()\n","print(out_after.shape)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"NHT4TjiIlCmo"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 500/500 [01:15<00:00,  6.66it/s]\n","100%|██████████| 100/100 [00:19<00:00,  5.13it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 1] test_loss: 0.1487, test_acc: 94.99%\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 500/500 [01:14<00:00,  6.67it/s]\n","100%|██████████| 100/100 [00:19<00:00,  5.10it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 2] test_loss: 0.1418, test_acc: 95.41%\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 500/500 [01:14<00:00,  6.70it/s]\n","100%|██████████| 100/100 [00:19<00:00,  5.12it/s]"]},{"name":"stdout","output_type":"stream","text":["[Epoch 3] test_loss: 0.1346, test_acc: 95.59%\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["import torch.nn as nn\n","import torch.optim as optim\n","\n","linear_classifier = nn.Linear(768, 10).to(device)\n","optimizer = optim.Adam(linear_classifier.parameters(), lr=1e-3)\n","criterion = nn.CrossEntropyLoss()\n","\n","for epoch in range(3):\n","    for x, y in tqdm(train_loader):\n","        x, y = x.to(device), y.to(device)\n","        with torch.no_grad():\n","            image_feature = model.encode_image(x).float()\n","        logits = linear_classifier(image_feature)\n","        loss = criterion(logits, y)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    test_loss = 0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for x, y in tqdm(test_loader):\n","            x, y = x.to(device), y.to(device)\n","            image_feature = model.encode_image(x).float()\n","            logits = linear_classifier(image_feature)\n","            loss = criterion(logits, y)\n","            test_loss += loss.item() * len(y)\n","            correct += (logits.argmax(dim=1) == y).sum().item()\n","            total += len(y)\n","    test_loss /= len(cifar10_test)\n","    test_acc = correct / total\n","\n","    print(f\"[Epoch {epoch+1}] test_loss: {test_loss:.4f}, test_acc: {test_acc * 100:.2f}%\")\n"]},{"cell_type":"markdown","metadata":{"id":"9kQ1BRyIiDlj"},"source":["## Linear probe option 2: using external library\n","\n","Another possible way to train a linear classifier on top of the learned feature is to first extract image features for all images and then use external library (e.g., scikit-learn) to train a linear classifier.\n","\n","This allows us to easily use more complicated optimization algorithms implemented in scikit-learn, such as [L-BFGS](https://ko.wikipedia.org/wiki/L-BFGS) which is a [Quasi-Newton Method](https://en.wikipedia.org/wiki/Quasi-Newton_method).\n","\n","In fact, the [CLIP paper](https://arxiv.org/pdf/2103.00020) uses this approach for linear probe evaluation (see Appendix A.3)\n","\n","\"We train a logistic regression classifier using scikit-learn’s L-BFGS implementation, with maximum 1,000 iteration\"\n","\n","scikit-learn LogisticRegression: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n","\n","However, if you need data augmentation, the first approach is preferable."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"1SiK9FUWjQEV"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 500/500 [01:20<00:00,  6.25it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","(50000, 768)\n","(50000,)\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:19<00:00,  5.01it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","(10000, 768)\n","(10000,)\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["import numpy as np\n","\n","# TODO:\n","# extract train image features, convert to numpy\n","# store both image feature and label (y) as numpy arrays, each with name `train_features` and `train_labels`\n","train_features = []\n","train_labels = []\n","for x, y in tqdm(train_loader):\n","    x = x.to(device)\n","    with torch.no_grad():\n","        image_feature = model.encode_image(x).float()\n","    train_features.append(image_feature.cpu().numpy())\n","    train_labels.append(y.numpy())\n","\n","train_features = np.concatenate(train_features, axis=0)\n","train_labels = np.concatenate(train_labels, axis=0)\n","\n","print()\n","print(train_features.shape)\n","print(train_labels.shape)\n","\n","# TODO:\n","# extract test image features, convert to numpy\n","# store both image feature and label (y) as numpy arrays, each with name `test_features` and `test_labels`\n","\n","test_features = []\n","test_labels = []\n","for x, y in tqdm(test_loader):\n","    x = x.to(device)\n","    with torch.no_grad():\n","        image_feature = model.encode_image(x).float()\n","    test_features.append(image_feature.cpu().numpy())\n","    test_labels.append(y.numpy())\n","\n","test_features = np.concatenate(test_features, axis=0)\n","test_labels = np.concatenate(test_labels, axis=0)\n","\n","print()\n","print(test_features.shape)\n","print(test_labels.shape)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"Vg9cs0UjpgsF"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test acc: 96.08%\n"]}],"source":["from sklearn.linear_model import LogisticRegression\n","\n","C = 0.1\n","logistic_regression = LogisticRegression(solver=\"lbfgs\", max_iter=1000, C=C)\n","logistic_regression.fit(train_features, train_labels)\n","\n","test_pred = logistic_regression.predict(test_features)\n","test_acc = (test_pred == test_labels).sum() / len(test_labels)\n","print(f\"Test acc: {test_acc * 100:.2f}%\")"]},{"cell_type":"markdown","metadata":{"id":"dLA7LHxau4Cn"},"source":["## Exercise: Linear probe vs zero-shot classification on CIFAR100\n","\n","1. Compute zero-shot classification accuracy of CLIP on CIFAR100 as in tutorial `7_1_Interacting_with_CLIP.ipynb`.\n","\n","2. Implement linear probe evaluation on CIFAR100 (option 2 using scikit-learn).\n","\n","3. Compare results."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"otIdDzEB1qKT"},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]}],"source":["from torchvision.datasets import CIFAR100\n","\n","cifar100_train = CIFAR100('D:/data', transform=preprocess, download=True, train=True)\n","cifar100_test = CIFAR100('D:/data', transform=preprocess, download=True, train=False)\n","\n","train_loader = DataLoader(cifar100_train, batch_size=100, shuffle=True, num_workers=2)\n","test_loader = DataLoader(cifar100_test, batch_size=100, shuffle=False, num_workers=2)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"UWb0vF0ZtYES"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:19<00:00,  5.23it/s]"]},{"name":"stdout","output_type":"stream","text":["Accuracy: 66.94%\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# TODO 1: zero-shot classification\n","\n","# make sure to re-set model.visual.proj with visual_proj for zero-shot classification\n","model.visual.proj = visual_proj\n","\n","templates = [\n","        \"a photo of a {c}.\",\n","        \"a blurry photo of a {c}.\",\n","        \"a black and white photo of a {c}.\",\n","        \"a low contrast photo of a {c}.\",\n","        \"a high contrast photo of a {c}.\",\n","        \"a bad photo of a {c}.\",\n","        \"a good photo of a {c}.\",\n","        \"a photo of a small {c}.\",\n","        \"a photo of a big {c}.\",\n","        \"a photo of the {c}.\",\n","        \"a blurry photo of the {c}.\",\n","        \"a black and white photo of the {c}.\",\n","        \"a low contrast photo of the {c}.\",\n","        \"a high contrast photo of the {c}.\",\n","        \"a bad photo of the {c}.\",\n","        \"a good photo of the {c}.\",\n","        \"a photo of the small {c}.\",\n","        \"a photo of the big {c}.\"\n","    ]\n","\n","text_features = []\n","for classname in cifar100_test.classes:\n","    text_descriptions = [template.format(c=classname) for template in templates]\n","    text_tokens = clip.tokenize(text_descriptions).to(device)\n","    with torch.no_grad():\n","        class_text_features = model.encode_text(text_tokens).float()\n","        class_text_features /= class_text_features.norm(dim=-1, keepdim=True)\n","\n","        class_text_feature = class_text_features.mean(dim=0)\n","        class_text_feature /= class_text_feature.norm(dim=-1, keepdim=True)\n","    text_features.append(class_text_feature)\n","\n","text_features = torch.stack(text_features, dim=0)\n","\n","\n","correct = 0\n","total = 0\n","for images, labels in tqdm(test_loader):\n","    images, labels = images.to(device), labels.to(device)\n","\n","    # TODO: extract image features, compute prediction, and compute accuracy\n","    with torch.no_grad():\n","        image_features = model.encode_image(images).float()\n","        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n","\n","    prediction = (image_features @ text_features.T).argmax(dim=1)\n","    correct += (prediction == labels).sum().item()\n","    total += len(labels)\n","\n","accuracy = correct / total\n","print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"peRT0Xi3yXyu"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 500/500 [01:20<00:00,  6.22it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","(50000, 768)\n","(50000,)\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:19<00:00,  5.11it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","(10000, 768)\n","(10000,)\n","Test acc: 82.64%\n"]}],"source":["# TODO 2: linear probe evaluation option 2\n","\n","# make sure to remove model.visual.proj for linear probe\n","model.visual.proj = None\n","\n","# TODO:\n","# extract train image features, convert to numpy\n","# store both image feature and label (y) as numpy arrays, each with name `train_features` and `train_labels`\n","train_features = []\n","train_labels = []\n","for x, y in tqdm(train_loader):\n","    x = x.to(device)\n","    with torch.no_grad():\n","        image_feature = model.encode_image(x).float()\n","    train_features.append(image_feature.cpu().numpy())\n","    train_labels.append(y.numpy())\n","\n","train_features = np.concatenate(train_features, axis=0)\n","train_labels = np.concatenate(train_labels, axis=0)\n","\n","print()\n","print(train_features.shape)\n","print(train_labels.shape)\n","\n","# TODO:\n","# extract test image features, convert to numpy\n","# store both image feature and label (y) as numpy arrays, each with name `test_features` and `test_labels`\n","\n","test_features = []\n","test_labels = []\n","for x, y in tqdm(test_loader):\n","    x = x.to(device)\n","    with torch.no_grad():\n","        image_feature = model.encode_image(x).float()\n","    test_features.append(image_feature.cpu().numpy())\n","    test_labels.append(y.numpy())\n","\n","test_features = np.concatenate(test_features, axis=0)\n","test_labels = np.concatenate(test_labels, axis=0)\n","\n","print()\n","print(test_features.shape)\n","print(test_labels.shape)\n","\n","C = 0.1\n","logistic_regression = LogisticRegression(solver=\"lbfgs\", max_iter=1000, C=C)\n","logistic_regression.fit(train_features, train_labels)\n","\n","test_pred = logistic_regression.predict(test_features)\n","test_acc = (test_pred == test_labels).sum() / len(test_labels)\n","print(f\"Test acc: {test_acc * 100:.2f}%\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9rlC9Jlq7XkI"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.11"}},"nbformat":4,"nbformat_minor":0}
