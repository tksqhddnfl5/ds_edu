{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTTCbheBgnQC"
      },
      "source": [
        "# Linear probe using CLIP features\n",
        "\n",
        "In our previous tutorial, `Interacting_with_CLIP.ipynb`, we evaluated CLIP in zero-shot setting in which we use the cosine similarity between image features and label features as model prediction.\n",
        "\n",
        "In this tutorial, we will cover another approach for using pretrained models for classification tasks, namely, linear probe.\n",
        "Unlike zero-shot classification, linear probe involves training using the training dataset.\n",
        "However, to keep the training cost low, we only train a linear classifier on top of the frozen pretrained model.\n",
        "\n",
        "Side Note: Linear probe is not something new. Indeed, we did similar thing in CNN transfer learning tutorial, when we froze the main CNN and only trained linear classifier. The name 'linear probe' is often used in self-supervised learning literature to highlight that only the linear classifier is trained while the main network is being frozen.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GdA9ohlgbJm"
      },
      "outputs": [],
      "source": [
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_C_1XGvIhMYU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from pkg_resources import packaging\n",
        "\n",
        "print(\"Torch version:\", torch.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQTKpQRZhNpc"
      },
      "source": [
        "## Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-WGj65thOYa"
      },
      "outputs": [],
      "source": [
        "import clip\n",
        "\n",
        "clip.available_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsKU-ToMhQbg"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model, preprocess = clip.load(\"ViT-B/16\")\n",
        "model = model.to(device).eval()\n",
        "input_resolution = model.visual.input_resolution\n",
        "context_length = model.context_length\n",
        "vocab_size = model.vocab_size\n",
        "\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "print(\"Input resolution:\", input_resolution)\n",
        "print(\"Context length:\", context_length)\n",
        "print(\"Vocab size:\", vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StobnseThflW"
      },
      "source": [
        "## Setting up train and test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcMXi3n0hhU3"
      },
      "outputs": [],
      "source": [
        "# We will evaluate CLIP on conventional image classification dataset (CIFAR10)\n",
        "\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "cifar10_train = CIFAR10('data', transform=preprocess, download=True, train=True)\n",
        "cifar10_test = CIFAR10('data', transform=preprocess, download=True, train=False)\n",
        "\n",
        "train_loader = DataLoader(cifar10_train, batch_size=100, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(cifar10_test, batch_size=100, shuffle=False, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQC2aFkoiBoB"
      },
      "source": [
        "## Linear probe option 1: using torch\n",
        "\n",
        "In the [CLIP paper](https://arxiv.org/pdf/2103.00020), the authors use image feature before projecting it to shared projection space for linear probe.\n",
        "\n",
        "To do so, we need to remove the projection layer (weight, to be specific) from the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IA4UsCdOiDNO"
      },
      "outputs": [],
      "source": [
        "# See: https://github.com/openai/CLIP/blob/main/clip/model.py\n",
        "\n",
        "sample_image = cifar10_test[0][0].unsqueeze(0).to(device)  # (1, 3, 224, 224)\n",
        "print(sample_image.shape)\n",
        "\n",
        "# before removing projection weight\n",
        "with torch.no_grad():\n",
        "    out_before = model.encode_image(sample_image).float()\n",
        "print(out_before.shape)\n",
        "\n",
        "# after removing projection weight\n",
        "visual_proj = model.visual.proj\n",
        "model.visual.proj = None\n",
        "\n",
        "with torch.no_grad():\n",
        "    out_after = model.encode_image(sample_image).float()\n",
        "print(out_after.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHT4TjiIlCmo"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "linear_classifier = nn.Linear(768, 10).to(device)\n",
        "optimizer = optim.Adam(linear_classifier.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(3):\n",
        "    for x, y in tqdm(train_loader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        # TODO: compute loss and update parameter using optimizer.step()\n",
        "        # 1. extract image feature and convert its dtype to float\n",
        "        # 2. compute logits using linear_classifier and image feature\n",
        "        # 3. compute loss using criterion\n",
        "\n",
        "    # run evaluation every epoch\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in tqdm(test_loader):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            image_feature = model.encode_image(x).float()\n",
        "            logits = linear_classifier(image_feature)\n",
        "            loss = criterion(logits, y)\n",
        "            test_loss += loss.item() * len(y)\n",
        "            correct += (logits.argmax(dim=1) == y).sum().item()\n",
        "            total += len(y)\n",
        "    test_loss = test_loss / len(cifar10_test)\n",
        "    test_acc = correct / total\n",
        "\n",
        "    print()\n",
        "    print(f\"[Epoch {epoch+1}] test_loss: {test_loss:.4f}, test_acc: {test_acc * 100:.2f}%\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kQ1BRyIiDlj"
      },
      "source": [
        "## Linear probe option 2: using external library\n",
        "\n",
        "Another possible way to train a linear classifier on top of the learned feature is to first extract image features for all images and then use external library (e.g., scikit-learn) to train a linear classifier.\n",
        "\n",
        "This allows us to easily use more complicated optimization algorithms implemented in scikit-learn, such as [L-BFGS](https://ko.wikipedia.org/wiki/L-BFGS) which is a [Quasi-Newton Method](https://en.wikipedia.org/wiki/Quasi-Newton_method).\n",
        "\n",
        "In fact, the [CLIP paper](https://arxiv.org/pdf/2103.00020) uses this approach for linear probe evaluation (see Appendix A.3)\n",
        "\n",
        "\"We train a logistic regression classifier using scikit-learnâ€™s L-BFGS implementation, with maximum 1,000 iteration\"\n",
        "\n",
        "scikit-learn LogisticRegression: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
        "\n",
        "However, if you need data augmentation, the first approach is preferable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SiK9FUWjQEV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# TODO:\n",
        "# extract train image features, convert to numpy\n",
        "# store both image feature and label (y) as numpy arrays, each with name `train_features` and `train_labels`\n",
        "\n",
        "print()\n",
        "print(train_features.shape)\n",
        "print(train_labels.shape)\n",
        "\n",
        "# TODO:\n",
        "# extract test image features, convert to numpy\n",
        "# store both image feature and label (y) as numpy arrays, each with name `test_features` and `test_labels`\n",
        "\n",
        "print()\n",
        "print(test_features.shape)\n",
        "print(test_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vg9cs0UjpgsF"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "C = 0.1\n",
        "logistic_regression = LogisticRegression(solver=\"lbfgs\", max_iter=1000, C=C)\n",
        "logistic_regression.fit(train_features, train_labels)\n",
        "\n",
        "test_pred = logistic_regression.predict(test_features)\n",
        "test_acc = (test_pred == test_labels).sum() / len(test_labels)\n",
        "print(f\"Test acc: {test_acc * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLA7LHxau4Cn"
      },
      "source": [
        "## Exercise: Linear probe vs zero-shot classification on CIFAR100\n",
        "\n",
        "1. Compute zero-shot classification accuracy of CLIP on CIFAR100 as in tutorial `7_1_Interacting_with_CLIP.ipynb`.\n",
        "\n",
        "2. Implement linear probe evaluation on CIFAR100 (option 2 using scikit-learn).\n",
        "\n",
        "3. Compare results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otIdDzEB1qKT"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import CIFAR100\n",
        "\n",
        "cifar100_train = CIFAR100('data', transform=preprocess, download=True, train=True)\n",
        "cifar100_test = CIFAR100('data', transform=preprocess, download=True, train=False)\n",
        "\n",
        "train_loader = DataLoader(cifar100_train, batch_size=100, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(cifar100_test, batch_size=100, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWb0vF0ZtYES"
      },
      "outputs": [],
      "source": [
        "# TODO 1: zero-shot classification\n",
        "\n",
        "# make sure to re-set model.visual.proj with visual_proj for zero-shot classification\n",
        "model.visual.proj = visual_proj\n",
        "\n",
        "\n",
        "print()\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peRT0Xi3yXyu"
      },
      "outputs": [],
      "source": [
        "# TODO 2: linear probe evaluation option 2\n",
        "\n",
        "# make sure to remove model.visual.proj for linear probe\n",
        "model.visual.proj = None\n",
        "\n",
        "\n",
        "print(f\"Test acc: {test_acc * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rlC9Jlq7XkI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
