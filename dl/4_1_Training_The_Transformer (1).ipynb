{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36k-EDb8F9Eh"
   },
   "source": [
    "## Transformers from Scratch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31141,
     "status": "ok",
     "timestamp": 1739172769055,
     "user": {
      "displayName": "Eunkyu Park",
      "userId": "11893515073306472432"
     },
     "user_tz": -540
    },
    "id": "Zr3QR_G0UTsY",
    "outputId": "7b59e18d-381c-4603-bda5-344cc02da17a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\edu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "from math import sqrt\n",
    "from torch import nn\n",
    "from loguru import logger\n",
    "from typing import Optional\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BruiP950Q9xC"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    value: torch.Tensor,\n",
    "    mask: Optional[torch.Tensor] = None,\n",
    "    dropout: Optional[nn.Dropout] = None\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute scaled dot product attention weights.\n",
    "\n",
    "    Args:\n",
    "        query: Tensor with shape [batch_size, seq_length_q, depth_q].\n",
    "        key: Tensor with shape [batch_size, seq_length_k, depth_k].\n",
    "        value: Tensor with shape [batch_size, seq_length_v, depth_v].\n",
    "        mask: Optional tensor with shape [batch_size, seq_length_q, seq_length_k],\n",
    "            containing values to be masked. Default is None.\n",
    "\n",
    "    Returns:\n",
    "        Tensor with shape [batch_size, seq_length_q, depth_v].\n",
    "    \"\"\"\n",
    "\n",
    "    dim_k = query.size(-1)\n",
    "    logger.debug(f\"query_size: {query.size()}\")\n",
    "    logger.debug(f\"key: {key.transpose(-2, -1).size()}\")\n",
    "    # TODO\n",
    "    # Compute the attention scores.\n",
    "    # Scale the dot product simialrity between the query and the key tensors.\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / sqrt(dim_k)\n",
    "    if mask is not None:\n",
    "        mask = mask.unsqueeze(1)\n",
    "        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "    # TODO:\n",
    "    # Compute attention weights w.\n",
    "    # Apply the softmax function to the scaled similarity matrix.\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "    if dropout is not None:\n",
    "        weights = dropout(weights)\n",
    "    # TODO:\n",
    "    # Update the token embeddings.\n",
    "    # Multiply the attention weights w by the value vectors V\n",
    "    # to obtain a weighted sum of the values.\n",
    "    return torch.matmul(weights, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emCFMXzTud9I"
   },
   "source": [
    "#### 1.2. Multi-head Attention\n",
    "- The multi-head attention is an extension of the self-attention mechanism. It enhances the modeling capability by performing multiple attention computations in parallel, with different learned linear projections.\n",
    "$$ \\begin{matrix}\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1,...,\\text{head}_h)W^O\\\\\n",
    "\\text{where}~\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K,VW_i^V)\n",
    "\\end{matrix} $$\n",
    "- Steps:\n",
    "  1. It applies linear transformations to the query, key, and value tensors using the learned linear layers self.q, self.k, and self.v, respectively. This projects the tensors to the appropriate dimensions for attention computation.\n",
    "  2. The attention scores are computed by performing matrix multiplication between the query and key tensors.\n",
    "  3. The attention scores are scaled by dividing them by the square root of the head dimension (`self.head_dim`).\n",
    "  4. If a mask is provided, the attention scores are masked by setting the scores corresponding to masked positions to negative infinity.\n",
    "  5. The attention scores are passed through a softmax activation function along the last dimension (`dim=-1`).\n",
    "  6. The attention probabilities are used to weight the value tensor.\n",
    "  7. The resulting attention output is transposed and reshaped to match the original shape.\n",
    "  8. Finally, the attention output is passed through the `self.output_linear` linear layer, which applies another linear transformation to the output representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "maV8g_uZsnQm"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention module.\n",
    "\n",
    "    Args:\n",
    "        config: Configuration for the multi-head attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        logger.debug(f\"hidden_dim: {self.embed_dim}\")\n",
    "        logger.debug(f\"num_heads: {self.num_heads}\")\n",
    "\n",
    "        assert self.embed_dim % self.num_heads == 0\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        logger.debug(f\"head_dim: {self.head_dim}\")\n",
    "\n",
    "        self.q = nn.Linear(self.embed_dim, self.head_dim * self.num_heads)\n",
    "        self.k = nn.Linear(self.embed_dim, self.head_dim * self.num_heads)\n",
    "        self.v = nn.Linear(self.embed_dim, self.head_dim * self.num_heads)\n",
    "        self.output_linear = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            query: torch.Tensor,\n",
    "            key: torch.Tensor,\n",
    "            value: torch.Tensor,\n",
    "            mask: Optional[torch.Tensor] = None\n",
    "        ) -> torch.Tensor:\n",
    "            \"\"\"\n",
    "            Perform a forward pass of the multi-head attention.\n",
    "\n",
    "            Args:\n",
    "                query: Query tensor of shape [batch_size, seq_len, embed_dim].\n",
    "                key: Key tensor of shape [batch_size, seq_len, embed_dim].\n",
    "                value: Value tensor of shape [batch_size, seq_len, embed_dim].\n",
    "                mask: Optional mask tensor. Default is None.\n",
    "\n",
    "            Returns:\n",
    "                Tensor of shape [batch_size, seq_len, embed_dim],\n",
    "                representing the output of the multi-head attention.\n",
    "            \"\"\"\n",
    "            # TODO:\n",
    "            # Apply linear transformations to the query, key, and value tensors\n",
    "            q = self.q(query)\n",
    "            k = self.k(key)\n",
    "            v = self.v(value)\n",
    "            logger.debug(f\"q_size: {q.size()}\")\n",
    "            logger.debug(f\"k_size: {k.size()}\")\n",
    "            logger.debug(f\"v_size: {v.size()}\")\n",
    "\n",
    "            # Reshape and transpose tensors for matrix multiplication\n",
    "            q = q.view(q.size(0), -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "            k = k.view(k.size(0), -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "            v = v.view(v.size(0), -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "            logger.debug(f\"qT_size: {q.size()}\")\n",
    "            logger.debug(f\"kT_size: {k.size()}\")\n",
    "            logger.debug(f\"vT_size: {v.size()}\")\n",
    "            # TODO:\n",
    "            # Calculate the attention scores using the\n",
    "            # scaled_dot_product_attention function defined earlier\n",
    "\n",
    "            attn_scores = scaled_dot_product_attention(q, k, v, mask, self.dropout)\n",
    "            attn_scores = attn_scores.transpose(1, 2).contiguous()\n",
    "            attn_scores = attn_scores.view(attn_scores.size(0), -1, self.embed_dim)\n",
    "            logger.debug(f\"attn_scores: {attn_scores.size()}\")\n",
    "\n",
    "            output = self.output_linear(attn_scores)\n",
    "            logger.debug(f\"output_size: {output.size()}\")\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 506,
     "referenced_widgets": [
      "1d628ec541414fd19a0574bdd8584267",
      "ba76572086a34f17b8bc74c24555ac25",
      "5646d8471ebf416480c51131617bc0b3",
      "5649283c39ad4c6fa9aff7bb111c6932",
      "d1b20c8d0f534d019ace9a5f658e7d44",
      "7f33513aa81f4a6a9d20fb1966eb7048",
      "250e6d5241fa47f6973d5336dea474d4",
      "830d3cd2251d40a5b353dc99abe3991c",
      "b12c0054e86f4d7db1375121871e9b11",
      "8084f3d45e8f47c782fa15f4a8b1a7aa",
      "498fb504dc0445f7bbd7ad2e4fe98345",
      "3739411d8ea3498a91c76a04987e4639",
      "934bcd7efc5a4172a9e01057b7b4dd19",
      "c4fd066fd96948ac8809bebb64eee3a5",
      "91d7d6b2df114294b22c831fb021e5e9",
      "449a9b149cf243a285dcc34d545a730b",
      "8b25ad83427b4026a6efb96c2e1dc30a",
      "7ac10266e2b644feb90eeffb19720b1a",
      "4d5ce915e12f4b9e8c9b5fb0c4a0d416",
      "f1071d22a10b410a95ed2fd97248b402",
      "0b6c917112794e70aa34ab7a55f2c53b",
      "828ede9769a4449bb6c1453dbf8cd4de",
      "8f220b6d94544d0696df990072a900cc",
      "e48fc25fe9004b8c85c19a5e7749342f",
      "445f7b14de9b488298f287195b06f163",
      "9ff9e5e617db4e3b91b6156e9cbffc7f",
      "23d576bf422044ba9a374725338135b3",
      "5c85d8ce83254d8bbae7e2cb76d4d68a",
      "f45509660995415aad510c3f476cedf8",
      "a5c8e7f33223416d882bd819241cc7dd",
      "e3ea71e0919d430faaefb6141c1dc6d5",
      "c0eb1f6e93b54a8bbe3e43e24d0147dc",
      "ce5d34df96104637899aec37c47e9d42",
      "1a799bd44e91400e85f619d5c3b8ef8a",
      "324b5437ce194083a8b1a4448adb5c00",
      "60712d98c2be4e68b4e74a61e611f6df",
      "1a4223e2bd404df59efc00768d684f07",
      "20406be4301c4e3c845215c7c5f5d2d6",
      "4f458200fb22483897f02c570eed5d8c",
      "00ed9fa791b64ca1bc96715d457d6bbf",
      "ec8a649e12a2414b87591957a42826db",
      "572b1e6014b947d89fb5adbacf149ae2",
      "34e317d5247d48179dc28312bd039e28",
      "4c92ba9abc45458eb478f8fcd6602d51"
     ]
    },
    "executionInfo": {
     "elapsed": 3077,
     "status": "ok",
     "timestamp": 1739172772152,
     "user": {
      "displayName": "Eunkyu Park",
      "userId": "11893515073306472432"
     },
     "user_tz": -540
    },
    "id": "qwuGuVlq2Ah1",
    "outputId": "35c3d272-e80e-44d3-a1bd-fa4f905322c2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-11 13:45:05.514\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m12\u001b[0m - \u001b[34m\u001b[1mhidden_dim: 768\u001b[0m\n",
      "\u001b[32m2025-02-11 13:45:05.516\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m13\u001b[0m - \u001b[34m\u001b[1mnum_heads: 12\u001b[0m\n",
      "\u001b[32m2025-02-11 13:45:05.516\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m17\u001b[0m - \u001b[34m\u001b[1mhead_dim: 64\u001b[0m\n",
      "\u001b[32m2025-02-11 13:45:05.535\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m50\u001b[0m - \u001b[34m\u001b[1mq_size: torch.Size([1, 9, 768])\u001b[0m\n",
      "\u001b[32m2025-02-11 13:45:05.536\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m51\u001b[0m - \u001b[34m\u001b[1mk_size: torch.Size([1, 9, 768])\u001b[0m\n",
      "\u001b[32m2025-02-11 13:45:05.536\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m52\u001b[0m - \u001b[34m\u001b[1mv_size: torch.Size([1, 9, 768])\u001b[0m\n",
      "\u001b[32m2025-02-11 13:45:05.537\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m59\u001b[0m - \u001b[34m\u001b[1mqT_size: torch.Size([1, 12, 9, 64])\u001b[0m\n",
      "\u001b[32m2025-02-11 13:45:05.538\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m60\u001b[0m - \u001b[34m\u001b[1mkT_size: torch.Size([1, 12, 9, 64])\u001b[0m\n",
      "\u001b[32m2025-02-11 13:45:05.539\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m61\u001b[0m - \u001b[34m\u001b[1mvT_size: torch.Size([1, 12, 9, 64])\u001b[0m\n",
      "\u001b[32m2025-02-11 13:45:05.539\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscaled_dot_product_attention\u001b[0m:\u001b[36m23\u001b[0m - \u001b[34m\u001b[1mquery_size: torch.Size([1, 12, 9, 64])\u001b[0m\n",
      "\u001b[32m2025-02-11 13:45:05.540\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscaled_dot_product_attention\u001b[0m:\u001b[36m24\u001b[0m - \u001b[34m\u001b[1mkey: torch.Size([1, 12, 64, 9])\u001b[0m\n",
      "\u001b[32m2025-02-11 13:45:05.543\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m69\u001b[0m - \u001b[34m\u001b[1mattn_scores: torch.Size([1, 9, 768])\u001b[0m\n",
      "\u001b[32m2025-02-11 13:45:05.545\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m72\u001b[0m - \u001b[34m\u001b[1moutput_size: torch.Size([1, 9, 768])\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model_ckpt = \"bert-base-uncased\"\n",
    "config = AutoConfig.from_pretrained(model_ckpt)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "token_emb = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "inputs_embeds = token_emb(inputs.input_ids)\n",
    "\n",
    "query = key = value = inputs_embeds\n",
    "\n",
    "multihead_attn = MultiHeadAttention(config)\n",
    "attn_output = multihead_attn(query, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMR5MVuKyf5D"
   },
   "source": [
    "### 2. The Feed-Forward Layer\n",
    "- The feed-forward layer is a type of neural network layer that processes the input data independently at each position in the input sequence, without considering the dependencies between different positions. This means that the computations for different positions can be parallelized, making the Transformer architecture highly efficient for sequence processing tasks.\n",
    "\n",
    "- The feed-forward layer in Transformers typically consists of two linear transformations with a non-linear activation function in between. The input to the feed-forward layer is a tensor representing the hidden states of the previous layer or the input embeddings.\n",
    "\n",
    "- The input to the feed-forward layer is a tensor representing the hidden states of the previous layer or the input embeddings. The feed-forward layer is a critical component of Transformers as it helps capture local patterns and dependencies in the input data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Mb5eXEE5ybmV"
   },
   "outputs": [],
   "source": [
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Feed-forward layer module.\n",
    "\n",
    "    Args:\n",
    "        config: Configuration for the feed-forward layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a forward pass of the feed-forward layer.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape [batch_size, seq_len, hidden_dim].\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, seq_len, hidden_dim],\n",
    "            representing the output of the feed-forward layer.\n",
    "        \"\"\"\n",
    "        x = self.linear_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.dropout(x)\n",
    "        logger.debug(f\"ff_output_size: {x.size()}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 114,
     "status": "ok",
     "timestamp": 1739172772268,
     "user": {
      "displayName": "Eunkyu Park",
      "userId": "11893515073306472432"
     },
     "user_tz": -540
    },
    "id": "uRjTrmh_19PO",
    "outputId": "43fb6ded-fe6c-42bf-8a80-f74315194a7f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-11 13:45:05.638\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m30\u001b[0m - \u001b[34m\u001b[1mff_output_size: torch.Size([1, 9, 768])\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "feed_forward = FeedForward(config)\n",
    "ff_outputs = feed_forward(attn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S20D0CzBzP4R"
   },
   "source": [
    "### 3. Positional Embeddings\n",
    "- The purpose of positional embeddings is to provide the model with a representation that encodes the relative positions of tokens within the sequence. This allows the model to differentiate between tokens based on their position, even though all tokens initially have the same embeddings.\n",
    "\n",
    "- In the original Transformer model, the positional embeddings used to encode the sequential order of tokens are learned as part of the model training process. The positional embeddings are initialized with fixed sinusoidal functions of different frequencies and then fine-tuned during training.\n",
    "\n",
    "- Steps:\n",
    "  1. The constructor of the Embedding class defines two embedding layers `self.token_embeddings` and `self.position_embeddings`. These layers are initialized with different vocabulary sizes and hidden sizes.\n",
    "  2. In the forward method, position IDs are created using `torch.arange(seq_length).unsqueeze(0)`.  This creates a tensor of sequential integers from 0 to seq_lenght-1 and unsqueezes it to have a shape of [1, seq_lenght]. These position IDs represent the positions of the tokens in the input sequence.\n",
    "  3. The token embeddings for the input sequence are obtained by passing `input_ids` to `self.token_embeddings`. This maps each token ID to its corresponding embedding vector. On the other hand, the position embeddings for the input sequence are obtained by passing `position_ids` to `self.position_embeddings`. This maps each position ID to its corresponding embedding vector.\n",
    "  4. The token embeddings and position embeddings are added element-wise (`token_embeddings` + `position_embeddings`) to create the combined embeddings. This operation incorporates both the token information and the positional information of each token in the input sequence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "U0xHcN1OzgIB"
   },
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Embeddings layer module.\n",
    "    Combines a token embedding layer that projects the `input_ids` to a dense hidden state\n",
    "    with the positional embedding that does the same for `position_ids`.\n",
    "    The resulting embedding is simply the sum of both embeddings.\n",
    "\n",
    "    Args:\n",
    "        config: Configuration for the embeddings layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a forward pass of the embeddings layer.\n",
    "\n",
    "        Args:\n",
    "            input_ids: Input tensor of shape [batch_size, seq_len].\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, seq_len, hidden_dim],\n",
    "            representing the embeddings of the input.\n",
    "\n",
    "        Notes:\n",
    "            1. Create position IDs for input sequence.\n",
    "            2. Create token and position embeddings.\n",
    "            3. Combine token and position embeddings.\n",
    "        \"\"\"\n",
    "        logger.debug(f\"input_size: {input_ids.size()}\")\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=device).unsqueeze(0)\n",
    "        token_embeddings = self.token_embeddings(input_ids)\n",
    "        logger.debug(f\"token_embd_size: {token_embeddings.size()}\")\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        logger.debug(f\"position_embd_size: {token_embeddings.size()}\")\n",
    "\n",
    "        embeddings = token_embeddings + position_embeddings\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        logger.debug(f\"embd_size: {token_embeddings.size()}\")\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWD4WqXry7m_"
   },
   "source": [
    "## The Encoder\n",
    "### 1. TransformerEncoderBlock\n",
    "- With the previously defined components, we can now define the TransformerEncoderBlock class. It is responsible for performing one layer of the encoder in a Transformer model.\n",
    "- Steps:\n",
    "  1. Layer Normalization: The input tensor `x` is first passed through a layer normalization operation using `self.layer_norm_1`. This operation normalizes the activations across the hidden dimension of x to have zero mean and unit variance. The result is stored in hidden_state.\n",
    "  2. Attention with Skip Connection: The attention mechanism is applied to `hidden_state` using `self.attention`. This attention operation takes hidden_state as the input and produces an attention-based output. The output is then element-wise added (`+`) to the original input tensor `x`. This skip connection allows the model to directly incorporate the original input along with the attention-based output.\n",
    "  3. Feed-Forward Layer with Skip Connection: The output of the previous step is passed through another layer normalization operation `self.layer_norm_2` to normalize the activations. Then, the result is passed through the feed-forward layer self.feed_forward. The output of the feed-forward layer is again element-wise added (`+`) to the input tensor from the previous step (`x`). This skip connection allows the model to combine the information from the original input with the transformed output from the feed-forward layer.\n",
    "\n",
    "- In summary, the skip connections enable the model to incorporate the original input tensor x into the output of each layer. By adding the transformed outputs to the original input, the model can retain important information from the input and facilitate the flow of gradients during training. The skip connections help in addressing the vanishing gradient problem and make it easier to train deep Transformer architectures by ensuring the model has access to the original input information at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1wEQzHPLyTPk"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Encoder block module.\n",
    "\n",
    "    Args:\n",
    "        config: Configuration for the encoder block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.feed_forward = FeedForward(config)\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor]=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a forward pass of the transformer encoder block.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape [batch_size, seq_len, hidden_dim].\n",
    "            mask: Optional mask tensor. Default is None.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, seq_len, hidden_dim],\n",
    "            representing the output of the encoder block.\n",
    "        \"\"\"\n",
    "        logger.debug(f\"encoder_block_input_size: {x.size()}\")\n",
    "        # TODO:\n",
    "        # Normalize input tensor x\n",
    "        hidden_state = self.layer_norm_1(x)\n",
    "        # TODO:\n",
    "        # Apply the attention mechanism to the hidden_state using self.attention\n",
    "        # Add the output to the original input tensor (skip connection)\n",
    "\n",
    "        attention_output = self.attention(hidden_state, hidden_state, hidden_state, mask)\n",
    "        x = x + self.dropout(attention_output)\n",
    "        # TODO:\n",
    "        # Normalize the activations using self.layer_norm_2\n",
    "        # Pass it to the feed-forward layer\n",
    "        # Add the output of the feed_forward layer to the input tensor from\n",
    "        # the previous step (skip connection)\n",
    "        x = self.layer_norm_2(x)\n",
    "        x = x + self.feed_forward(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        logger.debug(f\"encoder_block_output_size: {x.size()}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 122,
     "status": "ok",
     "timestamp": 1739172772474,
     "user": {
      "displayName": "Eunkyu Park",
      "userId": "11893515073306472432"
     },
     "user_tz": -540
    },
    "id": "Tm3nJF_g1lOA",
    "outputId": "37659e22-7d9e-4ab8-8169-48b987aaddf5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-11 13:45:05.728\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m12\u001b[0m - \u001b[34m\u001b[1mhidden_dim: 768\u001b[0m\n",
      "\u001b[32m2025-02-11 13:45:05.735\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m13\u001b[0m - \u001b[34m\u001b[1mnum_heads: 12\u001b[0m\n",
      "\u001b[32m2025-02-11 13:45:05.739\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m17\u001b[0m - \u001b[34m\u001b[1mhead_dim: 64\u001b[0m\n",
      "\u001b[32m2025-02-11 13:45:05.814\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m29\u001b[0m - \u001b[34m\u001b[1mencoder_block_input_size: torch.Size([1, 9, 768])\u001b[0m\n",
      "\u001b[32m2025-02-11 13:45:05.817\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m50\u001b[0m - \u001b[34m\u001b[1mq_size: torch.Size([1, 9, 768])\u001b[0m\n",
      "\u001b[32m2025-02-11 13:45:05.818\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m51\u001b[0m - \u001b[34m\u001b[1mk_size: torch.Size([1, 9, 768])\u001b[0m\n",
      "\u001b[32m2025-02-11 13:45:05.819\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m52\u001b[0m - \u001b[34m\u001b[1mv_size: torch.Size([1, 9, 768])\u001b[0m\n",
      "\u001b[32m2025-02-11 13:45:05.820\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m59\u001b[0m - \u001b[34m\u001b[1mqT_size: torch.Size([1, 12, 9, 64])\u001b[0m\n",
      "\u001b[32m2025-02-11 13:45:05.820\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m60\u001b[0m - \u001b[34m\u001b[1mkT_size: torch.Size([1, 12, 9, 64])\u001b[0m\n",
      "\u001b[32m2025-02-11 13:45:05.821\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m61\u001b[0m - \u001b[34m\u001b[1mvT_size: torch.Size([1, 12, 9, 64])\u001b[0m\n",
      "\u001b[32m2025-02-11 13:45:05.821\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscaled_dot_product_attention\u001b[0m:\u001b[36m23\u001b[0m - \u001b[34m\u001b[1mquery_size: torch.Size([1, 12, 9, 64])\u001b[0m\n",
      "\u001b[32m2025-02-11 13:45:05.822\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscaled_dot_product_attention\u001b[0m:\u001b[36m24\u001b[0m - \u001b[34m\u001b[1mkey: torch.Size([1, 12, 64, 9])\u001b[0m\n",
      "\u001b[32m2025-02-11 13:45:05.823\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m69\u001b[0m - \u001b[34m\u001b[1mattn_scores: torch.Size([1, 9, 768])\u001b[0m\n",
      "\u001b[32m2025-02-11 13:45:05.824\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m72\u001b[0m - \u001b[34m\u001b[1moutput_size: torch.Size([1, 9, 768])\u001b[0m\n",
      "\u001b[32m2025-02-11 13:45:05.828\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m30\u001b[0m - \u001b[34m\u001b[1mff_output_size: torch.Size([1, 9, 768])\u001b[0m\n",
      "\u001b[32m2025-02-11 13:45:05.831\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m48\u001b[0m - \u001b[34m\u001b[1mencoder_block_output_size: torch.Size([1, 9, 768])\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "encoder_layer = TransformerEncoderBlock(config)\n",
    "_ = encoder_layer(inputs_embeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Kfege8l2yzb"
   },
   "source": [
    "### 2. TransformerEncoder\n",
    "- Finally, putting everything together, we can now define the TransoformerEncoder class. It is responsible for processing the input sequence using multiple stacked Transformer Encoder Blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "RqThqFrx2wDN"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Encoder module.\n",
    "\n",
    "    Args:\n",
    "        config: Configuration for the encoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        self.embeddings = Embeddings(config)\n",
    "        self.layers = nn.ModuleList([TransformerEncoderBlock(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a forward pass of the transformer encoder.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape [batch_size, seq_len].\n",
    "            mask: Optional mask tensor. Default is None.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, seq_len, hidden_dim],\n",
    "            representing the output of the encoder.\n",
    "        \"\"\"\n",
    "        x = self.embeddings(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4293,
     "status": "ok",
     "timestamp": 1739172785718,
     "user": {
      "displayName": "Eunkyu Park",
      "userId": "11893515073306472432"
     },
     "user_tz": -540
    },
    "id": "xOoxpD5G15uV",
    "outputId": "71d3a440-703d-4c1b-c6e9-520f46fcab6f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"INFO\")\n",
    "encoder = TransformerEncoder(config)\n",
    "encoder_output = encoder(inputs.input_ids)\n",
    "encoder_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1739172785768,
     "user": {
      "displayName": "Eunkyu Park",
      "userId": "11893515073306472432"
     },
     "user_tz": -540
    },
    "id": "lZZOtkEL2yXv",
    "outputId": "5a14fc8d-2c78-4d5f-c50f-9ec7d425088b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1345,  0.0775,  0.5740,  ...,  0.1227, -0.1416, -0.4897],\n",
       "         [-0.6721,  4.9926,  1.5509,  ...,  0.7134,  1.1228,  0.0332],\n",
       "         [ 0.2856, -1.5069,  0.1794,  ...,  2.3478,  0.0000,  0.1848],\n",
       "         ...,\n",
       "         [-0.2085, -0.3087,  0.5413,  ..., -0.4112,  0.0000,  0.8980],\n",
       "         [-0.2039,  0.0180,  0.2897,  ...,  0.2765,  1.0491, -0.1463],\n",
       "         [-0.1788,  4.2346,  1.7248,  ..., -0.5369,  0.5729, -1.4365]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-MsG8s53FTZ"
   },
   "source": [
    "## The Decoder\n",
    "-  The main difference between the decoder and encoder is that the decoder has two attention sublayers.\n",
    "\n",
    "- The first attention sublayer, known as the self-attention sublayer, allows the decoder to attend to its own previously generated tokens, capturing dependencies and relationships within the output sequence.\n",
    "\n",
    "- The second attention sublayer is the encoder-decoder attention, which allows the decoder to attend to the encoded representations produced by the encoder, incorporating contextual information from the input sequence.\n",
    "\n",
    "- **Mask** is applied in the self-attention mechanism to enforce the causality constraint during the decoding process. Since the decoder generates the target sequence autoregressively, each position in the target sequence should only attend to previous positions and not future positions. If you recall the scaled_dot_product_attention function, we set the upper values to infinity. This guarantees that the attention weights are all zero once we take the softmax over the scores (as e^-∞=0).\n",
    "\n",
    "### 1. The Decoder Block\n",
    "- Similarly to the TransformerEncoderBlock, the TransformerDecoderBlock is responsible for performing one layer of the decoder in a Transformer model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "tPPEMp5K29Ok"
   },
   "outputs": [],
   "source": [
    "class TransformerDecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Decoder layer module.\n",
    "\n",
    "    Args:\n",
    "        config: Configuration for the decoder layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, ) -> None:\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.layer_norm_3 = nn.LayerNorm(config.hidden_size)\n",
    "        self.self_attention = MultiHeadAttention(config)\n",
    "        self.encoder_decoder_attention = MultiHeadAttention(config)\n",
    "        self.feed_forward = FeedForward(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        encoder_output: torch.Tensor,\n",
    "        source_mask: Optional[torch.Tensor] = None,\n",
    "        target_mask: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a forward pass of the transformer decoder block.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape [batch_size, seq_len, hidden_dim].\n",
    "            encoder_output: Output tensor from the encoder of shape [batch_size, seq_len, hidden_dim].\n",
    "            source_mask: Optional source mask tensor. Default is None.\n",
    "            target_mask: mask: Optional target mask tensor. Default is None.\n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, seq_len, hidden_dim],\n",
    "            representing the output of the decoder block.\n",
    "        \"\"\"\n",
    "        logger.debug(f\"decoder_block_input_size: {x.size()}\")\n",
    "\n",
    "        # TODO:\n",
    "        # First attention sublayer, attending to its own previously generated\n",
    "        # tokens. mask 필요: 디코더가 미래 단어를 참조하지 못하도록 차단\n",
    "        hidden_state = self.layer_norm_1(x)\n",
    "        attn_1_out = self.self_attention(hidden_state, hidden_state, hidden_state, target_mask)\n",
    "        x = x + self.dropout(attn_1_out)\n",
    "        # TODO:\n",
    "        # Second attention sublayer, attending to the encoded representations\n",
    "        # from the encoder.\n",
    "        # mask 필요 없음: 인코더 출력을 전체적으로 봐도 됨. 인코더-디코더 간의 Attention에서는 미래 정보 제한이 필요 없음\n",
    "        x = self.layer_norm_2(x)\n",
    "        attn_2_out = self.encoder_decoder_attention(x, encoder_output, encoder_output, source_mask)\n",
    "        x = x + self.dropout(attn_2_out)\n",
    "        x = self.layer_norm_3(x)\n",
    "\n",
    "        feed_forward_output = self.feed_forward(x)\n",
    "        x = x + self.dropout(feed_forward_output)\n",
    "        logger.debug(f\"decoder_block_output_size: {x.size()} \")\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUImseSe5XiJ"
   },
   "source": [
    "### 2. TransformerDecoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "-tAjtMyE30bi"
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, config) -> None:\n",
    "        \"\"\"\n",
    "        Transformer Decoder module.\n",
    "\n",
    "        Args:\n",
    "            config: Configuration object for the decoder.\n",
    "            mask: Masking object for attention layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embeddings = Embeddings(config)\n",
    "        self.layers = nn.ModuleList([TransformerDecoderBlock(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        encoder_output: torch.Tensor,\n",
    "        source_mask: torch.Tensor = None,\n",
    "        target_mask: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a forward pass of the transformer decoder.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape [batch_size, tgt_len].\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, tgt_len, vocab_size],\n",
    "            representing the predicted probabilities over the vocabulary.\n",
    "        \"\"\"\n",
    "        x = self.embeddings(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, source_mask=source_mask, target_mask=target_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DBrgosWp5fyt",
    "outputId": "0b6f7fb7-6109-441a-9c19-5430ccc569d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"INFO\")\n",
    "seq_len = inputs.input_ids.size(-1)\n",
    "mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0)\n",
    "encoder = TransformerEncoder(config)\n",
    "encoder_output = encoder(inputs.input_ids)\n",
    "decoder = TransformerDecoder(config)\n",
    "output = decoder(inputs.input_ids, encoder_output, target_mask=mask)\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_M3EgAk5t6s"
   },
   "source": [
    "## The Transformer\n",
    "- With all the required components now in place, we can proceed to define and implement this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "pUMsP5BT5gOT"
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder-Decoder model that combines the TransformerEncoder and TransformerDecoder.\n",
    "\n",
    "    Args:\n",
    "        encoder_config: Configuration for the encoder.\n",
    "        decoder_config: Configuration for the decoder.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(config)\n",
    "        self.decoder = TransformerDecoder(config)\n",
    "        self.fc = nn.Linear(config.hidden_size, config.vocab_size)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        target_ids: torch.Tensor,\n",
    "        source_mask: Optional[torch.Tensor] = None,\n",
    "        target_mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a forward pass of the encoder-decoder model.\n",
    "\n",
    "        Args:\n",
    "            input_ids: Input tensor of shape [batch_size, src_len].\n",
    "            target_ids: Target tensor of shape [batch_size, tgt_len].\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, tgt_len, vocab_size],\n",
    "            representing the predicted probabilities over the vocabulary.\n",
    "        \"\"\"\n",
    "        encoder_output = self.encoder(input_ids)\n",
    "        decoder_output = self.decoder(\n",
    "            target_ids,\n",
    "            encoder_output,\n",
    "            source_mask=source_mask,\n",
    "            target_mask=target_mask\n",
    "        )\n",
    "        x = self.fc(decoder_output)  # Apply linear layer to transform to vocab_size\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31CEsEwv52Bv"
   },
   "source": [
    "### Masking\n",
    "The mask used in the Transformer model should have a specific shape and values to ensure proper masking during the attention mechanism. Here's how you can define the mask:\n",
    "\n",
    "- Padding Mask: The padding mask is used to mask out padding tokens in the input sequences. It should have a shape of (batch_size, seq_length) and contain 1 where the padding tokens are present and 0 for the non-padding tokens. This mask ensures that the padding tokens do not contribute to the attention scores.\n",
    "\n",
    "- Future Mask: The future mask is used to prevent attending to future positions in the self-attention mechanism. It should have a shape of (seq_length, seq_length) and have 1 for positions that can be attended and 0 for positions that should be masked or ignored.\n",
    "\n",
    "- Combined Mask: To create the final mask, you need to combine the padding mask and the future mask. This can be done by applying logical operations, such as element-wise multiplication or logical OR, to the two masks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2ekXwjamXbk"
   },
   "source": [
    "\n",
    "\n",
    "<p align=\"center\"><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gIU1WTNJle6N0tw6P-C4OA.png\" width = \"400\" ></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "JD82AcP551Ku"
   },
   "outputs": [],
   "source": [
    "def create_mask(batch_size: int, seq_length: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create a lower triangular mask with ones below the diagonal.\n",
    "\n",
    "    Args:\n",
    "        batch_size: The batch size.\n",
    "        seq_length: The length of the sequence.\n",
    "\n",
    "    Returns:\n",
    "        The mask tensor with shape (batch_size, seq_length, seq_length).\n",
    "    \"\"\"\n",
    "    mask = torch.tril(torch.ones(seq_length, seq_length))\n",
    "    mask = mask.unsqueeze(0).expand(batch_size, seq_length, seq_length)  # Expand the mask along the batch dimension\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ZkrBt8vveyP"
   },
   "source": [
    "### Recap\n",
    "\n",
    "<p align=\"center\"><img src=\"https://cpm0722.github.io/assets/images/2021-01-28-Transformer-in-pytorch/qkv_vector.png\" width = \"600\" ></p>\n",
    "\n",
    "\n",
    "<p align=\"center\"><img src=\"https://cpm0722.github.io/assets/images/2021-01-28-Transformer-in-pytorch/attention_score_scalar.png\" width = \"600\" ></p>\n",
    "\n",
    "\n",
    "<p align=\"center\"><img src=\"https://cpm0722.github.io/assets/images/2021-01-28-Transformer-in-pytorch/qkv_matrix_1.png\" width = \"600\" ></p>\n",
    "\n",
    "<p align=\"center\"><img src=\"https://cpm0722.github.io/assets/images/2021-01-28-Transformer-in-pytorch/attention_score_vector.png\" width = \"600\" ></p>\n",
    "\n",
    "\n",
    "<p align=\"center\"><img src=\"https://cpm0722.github.io/assets/images/2021-01-28-Transformer-in-pytorch/attention_vector.png\" width = \"600\" ></p>\n",
    "\n",
    "<p align=\"center\"><img src=\"https://cpm0722.github.io/assets/images/2021-01-28-Transformer-in-pytorch/qkv_matrix_2.png\" width = \"600\" ></p>\n",
    "\n",
    "\n",
    "<p align=\"center\"><img src=\"https://cpm0722.github.io/assets/images/2021-01-28-Transformer-in-pytorch/attention_matrix.png\" width = \"600\" ></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrcH-_o4wAw2"
   },
   "source": [
    "output의 shape는 모두 동일할지라도, Q, K, V 의 실제 값들은 모두 다르다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkTDEl15wI9G"
   },
   "source": [
    "<p align=\"center\"><img src=\"https://cpm0722.github.io/assets/images/2021-01-28-Transformer-in-pytorch/qkv_fc_layer.png\" width = \"600\" ></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avG5Hi6CwN_d"
   },
   "source": [
    "Multi-head attention\n",
    "\n",
    "-  Scaled Dot-Product Attention에서는 Q, K , V\n",
    "를 위해 FC layer가 총 3개 필요했었는데, 이를 h회 수행한다고 했으므로 3*h 개의 FC layer가 필요함\n",
    "\n",
    "<p align=\"center\"><img src=\"https://cpm0722.github.io/assets/images/2021-01-28-Transformer-in-pytorch/multi_head_attention_concat.png\" width = \"600\" ></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YKYJHAfkwGyI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Yc5nw316ZhM"
   },
   "source": [
    "## Testing the Transformer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGhIpd41m5pn"
   },
   "source": [
    "<p align=\"center\"><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l46NMJUjXc7apz4xlsS61A.png\" width = \"600\" ></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "LnY4YNdo6uX7"
   },
   "outputs": [],
   "source": [
    "class TransformerConfig():\n",
    "    \"\"\"\n",
    "    Configuration class for the Transformer model.\n",
    "\n",
    "    Args:\n",
    "        hidden_size: Size of the hidden state.\n",
    "        intermediate_size: Size of the intermediate layer in the feed-forward network.\n",
    "        num_hidden_layers: Number of hidden layers in the Transformer.\n",
    "        vocab_size: Size of the vocabulary.\n",
    "        max_position_embeddings: Maximum number of positional embeddings.\n",
    "        hidden_dropout_prob: Dropout probability for the hidden layers.\n",
    "        num_attention_heads: Number of attention heads in the multi-head attention.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        intermediate_size: int,\n",
    "        num_hidden_layers: int,\n",
    "        vocab_size: int,\n",
    "        max_position_embeddings: int,\n",
    "        hidden_dropout_prob: float,\n",
    "        num_attention_heads: int\n",
    "    ):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.num_attention_heads = num_attention_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "5TpLAIm57AKl"
   },
   "outputs": [],
   "source": [
    "# Set up hyperparameters and configuration\n",
    "config = TransformerConfig(\n",
    "    hidden_size=512,\n",
    "    intermediate_size=2048,\n",
    "    num_hidden_layers=6,\n",
    "    vocab_size=100,\n",
    "    max_position_embeddings=512,\n",
    "    hidden_dropout_prob=0.1,\n",
    "    num_attention_heads=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1739173534528,
     "user": {
      "displayName": "Eunkyu Park",
      "userId": "11893515073306472432"
     },
     "user_tz": -540
    },
    "id": "kjcVfnNm64Et",
    "outputId": "82955978-5682-4f7d-8522-a465f0c4d334"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 10]), torch.Size([128, 12]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define some fake data\n",
    "batch_size = 128  # 배치 크기 설정\n",
    "source_length = 10  # 입력 시퀀스 길이 설정\n",
    "target_length = 12  # 출력 시퀀스 길이 설정\n",
    "\n",
    "source_ids =  torch.randint(0, config.vocab_size, (batch_size, source_length)) # TODO: (16, 10) 크기의 랜덤한 입력 데이터 생성\n",
    "target_ids =  torch.randint(0, config.vocab_size, (batch_size, target_length)) # TODO: (16, 12) 크기의 랜덤한 출력 데이터 생성\n",
    "\n",
    "source_ids.size(), target_ids.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1739173709038,
     "user": {
      "displayName": "Eunkyu Park",
      "userId": "11893515073306472432"
     },
     "user_tz": -540
    },
    "id": "VnUlJ6N08edU",
    "outputId": "22be2fd3-38d0-4ea0-cc2f-9b03db8f64d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 10, 10]), torch.Size([128, 12, 12]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_mask = create_mask(batch_size, source_length)# TODO: implement mask creation function\n",
    "target_mask = create_mask(batch_size, target_length)# TODO: implement mask creation function\n",
    "source_mask.size(), target_mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 701,
     "status": "ok",
     "timestamp": 1739173743130,
     "user": {
      "displayName": "Eunkyu Park",
      "userId": "11893515073306472432"
     },
     "user_tz": -540
    },
    "id": "wAhasmZd8i07",
    "outputId": "60b5f40f-3faf-4915-fa97-83fe6b338841"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_output_size: torch.Size([128, 10, 512])\n",
      "decoder_output_size: torch.Size([128, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "encoder = TransformerEncoder(config)\n",
    "encoder_output = encoder(source_ids) # 인코더에 입력 데이터 전달\n",
    "print(f\"encoder_output_size: {encoder_output.size()}\") # (16, 10, hidden_size) 출력\n",
    "decoder = TransformerDecoder(config)\n",
    "output = decoder(source_ids, encoder_output, source_mask=source_mask)\n",
    "print(f\"decoder_output_size: {output.size()}\")  # (16, 10, hidden_size) 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 584,
     "status": "ok",
     "timestamp": 1739173792793,
     "user": {
      "displayName": "Eunkyu Park",
      "userId": "11893515073306472432"
     },
     "user_tz": -540
    },
    "id": "6aWfXu5B8jMn",
    "outputId": "34a44202-7122-44f0-906f-1cb91010de64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape: torch.Size([128, 12, 100])\n",
      "Source Input Shape: torch.Size([128, 10])\n"
     ]
    }
   ],
   "source": [
    "# Define the EncoderDecoder model\n",
    "encoder_decoder = EncoderDecoder(config)\n",
    "output = encoder_decoder(source_ids, target_ids, target_mask=target_mask)\n",
    "print(\"Output Shape:\", output.shape)\n",
    "print(\"Source Input Shape:\", source_ids.shape)  # (16, 10) 출력\n",
    "# encoder_output = encoder(source_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1739173801821,
     "user": {
      "displayName": "Eunkyu Park",
      "userId": "11893515073306472432"
     },
     "user_tz": -540
    },
    "id": "gCeLQFe_8kuS",
    "outputId": "0c8254d0-ef43-4016-d5fa-7b0a57fc6908"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 12])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_ids.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNVyswOS8r7W"
   },
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "VQcHWjA08lnt"
   },
   "outputs": [],
   "source": [
    "class RandomDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Provides random data copy dataset for training.\n",
    "\n",
    "    Args:\n",
    "        vocabulary_size: The vocabulary size.\n",
    "        batch_size: The batch size.\n",
    "        num_samples: The number of samples.\n",
    "        sample_length: The length of each sample.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocabulary_size: int, batch_size: int, num_samples: int, sample_length: int):\n",
    "        self.samples = list()\n",
    "\n",
    "        for i in range(batch_size * num_samples):\n",
    "            data = torch.randint(1, vocabulary_size, (sample_length + 1,))  # +1 to avoid length mismatch\n",
    "            data[0] = 1\n",
    "            source = torch.autograd.Variable(data[:-1], requires_grad=False)\n",
    "            target = torch.autograd.Variable(data[1:], requires_grad=False)\n",
    "\n",
    "            sample = {\n",
    "                'source': source,\n",
    "                'target': target,\n",
    "                'target_y': target,  # Ensure it matches target length\n",
    "                'source_mask': (source != 0).unsqueeze(-2),\n",
    "                'target_mask': self.make_std_mask(target, 0),\n",
    "                'tokens_count': (target != 0).sum()\n",
    "            }\n",
    "\n",
    "            self.samples.append(sample)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the number of samples in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            The number of samples.\n",
    "        \"\"\"\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        \"\"\"\n",
    "        Get a sample from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx: The index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing the source, target, target_y, source_mask, target_mask, and tokens_count.\n",
    "        \"\"\"\n",
    "        return self.samples[idx]\n",
    "\n",
    "    @staticmethod\n",
    "    def make_std_mask(target: torch.Tensor, pad: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Create a mask to hide padding and future words.\n",
    "\n",
    "        Args:\n",
    "            target (torch.Tensor): The target tensor.\n",
    "            pad (int): The padding value.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The mask tensor.\n",
    "        \"\"\"\n",
    "        target_mask = (target != pad)\n",
    "        target_mask = target_mask & torch.autograd.Variable(\n",
    "            RandomDataset.subsequent_mask(target.size(-1)).type_as(target_mask.data))\n",
    "\n",
    "        return target_mask\n",
    "\n",
    "    @staticmethod\n",
    "    def subsequent_mask(size: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Mask out subsequent positions.\n",
    "\n",
    "        Args:\n",
    "            size: The size of the mask.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The subsequent mask tensor.\n",
    "        \"\"\"\n",
    "        attn_shape = (size, size)\n",
    "        subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "        return torch.from_numpy(subsequent_mask) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "jMijjGYv8yRK"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_samples = 1000\n",
    "samples_len = 10\n",
    "train_set = RandomDataset(config.vocab_size, batch_size, num_samples, samples_len)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 549,
     "status": "ok",
     "timestamp": 1739173849153,
     "user": {
      "displayName": "Eunkyu Park",
      "userId": "11893515073306472432"
     },
     "user_tz": -540
    },
    "id": "pDwV87ujnMhy",
    "outputId": "9d7af989-8246-47ef-ecb2-f70f04684a07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (embeddings): Embeddings(\n",
       "      (token_embeddings): Embedding(100, 512)\n",
       "      (position_embeddings): Embedding(512, 512)\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderBlock(\n",
       "        (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): MultiHeadAttention(\n",
       "          (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderBlock(\n",
       "        (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): MultiHeadAttention(\n",
       "          (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): TransformerEncoderBlock(\n",
       "        (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): MultiHeadAttention(\n",
       "          (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): TransformerEncoderBlock(\n",
       "        (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): MultiHeadAttention(\n",
       "          (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): TransformerEncoderBlock(\n",
       "        (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): MultiHeadAttention(\n",
       "          (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): TransformerEncoderBlock(\n",
       "        (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): MultiHeadAttention(\n",
       "          (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (embeddings): Embeddings(\n",
       "      (token_embeddings): Embedding(100, 512)\n",
       "      (position_embeddings): Embedding(512, 512)\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerDecoderBlock(\n",
       "        (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm_3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): MultiHeadAttention(\n",
       "          (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerDecoderBlock(\n",
       "        (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm_3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): MultiHeadAttention(\n",
       "          (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): TransformerDecoderBlock(\n",
       "        (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm_3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): MultiHeadAttention(\n",
       "          (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): TransformerDecoderBlock(\n",
       "        (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm_3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): MultiHeadAttention(\n",
       "          (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): TransformerDecoderBlock(\n",
       "        (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm_3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): MultiHeadAttention(\n",
       "          (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): TransformerDecoderBlock(\n",
       "        (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm_3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_decoder_attention): MultiHeadAttention(\n",
       "          (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = EncoderDecoder(config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9AYKuyXM8ta4",
    "outputId": "287a0cd7-8a27-4a72-e488-1324aa09f693"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  10%|████                                     | 100/1000 [00:27<04:03,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 100; Loss: 4.839642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  20%|████████▏                                | 200/1000 [00:54<03:37,  3.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 200; Loss: 4.687149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  30%|████████████▎                            | 300/1000 [01:21<03:09,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 300; Loss: 4.671585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  40%|████████████████▍                        | 400/1000 [01:48<02:43,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 400; Loss: 4.652823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  50%|████████████████████▌                    | 500/1000 [02:15<02:16,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 500; Loss: 4.635049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  60%|████████████████████████▌                | 600/1000 [02:45<01:49,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 600; Loss: 4.621610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  70%|████████████████████████████▋            | 700/1000 [03:12<01:21,  3.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 700; Loss: 4.614254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  80%|████████████████████████████████▊        | 800/1000 [03:40<00:54,  3.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 800; Loss: 4.608254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  90%|████████████████████████████████████▉    | 900/1000 [04:07<00:27,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 900; Loss: 4.606917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|████████████████████████████████████████| 1000/1000 [04:34<00:00,  3.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1000; Loss: 4.604630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = EncoderDecoder(config).to(device)\n",
    "\n",
    "# Initialize parameters.\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        torch.nn.init.xavier_uniform_(p)\n",
    "\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "current_loss = 0.0\n",
    "counter = 0\n",
    "\n",
    "# tqdm을 사용하여 progress bar를 추가합니다.\n",
    "for i, batch in enumerate(tqdm(train_loader, desc=\"Training Progress\", ncols=100)):\n",
    "    # 데이터도 GPU로 이동\n",
    "    source = batch['source'].to(device)\n",
    "    target = batch['target'].to(device)\n",
    "    source_mask = batch['source_mask'].to(device)\n",
    "    target_mask = batch['target_mask'].to(device)\n",
    "    target_y = batch['target_y'].to(device)\n",
    "\n",
    "    # position_ids나 다른 텐서들이 CPU에 있을 수 있으므로 해당 텐서도 GPU로 이동\n",
    "    position_ids = torch.arange(source.size(1), device=device).unsqueeze(0)  # 예시\n",
    "    batch['position_ids'] = position_ids\n",
    "\n",
    "    with torch.set_grad_enabled(True):\n",
    "        out = model.forward(source, target, source_mask, target_mask)\n",
    "        loss = loss_function(out.contiguous().view(-1, out.size(-1)), target_y.contiguous().view(-1).long())\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        current_loss += loss.item()\n",
    "        counter += 1\n",
    "\n",
    "        if counter % 100 == 0:\n",
    "            print(\"Batch: %d; Loss: %f\" % (i + 1, current_loss / counter))\n",
    "            current_loss = 0.0\n",
    "            counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "VU_QcuX_jQS2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12]) torch.Size([1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-5.1863,  0.4589,  0.2204,  0.3417,  0.2435,  0.0502, -0.1730,\n",
       "           0.4358,  0.3653,  0.4224,  0.0095,  0.1332,  0.2602, -0.1964,\n",
       "           0.2211,  0.1413,  0.2812,  0.0622,  0.1738,  0.6793,  0.0913,\n",
       "           0.1623,  0.2148,  0.5131,  0.2322, -0.0073,  0.2067,  0.0390,\n",
       "           0.1210,  0.3781,  0.0927,  0.3638, -0.0176,  0.0437,  0.3765,\n",
       "          -0.0105, -0.0474,  0.3564, -0.0599,  0.1322,  0.1855,  0.2391,\n",
       "           0.2340, -0.1156,  0.3410,  0.4357, -0.1607,  0.0582,  0.1494,\n",
       "          -0.0765,  0.0339,  0.2254,  0.2118,  0.0364,  0.3456,  0.2008,\n",
       "           0.3556, -0.0720, -0.3504,  0.0287,  0.1312,  0.2217,  0.6383,\n",
       "          -0.0321,  0.1059,  0.1555,  0.0279, -0.1671,  0.0205,  0.1103,\n",
       "          -0.0877,  0.4064,  0.1032,  0.1070,  0.0849, -0.1501,  0.1310,\n",
       "           0.2176, -0.0758,  0.0571, -0.0313,  0.0227,  0.0815,  0.3571,\n",
       "          -0.0710,  0.1529,  0.2885,  0.2498,  0.1252, -0.0137,  0.1849,\n",
       "           0.2626, -0.1972, -0.0943,  0.6266,  0.0367,  0.2213,  0.0606,\n",
       "           0.2750,  0.1654]]], device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inference\n",
    "src = torch.tensor([[0, 2, 5, 6, 4, 3, 9, 5, 2, 9, 10, 1]])\n",
    "trg = torch.tensor([[0]])\n",
    "print(src.shape, trg.shape)\n",
    "out = model.forward(src.to(device), trg.to(device))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1, 94, 94, 94, 94, 94, 94, 94, 94, 94],\n",
      "        [ 1, 94, 94, 94, 94, 94, 94, 94, 94, 94],\n",
      "        [ 1, 94, 94, 94, 94, 94, 94, 94, 94, 94],\n",
      "        [ 1, 94, 94, 94, 94, 94, 94, 94, 94, 94]], device='cuda:0')\n",
      "[unused0] [unused93] [unused93] [unused93] [unused93] [unused93] [unused93] [unused93] [unused93] [unused93]\n"
     ]
    }
   ],
   "source": [
    "test_batch_size = 4\n",
    "test_num_samples = 10\n",
    "test_sample_length = 10\n",
    "\n",
    "test_set = RandomDataset(config.vocab_size, test_batch_size, test_num_samples, test_sample_length)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=test_batch_size)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch = next(iter(test_loader))\n",
    "\n",
    "    source = batch['source'].to(device)\n",
    "    source_mask = batch['source_mask'].to(device)\n",
    "\n",
    "    # 디코더 입력을 위한 초기 토큰 생성 (시작 토큰 <s>로 가정, ID=1)\n",
    "    target_start = torch.full((test_batch_size, 1), 1, dtype=torch.long, device=device)\n",
    "\n",
    "    for _ in range(test_sample_length - 1):\n",
    "        out = model.forward(source, target_start, source_mask, None)\n",
    "        next_token = torch.argmax(out[:, -1, :], dim=-1, keepdim=True)  # 마지막 토큰 예측\n",
    "        target_start = torch.cat([target_start, next_token], dim=1)  # 새로운 토큰 추가\n",
    "\n",
    "print(target_start)\n",
    "\n",
    "# target_start가 토큰 ID 텐서라고 가정\n",
    "reconstructed_text = tokenizer.decode(target_start[0], skip_special_tokens=True)\n",
    "\n",
    "print(reconstructed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W6nYB4egzC3Z"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "OO9yPXOXeo-V"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00ed9fa791b64ca1bc96715d457d6bbf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0b6c917112794e70aa34ab7a55f2c53b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1a4223e2bd404df59efc00768d684f07": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_34e317d5247d48179dc28312bd039e28",
      "placeholder": "​",
      "style": "IPY_MODEL_4c92ba9abc45458eb478f8fcd6602d51",
      "value": " 466k/466k [00:00&lt;00:00, 3.33MB/s]"
     }
    },
    "1a799bd44e91400e85f619d5c3b8ef8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_324b5437ce194083a8b1a4448adb5c00",
       "IPY_MODEL_60712d98c2be4e68b4e74a61e611f6df",
       "IPY_MODEL_1a4223e2bd404df59efc00768d684f07"
      ],
      "layout": "IPY_MODEL_20406be4301c4e3c845215c7c5f5d2d6"
     }
    },
    "1d628ec541414fd19a0574bdd8584267": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ba76572086a34f17b8bc74c24555ac25",
       "IPY_MODEL_5646d8471ebf416480c51131617bc0b3",
       "IPY_MODEL_5649283c39ad4c6fa9aff7bb111c6932"
      ],
      "layout": "IPY_MODEL_d1b20c8d0f534d019ace9a5f658e7d44"
     }
    },
    "20406be4301c4e3c845215c7c5f5d2d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23d576bf422044ba9a374725338135b3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "250e6d5241fa47f6973d5336dea474d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "324b5437ce194083a8b1a4448adb5c00": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4f458200fb22483897f02c570eed5d8c",
      "placeholder": "​",
      "style": "IPY_MODEL_00ed9fa791b64ca1bc96715d457d6bbf",
      "value": "tokenizer.json: 100%"
     }
    },
    "34e317d5247d48179dc28312bd039e28": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3739411d8ea3498a91c76a04987e4639": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_934bcd7efc5a4172a9e01057b7b4dd19",
       "IPY_MODEL_c4fd066fd96948ac8809bebb64eee3a5",
       "IPY_MODEL_91d7d6b2df114294b22c831fb021e5e9"
      ],
      "layout": "IPY_MODEL_449a9b149cf243a285dcc34d545a730b"
     }
    },
    "445f7b14de9b488298f287195b06f163": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a5c8e7f33223416d882bd819241cc7dd",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e3ea71e0919d430faaefb6141c1dc6d5",
      "value": 231508
     }
    },
    "449a9b149cf243a285dcc34d545a730b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "498fb504dc0445f7bbd7ad2e4fe98345": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4c92ba9abc45458eb478f8fcd6602d51": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4d5ce915e12f4b9e8c9b5fb0c4a0d416": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4f458200fb22483897f02c570eed5d8c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5646d8471ebf416480c51131617bc0b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_830d3cd2251d40a5b353dc99abe3991c",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b12c0054e86f4d7db1375121871e9b11",
      "value": 570
     }
    },
    "5649283c39ad4c6fa9aff7bb111c6932": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8084f3d45e8f47c782fa15f4a8b1a7aa",
      "placeholder": "​",
      "style": "IPY_MODEL_498fb504dc0445f7bbd7ad2e4fe98345",
      "value": " 570/570 [00:00&lt;00:00, 16.9kB/s]"
     }
    },
    "572b1e6014b947d89fb5adbacf149ae2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5c85d8ce83254d8bbae7e2cb76d4d68a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "60712d98c2be4e68b4e74a61e611f6df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ec8a649e12a2414b87591957a42826db",
      "max": 466062,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_572b1e6014b947d89fb5adbacf149ae2",
      "value": 466062
     }
    },
    "7ac10266e2b644feb90eeffb19720b1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7f33513aa81f4a6a9d20fb1966eb7048": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8084f3d45e8f47c782fa15f4a8b1a7aa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "828ede9769a4449bb6c1453dbf8cd4de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "830d3cd2251d40a5b353dc99abe3991c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b25ad83427b4026a6efb96c2e1dc30a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8f220b6d94544d0696df990072a900cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e48fc25fe9004b8c85c19a5e7749342f",
       "IPY_MODEL_445f7b14de9b488298f287195b06f163",
       "IPY_MODEL_9ff9e5e617db4e3b91b6156e9cbffc7f"
      ],
      "layout": "IPY_MODEL_23d576bf422044ba9a374725338135b3"
     }
    },
    "91d7d6b2df114294b22c831fb021e5e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0b6c917112794e70aa34ab7a55f2c53b",
      "placeholder": "​",
      "style": "IPY_MODEL_828ede9769a4449bb6c1453dbf8cd4de",
      "value": " 48.0/48.0 [00:00&lt;00:00, 3.06kB/s]"
     }
    },
    "934bcd7efc5a4172a9e01057b7b4dd19": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8b25ad83427b4026a6efb96c2e1dc30a",
      "placeholder": "​",
      "style": "IPY_MODEL_7ac10266e2b644feb90eeffb19720b1a",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "9ff9e5e617db4e3b91b6156e9cbffc7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c0eb1f6e93b54a8bbe3e43e24d0147dc",
      "placeholder": "​",
      "style": "IPY_MODEL_ce5d34df96104637899aec37c47e9d42",
      "value": " 232k/232k [00:00&lt;00:00, 2.65MB/s]"
     }
    },
    "a5c8e7f33223416d882bd819241cc7dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b12c0054e86f4d7db1375121871e9b11": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ba76572086a34f17b8bc74c24555ac25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7f33513aa81f4a6a9d20fb1966eb7048",
      "placeholder": "​",
      "style": "IPY_MODEL_250e6d5241fa47f6973d5336dea474d4",
      "value": "config.json: 100%"
     }
    },
    "c0eb1f6e93b54a8bbe3e43e24d0147dc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c4fd066fd96948ac8809bebb64eee3a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4d5ce915e12f4b9e8c9b5fb0c4a0d416",
      "max": 48,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f1071d22a10b410a95ed2fd97248b402",
      "value": 48
     }
    },
    "ce5d34df96104637899aec37c47e9d42": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d1b20c8d0f534d019ace9a5f658e7d44": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3ea71e0919d430faaefb6141c1dc6d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e48fc25fe9004b8c85c19a5e7749342f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5c85d8ce83254d8bbae7e2cb76d4d68a",
      "placeholder": "​",
      "style": "IPY_MODEL_f45509660995415aad510c3f476cedf8",
      "value": "vocab.txt: 100%"
     }
    },
    "ec8a649e12a2414b87591957a42826db": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f1071d22a10b410a95ed2fd97248b402": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f45509660995415aad510c3f476cedf8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
