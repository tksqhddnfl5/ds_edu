{"cells":[{"cell_type":"markdown","metadata":{"id":"gogyMPbAi6lQ"},"source":["## Introduction"]},{"cell_type":"markdown","metadata":{"id":"Bp-l0KEbi6lR"},"source":["It was in January of 2021 that **OpenAI** announced two new models: **DALL-E** and **CLIP**, both **multimodal** models connecting **texts and images**. In this tutorial we are going to implement CLIP model from scratch in **PyTorch**."]},{"cell_type":"markdown","metadata":{"id":"-YuucuWpi6lR"},"source":["### What does CLIP do? Why is it fun?"]},{"cell_type":"markdown","metadata":{"id":"cccVdOPUi6lR"},"source":["In [Learning Transferable Visual Models From Natural Language Supervision paper](https://arxiv.org/abs/2103.00020), OpenAI introduces their new model which is called **CLIP**, for **Contrastive Language-Image Pre-training**. In a nutshell, this model learns the relationship between a whole sentence and the image it describes; in a sense that when the model is trained, given an input sentence it will be able to retrieve the most related images corresponding to that sentence (and vice versa). The important thing here is that it is trained on full sentences instead of single classes like car, dog, etc. The intuition is that when trained on whole sentences, the model can learn a lot more things and finds some pattern between images and texts.\n","They also show that when this model is trained on a huge dataset of images and their corresponding texts, it can also act as a classifier too. I encourage you to study the paper to learn more about this exciting model and their astonishing results on benchmarking datasets. To mention just one, CLIP model trained with this strategy classifies ImageNet better than those SOTA models trained on the ImageNet itself optimized for the only task of classification!"]},{"cell_type":"markdown","metadata":{"id":"HDteAZPCi6lS"},"source":["As a **teaser** (!), let's see what the final model that we will build in this article from scratch is capable of: given a query (raw text) like \"a boy jumping with skateboard\" or \"a girl jumping from swing\", the model will retrieve the most relevant images:"]},{"cell_type":"markdown","metadata":{"id":"sYa6nIYei6lS"},"source":["![](https://i.ibb.co/9gdYqNP/teaser-cropped.png)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZSvBQoszjYJS"},"outputs":[],"source":["# !pip install timm\n","# !pip install transformers"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"bvXzfQqgi6lT"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_3724\\1385263093.py:12: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from tqdm.autonotebook import tqdm\n"]}],"source":["import os\n","import json\n","from pathlib import Path\n","import random\n","import json\n","\n","import cv2\n","from PIL import Image\n","import numpy as np\n","import pandas as pd\n","import itertools\n","from tqdm.autonotebook import tqdm\n","import matplotlib.pyplot as plt\n","\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torchvision.transforms as transforms\n","import timm\n","from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OQ0mNC5j57Z7"},"outputs":[],"source":["# !wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n","# !unzip -qq Flickr8k_Dataset.zip\n","# !wget https://github.com/Delphboy/karpathy-splits/raw/refs/heads/main/dataset_flickr8k.json\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[5], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8192\u001b[39m):\n\u001b[0;32m     19\u001b[0m                 file\u001b[38;5;241m.\u001b[39mwrite(chunk)\n\u001b[1;32m---> 21\u001b[0m \u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_zip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m zipfile\u001b[38;5;241m.\u001b[39mZipFile(dataset_zip, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m zip_ref:\n\u001b[0;32m     24\u001b[0m     zip_ref\u001b[38;5;241m.\u001b[39mextractall(data_dir)\n","Cell \u001b[1;32mIn[5], line 18\u001b[0m, in \u001b[0;36mdownload_file\u001b[1;34m(url, save_path)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(save_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m---> 18\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8192\u001b[39m):\n\u001b[0;32m     19\u001b[0m             file\u001b[38;5;241m.\u001b[39mwrite(chunk)\n","File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n","File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp):\n\u001b[1;32m--> 628\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m    631\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n","File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    564\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 567\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    569\u001b[0m         flush_decoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n","File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\http\\client.py:463\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;66;03m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[0;32m    462\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(amt)\n\u001b[1;32m--> 463\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b)[:n]\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;66;03m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;66;03m# and self.chunked\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\http\\client.py:507\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    502\u001b[0m         b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmemoryview\u001b[39m(b)[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength]\n\u001b[0;32m    504\u001b[0m \u001b[38;5;66;03m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;66;03m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m n \u001b[38;5;129;01mand\u001b[39;00m b:\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n","File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1239\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1240\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n","File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1099\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import os\n","import requests\n","import zipfile\n","\n","data_dir = \"D:/data\"\n","os.makedirs(data_dir, exist_ok=True)\n","\n","dataset_url = \"https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\"\n","dataset_zip = os.path.join(data_dir, \"Flickr8k_Dataset.zip\")\n","\n","json_url = \"https://github.com/Delphboy/karpathy-splits/raw/refs/heads/main/dataset_flickr8k.json\"\n","json_path = os.path.join(data_dir, \"dataset_flickr8k.json\")\n","\n","def download_file(url, save_path):\n","    response = requests.get(url, stream=True)\n","    if response.status_code == 200:\n","        with open(save_path, \"wb\") as file:\n","            for chunk in response.iter_content(chunk_size=8192):\n","                file.write(chunk)\n","\n","download_file(dataset_url, dataset_zip)\n","\n","with zipfile.ZipFile(dataset_zip, \"r\") as zip_ref:\n","    zip_ref.extractall(data_dir)\n","\n","download_file(json_url, json_path)\n","\n","print(\"완료.\")\n"]},{"cell_type":"markdown","metadata":{"id":"aF8XzKlSi6lT"},"source":["## Some pre-preocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k1XwAuOy8E8H"},"outputs":[],"source":["image_path = \"/content/Flicker8k_Dataset/\"\n","captions_path = \"/content/dataset_flickr8k.json\""]},{"cell_type":"markdown","metadata":{"id":"o54bL4gji6lU"},"source":["## Config"]},{"cell_type":"markdown","metadata":{"id":"P6En0z5Si6lU"},"source":["*A note on config and CFG: I wrote the codes with python scripts and then converted it into a Jupyter Notebook. So, in case of python scripts, config is a normal python file where I put all the hyperparameters and in the case of Jupyter Notebook, its a class defined in the beginning of the notebook to keep all the hyperparameters.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ulGHg9ai6lV"},"outputs":[],"source":["class CFG:\n","    image_path = image_path\n","    captions_path = captions_path\n","    batch_size = 32\n","    num_workers = 2\n","\n","    head_lr = 1e-3\n","    image_encoder_lr = 1e-4\n","    text_encoder_lr = 1e-5\n","    weight_decay = 1e-3\n","    epochs = 4\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    model_name = 'resnet50'\n","    image_embedding = 2048\n","    text_encoder_model = \"distilbert-base-uncased\"\n","    text_embedding = 768\n","    text_tokenizer = \"distilbert-base-uncased\"\n","    max_length = 200\n","\n","    pretrained = True # for both image encoder and text encoder\n","    trainable = True # for both image encoder and text encoder\n","    temperature = 0.07\n","\n","    # image size\n","    size = 224\n","\n","    # for projection head; used for both image and text encoders\n","    projection_dim = 256"]},{"cell_type":"markdown","metadata":{"id":"P8OQpI9Xi6lV"},"source":["## Utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_piLd4dxi6lV"},"outputs":[],"source":["class AvgMeter:\n","    def __init__(self, name=\"Metric\"):\n","        self.name = name\n","        self.reset()\n","\n","    def reset(self):\n","        self.avg, self.sum, self.count = [0] * 3\n","\n","    def update(self, val, count=1):\n","        self.count += count\n","        self.sum += val * count\n","        self.avg = self.sum / self.count\n","\n","    def __repr__(self):\n","        text = f\"{self.name}: {self.avg:.4f}\"\n","        return text\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uM65_Loji6lW"},"source":["## Dataset"]},{"cell_type":"markdown","metadata":{"id":"-qJIVdNWi6lW"},"source":["As you can see in the tittle image of this article, we need to encode both images and their describing texts. So, the dataset needs to **return both images and texts**.\n","The datset we will be using is Flickr8k dataset, which contains (as name suggested) 8,000 image-text pairs."]},{"cell_type":"markdown","metadata":{"id":"UWAVxFMwi6lW"},"source":["I did not use additional data augmentations but you can add them if you want to improve the model's performance."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fKWUe5vZrpbO"},"outputs":[],"source":["with open(captions_path) as f:\n","    flickr_json = json.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l9V91XcNi6lW"},"outputs":[],"source":["class Flickr8kCaptions(torch.utils.data.Dataset):\n","    \"\"\"\n","    Flickr8K captions dataset.\n","\n","    Karpathy split JSON can be downloaded from this webpage:\n","    https://cs.stanford.edu/people/karpathy/deepimagesent/\n","    \"\"\"\n","\n","    def __init__(self, image_path, captions_path, split, transform=None,\n","                 return_single_text=False):\n","        self.image_path = image_path\n","        self.captions_path = captions_path\n","        self.split = split\n","        self.transform = transform\n","        self.return_single_text = return_single_text\n","\n","        # Read annotations and keep only those belonging to specified split.\n","        with open(self.captions_path) as f:\n","            flickr_json = json.load(f)\n","\n","        # Convert the filtered list of tuples formatted as:\n","        # `(image_id, image_path, list[caption_ids], list[caption])`.\n","        # Only keep images that belong to required split.\n","        self.samples = [\n","            (\n","                ann[\"filename\"][:-4],\n","                os.path.join(self.image_path, ann[\"filename\"]),\n","                ann[\"sentids\"],\n","                [entry[\"raw\"] for entry in ann[\"sentences\"]],\n","            )\n","            for ann in flickr_json[\"images\"]\n","            if ann[\"split\"] == split\n","        ]\n","\n","    def __len__(self) -> int:\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx: int) -> dict:\n","        image_id, image_path, caption_ids, captions = self.samples[idx]\n","        if self.return_single_text:\n","            caption_idx = random.randrange(0, len(captions))\n","            captions = captions[caption_idx]\n","            caption_ids = caption_ids[caption_idx]\n","        image = Image.open(image_path).convert(\"RGB\")\n","        if self.transform is not None:\n","            image = self.transform(image)\n","\n","        return {\n","            \"image_id\": image_id,\n","            \"caption_ids\": caption_ids,\n","            \"image\": image,\n","            \"captions\": captions,\n","        }\n","\n","\n","data_config = timm.data.resolve_model_data_config(CFG.model_name)\n","transform = timm.data.create_transform(**data_config, is_training=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l67zYC8giPCg"},"outputs":[],"source":["transform"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"neJOt4XviRxu"},"outputs":[],"source":["timm.data.create_transform(**data_config, is_training=True)"]},{"cell_type":"markdown","metadata":{"id":"yk394aMmi6lX"},"source":["## Image Encoder"]},{"cell_type":"markdown","metadata":{"id":"3Lnwx026i6lY"},"source":["The image encoder code is straight forward. I'm using PyTorch Image Models library (timm) here which makes a lot of different image models available from ResNets to EfficientNets and many more. Here we will use a ResNet50 as our image encoder. You can easily use torchvision library to use ResNets if you don't want to install a new library."]},{"cell_type":"markdown","metadata":{"id":"xHIYjFc0i6lY"},"source":["The code encodes each image to a fixed size vector with the size of the model's output channels (in case of ResNet50 the vector size will be **2048**). This is the output after the nn.AdaptiveAvgPool2d() layer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i0flfMTRi6lY"},"outputs":[],"source":["class ImageEncoder(nn.Module):\n","    \"\"\"\n","    Encode images to a fixed size vector\n","    \"\"\"\n","\n","    def __init__(\n","        self, model_name=CFG.model_name, pretrained=CFG.pretrained, trainable=CFG.trainable\n","    ):\n","        super().__init__()\n","        self.model = timm.create_model(\n","            model_name, pretrained, num_classes=0, global_pool=\"avg\"\n","        )\n","        for p in self.model.parameters():\n","            p.requires_grad = trainable\n","\n","    def forward(self, x):\n","        return self.model(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A39-D7U7r21O"},"outputs":[],"source":["image_model = timm.create_model(CFG.model_name, pretrained=True, num_classes=0, global_pool='avg')\n","image_model"]},{"cell_type":"markdown","metadata":{"id":"JKRoQ0o9i6lY"},"source":["## Text Encoder"]},{"cell_type":"markdown","metadata":{"id":"-6jTZWtoi6lZ"},"source":["As I mentioned before, I'll use DistilBERT as the text encoder. Like its bigger brother BERT, two special tokens will be added to the actual input tokens: **CLS** and **SEP** which mark the start and end of a sentence. To grab the whole representation of a sentence (as the related BERT and DistilBERT papers point out) we use the final representations of the CLS token and we hope that this representation captures the overall meaning of the sentence (caption). Thinking it in this way, it is similar to what we did to images and converted them into a fixed size vector.\n","\n","In the case of DistilBERT (and also BERT) the output hidden representation for each token is a vector with size **768**. So, the whole caption will be encoded in the CLS token representation whose size is 768."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"am5VR4Ezi6lZ"},"outputs":[],"source":["class TextEncoder(nn.Module):\n","    def __init__(self, model_name=CFG.text_encoder_model, pretrained=CFG.pretrained, trainable=CFG.trainable):\n","        super().__init__()\n","        if pretrained:\n","            self.model = DistilBertModel.from_pretrained(model_name)\n","        else:\n","            self.model = DistilBertModel(config=DistilBertConfig())\n","\n","        for p in self.model.parameters():\n","            p.requires_grad = trainable\n","\n","        # we are using the CLS token hidden representation as the sentence's embedding\n","        self.target_token_idx = 0\n","\n","    def forward(self, input_ids, attention_mask):\n","        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n","        last_hidden_state = output.last_hidden_state\n","        return last_hidden_state[:, self.target_token_idx, :]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oQSpF60LsAAw"},"outputs":[],"source":["text_model = DistilBertModel.from_pretrained(CFG.text_encoder_model)"]},{"cell_type":"markdown","metadata":{"id":"tDKLE4cKi6lZ"},"source":["## Projection Head"]},{"cell_type":"markdown","metadata":{"id":"h64obouni6lZ"},"source":["\n","Now that we have encoded both our images and texts into fixed size vectors (2048 for image and 768 for text) we need to bring (project) them into a **same space** for both images and texts in order to be able to compare them and push apart the non-relevant image and texts and pull together those that match. So, the following code will bring the 2048 and 768 dimensional vectors into a 256 (projection_dim) dimensional world, where we can **compare** them.\n","\n","\"embedding_dim\" is the size of the input vector (2048 for images and 768 for texts) and \"projection_dim\" is the the size of the output vector which will be 256 for our case. For understanding the details of this part you can refer to the CLIP paper."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SJbY9Yrui6la"},"outputs":[],"source":["class ProjectionHead(nn.Module):\n","    def __init__(\n","        self,\n","        embedding_dim,\n","        projection_dim=CFG.projection_dim,\n","    ):\n","        super().__init__()\n","        self.projection = nn.Linear(embedding_dim, projection_dim, bias=False)\n","\n","    def forward(self, x):\n","        x = self.projection(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"FUs23eeoi6la"},"source":["## CLIP"]},{"cell_type":"markdown","metadata":{"id":"eFva6mf0i6la"},"source":["This part is where all the fun happens! I'll also talk about the loss function here. I translated some of the code from Keras code examples into PyTorch for writing this part. Take a look at the code and then read the explanation below this code block."]},{"cell_type":"markdown","metadata":{"id":"XMPlIhwHi6lb"},"source":["Here we will use the previous modules that we built to implement the main model. The \\_\\_init\\_\\_ function is self-explanatory. In the forward function, we first encode the images and texts separately into fixed size vectors (with different dimensionalities). After that, using separate projection modules we project them to that shared world (space) that I talked about previously. Here the encodings will become of similar shape (256 in our case). After that we will compute the loss. Again I recommend reading CLIP paper to get it better but I'll try my best to explain this part.\n","\n","In **Linear Algebra**, one common way to measure if two vectors are of similar characteristics (they are like each other) is to calculate their **cosine similarity**; if the cosine similarity is big, they are alike and if it is small they are not (relatively speaking)!"]},{"cell_type":"markdown","metadata":{"id":"PMCKyV5Ei6lb"},"source":["Okay! What I just said is the most important thing to have in mind to understand this loss function. Let's continue. We talked about two vectors, but, what do we have here? We have image_embeddings, a matrix with shape (batch_size, 256) and text_embeddings with shape (batch_size, 256). Easy enough! it means we have two groups of vectors instead of two single vectors. How do we measure how similar two groups of vectors (two matrices) are to each other? Again, with dot product (@ operator in PyTorch does the dot product or matrix multiplication in this case). To be able to multiply these two matrices together, we transpose the second one. Okay, we get a matrix with shape (batch_size, batch_size) which we will call logits."]},{"cell_type":"markdown","metadata":{"id":"IZ88Mer8i6lb"},"source":["Let's consider what we hope that this model learns: **we want it to learn \"similar representations (vectors)\" for a given image and the caption describing it. Meaning that either we give it an image or the text describing it, we want it to produce same 256 sized vectors for both.**"]},{"cell_type":"markdown","metadata":{"id":"1lhR_hNPknvD"},"source":["CLIP training objective (loss):\n","\n","![clip loss](https://miro.medium.com/v2/resize:fit:1400/1*KbxO4qPaq3z8dJ8vw5h1Qg.png)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7MQnmwsWi6lc"},"outputs":[],"source":["class CLIPModel(nn.Module):\n","    def __init__(\n","        self,\n","        temperature=CFG.temperature,\n","        image_embedding=CFG.image_embedding,\n","        text_embedding=CFG.text_embedding,\n","    ):\n","        super().__init__()\n","        self.image_encoder = ImageEncoder()\n","        self.text_encoder = TextEncoder()\n","        self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n","        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n","\n","        # temperature in original CLIP is also a learnable parameter, but we fix this for easier implementation\n","        self.temperature = temperature\n","\n","    def encode_image(self, image):\n","        # TODO: compute normalized image embeddings\n","\n","        return image_embeddings\n","\n","    def encode_text(self, input_ids, attention_mask):\n","        # TODO: compute normalized text embeddings\n","\n","        return text_embeddings\n","\n","    def forward(self, batch):\n","        image_embeddings = self.encode_image(batch['image'])\n","        text_embeddings = self.encode_text(batch['input_ids'], batch['attention_mask'])\n","\n","        # TODO: calculate the loss\n","        # Hint: divide cosine similarity by self.temperature\n","        # Hint: use cross entropy loss (F.cross_entropy) using label targets [0, 1, ...., batch_size]\n","\n","        return loss\n","\n"]},{"cell_type":"markdown","metadata":{"id":"JXkzSurfi6ld"},"source":["## Train"]},{"cell_type":"markdown","metadata":{"id":"BYrHf4yQi6ld"},"source":["Here are some funtions to help us load train and valid dataloaders, our model and then train and evaluate our model on those. There's not much going on here; just simple training loop and utility functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HUSC32Cei6ld"},"outputs":[],"source":["\n","def build_loaders(split='train'):\n","    dataset = Flickr8kCaptions(\n","        image_path,\n","        captions_path,\n","        split=split,\n","        transform=transform,\n","        return_single_text=True\n","    )\n","    dataloader = torch.utils.data.DataLoader(\n","        dataset,\n","        batch_size=CFG.batch_size,\n","        num_workers=CFG.num_workers,\n","        shuffle=True if split == 'train' else False,\n","    )\n","    return dataloader"]},{"cell_type":"markdown","metadata":{"id":"i4FcULUYi6le"},"source":["Here's a handy function to train our model. There's not much happening here; just loading the batches, feeding them to the model and stepping the optimizer and lr_scheduler."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jkjp0BEPi6le"},"outputs":[],"source":["def train_epoch(model, train_loader, optimizer, tokenizer):\n","    loss_meter = AvgMeter()\n","    tqdm_object = tqdm(train_loader, total=len(train_loader))\n","    for batch in tqdm_object:\n","        batch['image'] = batch['image'].to(CFG.device)\n","        encoded_captions = tokenizer(\n","            batch['captions'], padding=True, truncation=True, max_length=CFG.max_length,\n","            return_tensors='pt'\n","        )\n","        batch['input_ids'] = encoded_captions['input_ids'].to(CFG.device)\n","        batch['attention_mask'] = encoded_captions['attention_mask'].to(CFG.device)\n","\n","        loss = model(batch)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        count = batch[\"image\"].size(0)\n","        loss_meter.update(loss.item(), count)\n","\n","        tqdm_object.set_postfix(train_loss=loss_meter.avg)\n","    return loss_meter\n","\n","\n","def train():\n","    tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n","    train_loader = build_loaders()\n","\n","    model = CLIPModel().to(CFG.device)\n","    params = [\n","        {\"params\": model.image_encoder.parameters(), \"lr\": CFG.image_encoder_lr},\n","        {\"params\": model.text_encoder.parameters(), \"lr\": CFG.text_encoder_lr},\n","        {\"params\": list(model.image_projection.parameters()) + list(model.text_projection.parameters()),\n","         \"lr\": CFG.head_lr, \"weight_decay\": CFG.weight_decay}\n","    ]\n","    optimizer = torch.optim.AdamW(params, weight_decay=0.)\n","\n","    for epoch in range(CFG.epochs):\n","        print(f\"Epoch: {epoch + 1}\")\n","        model.train()\n","        train_loss = train_epoch(model, train_loader, optimizer, tokenizer)\n","\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"8vBQ2Wuyi6le"},"source":["Running the next cell start training the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2G3S841Vi6le"},"outputs":[],"source":["model = train()"]},{"cell_type":"markdown","metadata":{"id":"eiMjVEVci6le"},"source":["## Inference"]},{"cell_type":"markdown","metadata":{"id":"WvtxDLeii6lf"},"source":["Okay! We are done with training the model. Now, we need to do inference which in our case will be giving the model a piece of text and want it to retrieve the most relevant images from an unseen validation (or test) set."]},{"cell_type":"markdown","metadata":{"id":"G-dteKJwi6lf"},"source":["### Getting Image Embeddings"]},{"cell_type":"markdown","metadata":{"id":"1JTMqjOwi6lf"},"source":["In this function, we feed our trained model validation set images and return the image_embeddings with shape (valid_set_size, 256)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gKkDZNP3i6lf"},"outputs":[],"source":["def get_image_embeddings(model):\n","    tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n","    valid_loader = build_loaders(split=\"val\")\n","\n","    model.eval()\n","\n","    valid_image_embeddings = []\n","    with torch.no_grad():\n","        for batch in tqdm(valid_loader):\n","            image_embeddings = model.encode_image(batch[\"image\"].to(CFG.device))\n","            valid_image_embeddings.append(image_embeddings)\n","    return torch.cat(valid_image_embeddings, dim=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ySr-oGpMi6lf"},"outputs":[],"source":["image_embeddings = get_image_embeddings(model)"]},{"cell_type":"markdown","metadata":{"id":"yP81I_wxi6lf"},"source":["### Finding Matches"]},{"cell_type":"markdown","metadata":{"id":"8LF3NabVi6lg"},"source":["This function does the final task that we wished our model would be capable of: it gets the model, image_embeddings, and a text query. It will display the most relevant images from the validation set! Isn't it amazing? Let's see how it performs after all!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"un6RzF4Ri6lg"},"outputs":[],"source":["def find_matches(model, image_embeddings, query, image_filenames, n=9):\n","    tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n","    encoded_query = tokenizer([query], return_tensors='pt')\n","    batch = {\n","        key: values.to(CFG.device)\n","        for key, values in encoded_query.items()\n","    }\n","    with torch.no_grad():\n","        text_embeddings = model.encode_text(batch[\"input_ids\"], batch[\"attention_mask\"])\n","\n","    similarity = text_embeddings @ image_embeddings.T\n","\n","    values, indices = torch.topk(similarity.squeeze(0), n)\n","    matches = [image_filenames[idx] for idx in indices]\n","\n","    _, axes = plt.subplots(3, 3, figsize=(10, 10))\n","    for match, ax in zip(matches, axes.flatten()):\n","        image = cv2.imread(os.path.join(f\"{CFG.image_path}\", f\"{match}\"))\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        ax.imshow(image)\n","        ax.axis(\"off\")\n","\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"zoOu3JE8i6lg"},"source":["This is how we use this function. Aaaannnndddd the results:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x4-FTuNKKVBB"},"outputs":[],"source":["with open(captions_path) as f:\n","    flickr_json = json.load(f)\n","\n","image_filenames = [os.path.join(image_path, ann[\"filename\"]) for ann in flickr_json[\"images\"] if ann[\"split\"] == 'val']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"waXVzbioi6lg"},"outputs":[],"source":["\n","find_matches(model,\n","             image_embeddings,\n","             query=\"dogs on the grass\",\n","             image_filenames=image_filenames,\n","             n=9)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"23dYXwBRi6lg"},"outputs":[],"source":["# try your own queries!\n","find_matches(model,\n","             image_embeddings,\n","             query=\"a boy skateboarding\",\n","             image_filenames=image_filenames,\n","             n=9)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ElDPDkXDCLx_"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.11"}},"nbformat":4,"nbformat_minor":0}
